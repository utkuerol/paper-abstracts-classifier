,sentence,background_motivation,research_method,aim_contribution_research_object,results_findings_summary
0,"Context: Open tools (e.g., Jenkins, Gerrit and Git) offer a lucrative alternative to commercial tools.",1,0,0,0
1,Many companies and developers from OSS communities make a collaborative effort to improve the tools.,1,0,0,0
2,"Prior to this study, we developed an empirically based theory for companies' strategic choices on the development of these tools, based on empirical observations in the telecom domain.",1,0,0,0
3,"Aim: The aim of this study is to validate the theory of openness for tools in software engineering, in another domain, automotive.",0,0,1,0
4,"Specifically, we validated the theory propositions and mapped the case companies onto the model of openness.",0,0,1,0
5,"Method: We run focus groups in two automotive companies, collecting data in a survey and follow-up discussions.",0,1,0,0
6,"We used the repertory grid technique to analyze the survey responses, in combination with qualitative data from the focus group, to validate the propositions.",0,1,0,0
7,"Results: Openness of tools has the potential to reduce development costs and time, and may lead to process and product innovation.",0,0,0,1
8,"This study confirms three out of five theory propositions, on cost and time reduction, and the complementary role of open tools.",0,0,0,1
9,One propositions was not possible to validate due to lack of investment in OSS tools communities by both companies.,0,0,0,1
10,"However, our findings extend the fifth proposition to require management being involved for both the proactive and reactive strategy.",0,0,0,1
11,"Further, we observe that the move towards open tools happen with a paradigm shift towards openness in the automotive domain, and lead to standardization of tools.",0,0,0,1
12,"Both companies confirm that they need legal procedures for the contribution, as well as an internal champion, driving the open tools strategy.",0,0,0,1
13,"Conclusion: We validated the theory, originating from the telecom domain, partially using two automotive companies.",0,0,0,1
14,"Both case companies are classified as laggards (reactive, cost saving) in the model of openness presented in the theory.",0,0,0,1
15,"Furthermore, we would like to have more validations studies to validate the remaining quadrants (e.g., leverage, lucrativeness and leaders).",0,0,0,1
16,"Microsoft VBA (Visual Basic for Applications) is a programming language widely used by end-user programmers, often alongside the popular spreadsheet software Excel.",1,0,0,0
17,Together they form the popular Excel-VBA application ecosystem.,1,0,0,0
18,"Despite being popular, spreadsheets are known to be fault-prone, and to minimize risk of faults in the overall Excel-VBA ecosystem, it is important to support end-user programmers in improving the code quality of their VBA programs also, in addition to improving spreadsheet technology and practices.",1,0,0,0
19,"In traditional software development, automatic code inspection using static analysis tools has been found effective in improving code quality, but the practical relevance of this technique in an end-user development context remains unexplored.",1,0,0,0
20,"With the aim of popularizing it in the end-user community, in this paper we examine the relevance of automatic code inspection in terms of how inspection rules are perceived by VBA programmers.",0,0,1,0
21,"We conduct a qualitative study consisting of interviews with 14 VBA programmers, who share their perceptions about 20 inspection rules that most frequently detected code quality issues in an industrial dataset of 25 VBA applications, obtained from a financial services company.",0,1,0,0
22,"Results show that the 20 studied inspection rules can be grouped into three categories of user perceptions based on the type of issues they warn about: i) 11 rules that warn about serious problems which need fixing, ii) 7 rules that warn about bad practices which do not mandate fixing, and iii) 2 rules that warn about purposeful code elements rather than issues.",0,0,0,1
23,"Based on these perceptions, we conclude that automatic code inspection is considerably relevant in an end-user development context such as VBA.",0,0,0,1
24,The perceptions also indicate which inspection rules deserve the most attention from interested researchers and tool developers.,0,0,0,1
25,"Lastly, our results also reveal 3 additional issue types that are not covered by the existing inspection rules, and are therefore impetus for creating new rules.",0,0,0,1
26,Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR).,1,0,0,0
27,Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines.,1,0,0,0
28,"One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs.",1,0,0,0
29,Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE.,0,0,1,0
30,Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs.,0,1,0,0
31,We carried out an empirical inquiry by integrating SLR and confirmatory email survey.,0,1,0,0
32,"Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it.",0,0,0,1
33,Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs.,0,0,0,1
34,Our study reveals that meta-ethnography is a suitable method to SE research.,0,0,0,1
35,We discuss challenges and propose recommendations of adopting meta-ethnography in SE.,0,0,0,1
36,Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.,0,0,0,1
37,Learning software security is a big challenging task in the information technology sector due to the vast amount of security knowledge and the difficulties in understanding the practical applications.,1,0,0,0
38,"The traditional teaching and learning materials, which are usually organized topically and security-centric, have fewer linkages with learners' experience and prior knowledge that they bring to the learning sessions.",1,0,0,0
39,Learners often do not associate vulnerabilities or coding practices with programs similar to what they were writing in their previous time.,1,0,0,0
40,"Consequently, their motivation for learning is not touched by conventional methods.",1,0,0,0
41,"The aim of this paper is the presentation of an ontology-based learning system for software security with contextualized learning approaches, and of the results of an initial evaluation using a controlled quasi-experiment in a university learning environment.",0,0,1,0
42,"This system facilitates the contextual learning process by providing contextualized access to security knowledge via real software application scenarios, in which learners can explore and relate the security knowledge to the context they are already familiar with.",0,0,1,0
43,The experiment results show that the prototyped system with the proposed learning approach not only yields significant knowledge gain compared to the conventional learning approach but also gains better learning satisfaction of students.,0,0,0,1
44,"Role stereotypes indicate generic roles that classes play in the design of software systems (e.g. controller, information holder, or interfacer).",1,0,0,0
45,"Knowledge about the role-stereotypes can help in various tasks in software development and maintenance, such as program understanding, program summarization, and quality assurance.",1,0,0,0
46,This paper presents an automated machine learning-based approach for classifying the role-stereotype of classes in Java.,0,0,1,0
47,We analyse the performance of this approach against a manually labelled ground truth for a sizable open source project (of 770+ Java classes) for the Android platform.,0,1,0,0
48,"Moreover, we compare our approach to an existing rule-based classification approach.",0,1,0,0
49,The contributions of this paper include an analysis of which machine learning algorithms and which features provide the best classification performance.,0,0,1,0
50,This analysis shows that the Random Forest algorithm yields the best classification performance.,0,0,0,1
51,"We find however, that the performance of the ML-classifier varies a lot for classifying different role-stereotypes.",0,0,0,1
52,In particular its performs degrades for rare role-types.,0,0,0,1
53,"Our ML-classifier improves over the existing rule-based classification method in that the ML-approach classifies all classes, while rule-based approaches leave a significant number of classes unclassified.",0,0,0,1
54,"Background Binary Logistic Regression is widely used in Empirical Software Engineering to build estimation models, e.g., fault-proneness models, which estimate the probability that a given module is faulty, based on some measures of the module.",1,0,0,0
55,"Fault-proneness models are then used to build faultiness model, i.e., models that estimate whether a given module is faulty or non-faulty.",1,0,0,0
56,"Objective Because of the very nature of Binary Logistic Regression, there is always a range of values of the independent variable in which estimates are very close to the estimates that would be obtained via random estimation.",1,0,0,0
57,"These estimates are hardly accurate, and should be regarded as not reliable.",1,0,0,0
58,"For Binary Logistic Regression models used to build faultiness models---i.e., binary classifiers---a range where estimates are inaccurate can be regarded as an ""uncertainty"" area, where the model is uncertain about the faultiness of the given modules.",1,0,0,0
59,We define and empirically validate a simple method to identify the uncertainty region.,0,0,1,0
60,Method We compute the standard deviation of a probability estimate provided by a Binary Logistic Regression model.,0,1,0,0
61,"If the random estimate is within the range centered on the estimate and spanning a standard deviation, we regard the estimate as ""too close"" to the random estimate, hence unreliable.",0,1,0,0
62,"On the contrary, estimates that are far enough from the random estimate are considered reliable.",0,1,0,0
63,This method was tested on 54 datasets from the PROMISE (now SEACRAFT) repository.,0,1,0,0
64,Results Our results show that the variance of estimates can be effectively used to detect the uncertainty region.,0,0,0,1
65,"Estimates in the uncertainty area are rarely statistically significant, and always much less accurate than estimates obtained out of the uncertainty area.",0,0,0,1
66,"Conclusions Practitioners and researchers can use our results to assess the reliability of estimates obtained via Binary Logistic Regression models, and reject (or challenge) those estimates that fall in the uncertainty region.",0,0,0,1
67,"Background: Blog posts offer potential benefits for research, but also present challenges.",1,0,0,0
68,The use of blog posts in SE research is contentious for some members of the community.,1,0,0,0
69,"Also, there are no guidelines for evaluating the credibility of blog posts.",1,0,0,0
70,"Objective: To empirically investigate SE researchers' opinions on the credibility of blog posts, and identify criteria for evaluating blog posts.",0,1,1,0
71,"Method: We conduct an online survey of software engineering researchers (n=43), to gather opinions on blog-post credibility and credibility criteria.",0,1,0,0
72,Results: There is diversity of opinion.,0,0,0,1
73,"The majority of researchers provide a qualified response to the credibility of blog posts: essentially, it depends.",0,0,0,1
74,"Several credibility criteria are valued by researchers, such as Reasoning, Clarity of writing, Reporting empirical data and Reporting methods of data collection.",0,0,0,1
75,Approximately 60% of respondents thought the criteria generalised to other practitioner-and researcher--generated content.,0,0,0,1
76,"Conclusion: The survey constitutes the first empirical benchmark of the credibility of blog posts in SE research, and presents an initial set of criteria for evaluating the credibility of blog posts.",0,0,0,1
77,The study would benefit from independent replication and evaluation.,0,0,0,1
78,Securing information systems has become a high priority as our reliance on them increases.,1,0,0,0
79,"Global multi-billion dollar companies have their critical information regularly exposed, costing them money and impairing their users' privacy.",1,0,0,0
80,"To defend against security breaches, IDE-integrated plugins to detect and remove security vulnerabilities in the first place are being used more frequently.",1,0,0,0
81,More information about these plugins is needed in order to improve the state of the art within the field.,1,0,0,0
82,Five open-source IDE plugins which can identify and report vulnerabilities are evaluated.,0,0,1,0
83,"We evaluate and compare how many categories of vulnerabilities the plugins can detect, how well the plugins detect the vulnerabilities, and how user-friendly the output of the plugin is to the developers.",0,0,1,0
84,"Our results show that certain vulnerabilities such as injection and broken access control are vastly covered by most plugins, while others have been completely ignored.",0,0,0,1
85,"A discrepancy between the claimed and actually confirmed coverage of the plugins is discovered, underlining the importance of this research.",0,0,0,1
86,High false positive rate and obvious limitations in usability show that more work is needed before these plugins can be widely used and relied upon in a corporate setting.,0,0,0,1
87,Models describe a software system on an abstraction level higher than its actual implementation.,1,0,0,0
88,Recent research results show that bringing models and a running system closer together by establishing traceability links between recorded runtime events and corresponding model elements improves the analysis performance of human observers when assessing the behaviour of the running system.,1,0,0,0
89,"Despite these results, common techniques for analyzing runtime events are rarely integrated into the models that are used for assessing the system behaviour from a high-level perspective.",1,0,0,0
90,This paper presents a controlled experiment where model-integrated analysis facilities are compared with a more traditional analysis approach based on SQL queries to a system's database in terms of correctness and completion time of analysis tasks.,0,1,1,0
91,"The results show that model-integrated analyses allow analysts to give more correct answers to questions about the system behaviour, but provide no improvement of the time spent for completing the analysis tasks.",0,0,0,1
92,Mutation testing is a means to assess the effectiveness of a test suite and its outcome is considered more meaningful than code coverage metrics.,1,0,0,0
93,"However, despite several optimizations, mutation testing requires a significant computational effort and has not been widely adopted in industry.",1,0,0,0
94,"Therefore, we study in this paper whether test effectiveness can be approximated using a more light-weight approach.",0,0,1,0
95,We hypothesize that a test case is more likely to detect faults in methods that are close to the test case on the call stack than in methods that the test case accesses indirectly through many other methods.,0,1,0,0
96,"Based on this hypothesis, we propose the minimal stack distance between test case and method as a new test measure, which expresses how close any test case comes to a given method, and study its correlation with test effectiveness.",0,1,0,0
97,"We conducted an empirical study with 21 open-source projects, which comprise in total 1.8 million LOC, and show that a correlation exists between stack distance and test effectiveness.",0,1,0,0
98,The correlation reaches a strength up to 0.58.,0,0,0,1
99,We further show that a classifier using the minimal stack distance along with additional easily computable measures can predict the mutation testing result of a method with 92.9% precision and 93.4% recall.,0,0,0,1
100,"Hence, such a classifier can be taken into consideration as a light-weight alternative to mutation testing or as a preceding, less costly step to that.",0,0,0,1
101,"Background Examples of questionable statistical practice, when published in high quality software engineering (SE) journals, may lead to novice researchers adopting incorrect statistical practices.",1,0,0,0
102,Objective Our goal is to highlight issues contributing to poor statistical practice in human-centric SE experiments.,0,0,1,0
103,Method We reviewed the statistical analysis practices used in the 13 papers that reported families of human-centric SE experiments and were published in high quality journals.,0,1,0,0
104,Results Reviewed papers related to 45 experiments and involved a total of 1303 human participants.,0,0,0,1
105,We searched for issues that were related to questionable statistical practice that were found in more than one paper.,0,1,0,0
106,"We observed three types of bad practice: incorrect use of terminology, incorrect analysis of repeated measures designs, and post-hoc power testing.",0,0,0,1
107,"We also found two analysis practices (i.e., multiple testing and pre-testing for normality) where statisticians disagree about good practice.",0,0,0,1
108,"Conclusions Identified issues pose a problem because readers may expect the statistical methods used in papers published in top quality, peer-reviewed journals to be correct.",0,0,0,1
109,We explain why the practices are problematic and provide recommendations for improved practice.,0,0,0,1
110,Test automation is important in the software industry but self-assessment instruments for assessing its maturity are not sufficient.,1,0,0,0
111,The two objectives of this study are to synthesize what an organization should focus to assess its test automation; develop a self-assessment instrument (a survey) for assessing test automation maturity and scientifically evaluate it.,0,0,1,0
112,We carried out the study in four stages.,0,1,0,0
113,"First, a literature review of 25 sources was conducted.",0,1,0,0
114,"Second, the initial instrument was developed.",0,1,0,0
115,"Third, seven experts from five companies evaluated the initial instrument.",0,1,0,0
116,Content Validity Index and Cognitive Interview methods were used.,0,1,0,0
117,"Fourth, we revised the developed instrument.",0,1,0,0
118,Our contributions are as follows: (a) we collected practices mapped into 15 key areas that indicate where an organization should focus to assess its test automation; (b) we developed and evaluated a self-assessment instrument for assessing test automation maturity; (c) we discuss important topics such as response bias that threatens self-assessment instruments.,0,0,1,1
119,Our results help companies and researchers to understand and improve test automation practices and processes.,0,0,0,1
120,"Context: Different approaches exist for automated GUI testing of Android applications, each with its peculiarities, advantages, and drawbacks.",1,0,0,0
121,The most common are either based on the structure of the GUI or use visual recognition.,1,0,0,0
122,"Goal: In this paper, we present an empirical evaluation of two different GUI testing techniques with the use for each of a representative tool: (1) Visual GUI testing, with the use of EyeAutomate, and (2) Layout-based GUI testing, with the use of Espresso.",0,0,1,0
123,Method: We conducted an experiment with a population of 78 graduate students.,0,1,0,0
124,"The participants of the study were asked to create the same test suite for a popular, open-source Android app (Omni-Notes) with both the tools, and to answer a survey about their preference to the one or the other, and the perceived difficulties when developing the test scripts.",0,1,0,0
125,"Results: By analyzing the outcomes of the delivered test suites (in terms of number of test scripts delivered and ratio of working ones) and the answers to the survey, we found that the participants showed similar productivity with both the tools, but the test suites developed with EyeAutomate were of higher quality (in terms of correctly working test scripts).",0,0,0,1
126,"The participants expressed a slight preference towards the EyeAutomate testing tool, reflecting a general complexity of Layout-based techniques -- represented by Espresso -- and some obstacles that may make the identification of components of the GUI quite a long and laborious task.",0,0,0,1
127,Conclusions: The evidence we collected can provide useful hints for researchers aiming at making GUI testing techniques for mobile applications more usable and effective.,0,0,0,1
128,"This research investigates the evolution of object-oriented inheritance hierarchies in open source, Java systems.",0,0,1,0
129,"The paper contributes an understanding of how hierarchies, particularly large complex hierarchies, evolve in 'real world' systems.",0,0,1,0
130,It informs object-oriented design practices that aim to control or avoid these complicated design structures.,0,0,1,0
131,The study is based on a detailed analysis of 665 inheritance hierarchies drawn from a total of 262 versions of 10 open source systems.,0,1,0,0
132,"The research contributions include that: i) the majority of inheritance hierarchies are 'simple' in structure and remain that way throughout their lifetimes ii) the majority of hierarchies are stable in terms of size and shape throughout their lifetimes iii) there is a minority of large, complex, branching 'Subtree' hierarchies that continue to grow ever more complicated as the systems evolve iv) a detailed analysis of some of these larger hierarchies finds evidence of 'good' object-oriented design practices being used but also highlights the significant challenges involved in understanding and refactoring these complex structures.",0,0,0,1
133,There is clear evidence that some of the complex hierarchies are emphasising reuse while others appear focused on type inheritance.,0,0,0,1
134,"Although managing architectural assumptions can benefit software development in several aspects (e.g., reducing architectural misunderstanding and mismatch), the effort required is a key obstacle towards employing architectural assumption management in practice.",1,0,0,0
135,One potential solution is to apply agile practices in order to reduce this effort.,1,0,0,0
136,"To this end, we conducted a survey with 91 practitioners to investigate the possibility of integrating agile practices into architectural assumption management in industrial practice.",0,1,1,0
137,The results offer an overview of which agile practices can be integrated in architectural assumption management and how.,0,0,0,1
138,"Six agile practices were selected by more than half of the subjects: ""Backlog"", ""Iterative and Incremental Development"", ""Refactoring"", ""Continuous Integration"", ""Effective Communication"", and ""Just Enough Work"".",0,0,0,1
139,Twelve agile practices were further elaborated by the subjects regarding how they can be used in architectural assumption management.,0,0,0,1
140,"Based on the survey results, we developed a classification of agile practices for agile architectural assumption management, which can act as a reference for researchers and practitioners to employ certain agile practices in architectural assumption management.",0,0,0,1
141,Early identification of software modules that are likely to be faulty helps practitioners take timely actions to improve these modules' quality and reduce development costs in the remainder of the development process.,1,0,0,0
142,"To this end, module faultiness estimation models can be built at any point during development by using measures collected up to that time.",1,0,0,0
143,Models available in later phases are expected to be more accurate than those available in earlier phases.,1,0,0,0
144,"However, waiting until late in the development process may reduce the impact of the effectiveness and efficacy of any software quality improvement actions and increase their cost.",1,0,0,0
145,Our goal is to investigate to what extent using software code measures along with software design measures helps improve the accuracy of module faultiness estimation with respect to using software design measures alone.,0,0,1,0
146,"We built faultiness estimation models---by using Binary Logistic Regression, Naive Bayes, Support Vector Machines, and Decision Trees---for 54 datasets from the PROMISE repository.",0,1,0,0
147,These datasets contain design and code measures and faultiness data of software modules of real-life projects.,0,1,0,0
148,We compared the models built by using the code measures and design measures together against the models built by using design measures alone via a few accuracy indicators.,0,1,0,0
149,The results indicate that the models built by using code measures and design measures together are only slightly more accurate than the models built by using design measures alone.,0,0,0,1
150,Our analysis shows that measures that can be obtained during design can provide models that are almost as accurate as models that can be achieved in later development phases.,0,0,0,1
151,"This is good news for practitioners, who can start early ---hence cheaper and more effective---quality improvement initiatives based on fairly reliable models.",0,0,0,1
152,"In software engineering practice, evaluating and selecting the software testing tools that best fit the project at hand is an important and challenging task.",1,0,0,0
153,"In scientific studies of software engineering, practitioner evaluations and beliefs have recently gained interest, and some studies suggest that practitioners find beliefs of peers more credible than empirical evidence.",1,0,0,0
154,"To study how software practitioners evaluate testing tools, we applied online opinion surveys (n=89).",0,1,1,0
155,"We analyzed the reliability of the opinions utilizing Krippendorff's alpha, intra-class correlation coefficient (ICC), and coefficients of variation (CV).",0,1,0,0
156,Negative binomial regression was used to evaluate the effect of demographics.,0,1,0,0
157,We find that opinions towards a specific tool can be conflicting.,0,0,0,1
158,We show how increasing the number of respondents improves the reliability of the estimates measured with ICC.,0,0,0,1
159,"Our results indicate that on average, opinions from seven experts provide a moderate level of reliability.",0,0,0,1
160,"From demographics, we find that technical seniority leads to more negative evaluations.",0,0,0,1
161,"To improve the understanding, robustness, and impact of the findings, we need to conduct further studies by utilizing diverse sources and complementary methods.",0,0,0,1
162,Context: Stack Overflow is a popular community question and answer portal used by practitioners to solve problems during software development.,1,0,0,0
163,Developers can focus their attention on answers that have been accepted or where members have recorded high votes in judging good answers when searching for help.,1,0,0,0
164,"However, the latter mechanism (votes) can be unreliable, and there is currently no way to differentiate between an answer that is likely to be accepted and those that will not be accepted by looking at the answer's characteristics.",1,0,0,0
165,"Objective: In potentially providing a mechanism to identify acceptable answers, this study examines the features that distinguish an accepted answer from an unaccepted answer.",0,0,1,0
166,Methods: We studied the Stack Overflow dataset by analyzing questions and answers for the two most popular tags (Java and JavaScript).,0,1,0,0
167,"Our dataset comprised 249,588 posts drawn from 2014-2016.",0,1,0,0
168,"We use random forest and neural network models to predict accepted answers, and study the features with the highest predictive power in those two models.",0,1,0,0
169,"Results: Our findings reveal that the length of code in answers, reputation of users, similarity of the text between questions and answers, and the time lag between questions and answers have the highest predictive power for differentiating accepted and unaccepted answers.",0,0,0,1
170,Conclusion: Tools may leverage these findings in supporting developers and reducing the effort they must dedicate to searching for suitable answers on Stack Overflow.,0,0,0,1
171,Systematic Literature Reviews (SLRs) are increasingly popular to categorize and identify research gaps.,1,0,0,0
172,Current Qualitative Data Analysis Software (QDAS) lack of a common format.,1,0,0,0
173,"Yet, the result of a recent survey indicates that 71,4% of participants (expert SLR reviewers) are ready to share SLR artifacts in a common repository.",1,0,0,0
174,"On the road towards open coding-data repositories, this work looks into W3C's Open Annotation as the way to RDFized those coding data.",0,0,1,0
175,This paper rephrases coding practices as annotation practices where data is captured as W3C's Open Annotations.,0,0,0,1
176,"Deployability is proven by describing two clients on top of this repository: (1) a write client that populates the repository through a color-coding highlighter, and (2), a read client that obtains a traditional SLR spreadsheets by querying so-populated repositories.",0,1,0,0
177,"Context: Technical Debt (TD) quantification has been studied in the literature and is supported by various tools; however, there is no common ground on what information shall be presented to stakeholders.",1,0,0,0
178,"Similarly to other quality monitoring processes, it is desirable to provide several views of quality through a dashboard, in which metrics concerning the phenomenon of interest are displayed.",1,0,0,0
179,"Objective: The aim of this study is to investigate the indicators that shall be presented in such a dashboard, so as to: (a) be meaningful for industrial stakeholders, (b) present all necessary information, and (c) be simple enough so that stakeholders can use them.",0,0,1,0
180,"Method: We explore TD Management (TDM) activities (i.e., measurement, prioritization, repayment) and choose the main concepts that need to be visualized, based on existing literature and toolsupport.",0,1,0,0
181,"Results / Conclusions: The results of the study suggest that different stakeholders need a different view of the quality dashboard, but also some commonalities can be identified.",0,0,0,1
182,"For example, on the one hand, managers are mostly interested in financial concepts, whereas on the other hand developers are more interested in the nature of the problems that exist in the code.",0,0,0,1
183,"The outcomes of this study can be useful to both researchers and practitioners, in the sense that the former can focus their efforts on aspects that are meaningful to industry, whereas the latter to develop meaningful dashboards, with multiple views.",0,0,0,1
184,Developers have access to tools like Google Lighthouse to assess the performance of web apps and to guide the adoption of development best practices.,1,0,0,0
185,"However, when it comes to energy consumption of mobile web apps, these tools seem to be lacking.",1,0,0,0
186,This study investigates on the correlation between the performance scores produced by Lighthouse and the energy consumption of mobile web apps.,0,0,1,0
187,We design and conduct an empirical experiment where 21 real mobile web apps are (i) analyzed via the Lighthouse performance analysis tool and (ii) measured on an Android device running a software-based energy profiler.,0,1,0,0
188,"Then, we statistically assess how energy consumption correlates with the obtained performance scores and carry out an effect size estimation.",0,1,0,0
189,"We discover a statistically significant negative correlation between performance scores and the energy consumption of mobile web apps (with medium to large effect sizes), implying that an increase of the performance score tend to lead to a decrease of energy consumption.",0,0,0,1
190,"We recommend developers to strive to improve the performance level of their mobile web apps, as this can also have a positive impact on their energy consumption on Android devices.",0,0,0,1
191,Factors such as app stores or platform choices heavily affect functional and non-functional mobile app requirements.,1,0,0,0
192,We surveyed 45 companies and interviewed ten experts to explore how factors that impact mobile app requirements are understood by requirements engineers in the mobile app industry.,0,1,0,0
193,We observed the lack of knowledge in several areas.,0,0,0,1
194,"For instance, we observed that all practitioners were aware of data privacy concerns, however, they did not know that certain third-party libraries, usage aggregators, or advertising libraries also occasionally leak sensitive user data.",0,0,0,1
195,"Similarly, certain functional requirements may not be implementable in the absence of a third-party library that is either banned from an app store for policy violations or lacks features, for instance, missing desired features in ARKit library for iOS made practitioners turn to Android.",0,0,0,1
196,"We conclude that requirements engineers should have adequate technical experience with mobile app development as well as sufficient knowledge in areas such as privacy, security and law, in order to make informed decisions during requirements elicitation.",0,0,0,1
197,The software engineering industry is increasingly aware of the role and value of neurodiverse engineers within the workforce.,1,0,0,0
198,One motivation is the alignment between skills needed for software development and the processing strengths of individuals with autistic spectrum conditions.,1,0,0,0
199,"One aspect of neurodiversity is dyslexia, typically presenting in individuals through a range of reading deficiencies.",1,0,0,0
200,In this paper we build on recent work which has sought to investigate if programmers with dyslexia read program code in a way which is different from programmers without dyslexia.,0,0,1,0
201,The particular focus of this analysis is the nature of saccadic movement and patterns of linearity when reading code.,0,0,1,0
202,A study is presented in which the eye gaze of 28 programmers (14 with dyslexia and 14 without) was recorded using an eye tracking device while reading and understanding three on-screen Java programs.,0,1,0,0
203,"Using insights from the wider dyslexia literature, hypotheses are formulated to reflect the expected saccadic gaze behaviour of programmers with dyslexia.",0,1,0,0
204,A range of existing metrics for linearity of program reading are adapted and used for statistical analysis of the data.,0,1,0,0
205,Results are consistent with recent work elsewhere and indicate that programmers with dyslexia do not exhibit patterns of linearity significantly different from the control group.,0,0,0,1
206,Non-linear gaze is shown to be approximately 40% of all saccadic movement.,0,0,0,1
207,"Some preliminary insights are offered based on the data available, suggesting that the extent of non-linear reading when comprehending program code might complement the processing and problem solving style of the programmer with dyslexia.",0,0,0,1
208,"In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way.",1,0,0,0
209,"Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature.",1,0,0,0
210,Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature.,1,0,0,0
211,"Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis.",1,0,0,0
212,"In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms.",0,0,1,0
213,"To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset.",0,1,0,0
214,"Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset.",0,1,0,0
215,We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains.,0,1,0,0
216,"Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.",0,0,0,1
217,GitHub has become a precious service for storing and managing software source code.,1,0,0,0
218,"Over the last year, 10M new developers have joined the GitHub community, contributing to more than 44M repositories.",1,0,0,0
219,"In order to help developers increase the reachability of their repositories, in 2017 GitHub introduced the possibility to classify them by means of topics.",1,0,0,0
220,"However, assigning wrong topics to a given repository can compromise the possibility of helping other developers approach it, and thus preventing them from contributing to its development.",1,0,0,0
221,In this paper we investigate the application of Multinomial Naïve Bayesian (MNB) networks to automatically classify GitHub repositories.,0,0,1,0
222,"By analyzing the README file(s) of the repository to be classified and the source code implementing it, the conceived approach is able to recommend GitHub topics.",0,1,0,0
223,"To the best of our knowledge, this is the first supervised approach addressing the considered problem.",0,0,0,1
224,"Consequently, since there exists no suitable baseline for the comparison, we validated the approach by considering different metrics, aiming to study various quality aspects.",0,0,0,1
225,Background: Publication bias is the failure to publish the results of a study based on the direction or strength of the study findings.,1,0,0,0
226,The existence of publication bias is firmly established in areas like medical research.,1,0,0,0
227,Recent research suggests the existence of publication bias in Software Engineering.,1,0,0,0
228,Aims: Finding out whether experiments published in the International Workshop on Empirical Software Engineering and Measurement (ESEM) are affected by publication bias.,0,0,1,0
229,Method: We review experiments published in ESEM.,0,1,0,0
230,We also survey with experimental researchers to triangulate our findings.,0,1,0,0
231,Results: ESEM experiments do not define hypotheses and frequently perform multiple testing.,0,0,0,1
232,One-tailed tests have a slightly higher rate of achieving statistically significant results.,0,0,0,1
233,We could not find other practices associated with publication bias.,0,0,0,1
234,"Conclusions: Our results provide a more encouraging perspective of SE research than previous research: (1) ESEM publications do not seem to be strongly affected by biases and (2) we identify some practices that could be associated with p-hacking, but it is more likely that they are related to the conduction of exploratory research.",0,0,0,1
235,Context: There is considerable diversity in the range and design of computational experiments to assess classifiers for software defect prediction.,1,0,0,0
236,"This is particularly so, regarding the choice of classifier performance metrics.",1,0,0,0
237,"Unfortunately some widely used metrics are known to be biased, in particular F1.",1,0,0,0
238,Objective: We want to understand the extent to which the widespread use of the F1 renders empirical results in software defect prediction unreliable.,0,0,1,0
239,Method: We searched for defect prediction studies that report both F1 and the Matthews correlation coefficient (MCC).,0,1,0,0
240,This enabled us to determine the proportion of results that are consistent between both metrics and the proportion that change.,0,1,0,0
241,Results: Our systematic review identifies 8 studies comprising 4017 pairwise results.,0,0,0,1
242,"Of these results, the direction of the comparison changes in 23% of the cases when the unbiased MCC metric is employed.",0,0,0,1
243,"Conclusion: We find compelling reasons why the choice of classification performance metric matters, specifically the biased and misleading F1 metric should be deprecated.",0,0,0,1
244,"Context: Design pattern detection is a very important research on software reuse, which can greatly help software maintenance and reconstruction.",1,0,0,0
245,"At present, many researchers have invested in this work and proposed a variety of methods for detection.",1,0,0,0
246,Method: This paper extends the graph matching technology based on the past and proposes a new method that combines network analysis and structural matching for detection.,0,1,0,0
247,"First, we use network level analysis to obtain important nodes, then use the neighborhood path matching algorithm to match the pattern instance.",0,1,0,0
248,"Result: We describe the detection of five patterns on four open source systems, then analyze and compare with the other three methods, for achieving high precision and recall, which demonstrates that our method is effective.",0,0,0,1
249,"Conclusion: Using combining network analysis with structural matching can well detect these pattern instances, and these instances are also especially important for future software refactoring.",0,0,0,1
250,Spectrum-Based Fault Localization (SBFL) follows the basic intuitions that the faulty parts are more likely to be covered by failure-revealing test cases and less likely to be covered by passed test cases.,1,0,0,0
251,"However, due to the diversity of programs and faults, many other characteristics (related to program structure, test suites, and type of faulty components) will influence the practical application of SBFL.",1,0,0,0
252,"For example, a statement can be covered by numerous failure-revealing test cases, and also covered by numerous passed test cases.",1,0,0,0
253,"To get more indicators about the faulty components towards a better application of SBFL, we extend the scope of spectrum-based knowledge from the basic intuitions to the Characteristics of Spectra Distribution (CSDs for short).",0,0,1,0
254,"That is, we explore the relationships between different types of statements and their spectra.",0,0,1,0
255,"Firstly, we introduce the concepts of Failure-Independent, Failure-Related, and Failure-Exclusionary to describe the relationships between different types of statements and their executions.",0,0,0,1
256,"Then, we propose two probabilistic models, with and without the noise of fault interference, respectively, to identify various CSDs for each type of statements.",0,0,0,1
257,"As the analysis results, we introduce a visualization technique to generalize the identified CSDs and provide an overall picture of spectra distribution and its dynamics.",0,0,0,1
258,"Finally, based on our analysis and also the observation of the program spectra of current benchmarks, we design a technique to filter the potential non-faulty statements to improve the accuracy of SBFL.",0,0,0,1
259,"Context: With the growing popularity of rapid software delivery and deployment, the methods, practices and technologies of Continuous Software Engineering (CSE) are evolving steadily.",1,0,0,0
260,"This creates the need for understanding the recent trends of the technologies, practitioners' challenges and views in this domain.",1,0,0,0
261,"Objective: In this paper, we present an empirical study aimed at exploring CSE from the practitioners' perspective by mining discussions from Q&A websites.",0,0,1,0
262,"Method: We have analyzed 12,989 questions and answers posted on Stack Overflow.",0,1,0,0
263,Topic modelling is conducted to derive the dominant topics in this domain.,0,1,0,0
264,"Further, a qualitative analysis was conducted to identify the key challenges discussed.",0,1,0,0
265,"Findings: Whilst the trend of posted questions is sharply increasing, the questions are becoming more specific to technologies and more difficult to attract answers.",0,0,0,1
266,"We identified 32 topics of discussions, among which ""Error messages in Continuous Integration/Deployment"" and ""Continuous Integration concepts"" are the most dominant.",0,0,0,1
267,We also present the most challenging areas in this domain from the practitioners' perspectives.,0,0,0,1
268,Decisions run through the whole software development and maintenance processes.,1,0,0,0
269,"Explicitly documenting these decisions helps to organize development knowledge and to reduce its vaporization, thereby controlling the development process and maintenance costs.",1,0,0,0
270,It can also support the knowledge acquisition process for stakeholders of the project.,1,0,0,0
271,"Meanwhile, developers (e.g., architects) and managers will be able to rely on the decisions made in the past to solve the problems encountered in their current projects.",1,0,0,0
272,"However, identifying decisions from massive textual artifacts, which involves considerable human effort, time, and cost, is usually unaffordable due to limited resources.",1,0,0,0
273,"To address this problem, we conducted an experiment to automatically identify decisions from textual artifacts using machine learning techniques.",0,0,1,0
274,"We created a dataset of 1,300 sentences labelled from the Hibernate developer mailing list, containing 650 decision sentences and non-decision sentences respectively, and trained machine learning models using 160 configurations regarding text preprocessing, feature extraction, and classification algorithms.",0,1,0,0
275,"The results show that (1) the text preprocessing method with Including Stop Words, No Stemming and Lemmatization, and No Filtering Out Sentences performs best when preprocessing posts to identify decisions; (2) the simple Bag-of-Words (BoW) model works best when extracting features to identify decisions; (3) the Support Vector Machine (SVM) algorithm gets the best result when training classifiers to identify decisions; and (4) the SVM algorithm with Including Stop Words (ISW), No Stemming and Lemmatization (NSaL), Filtering Out Sentences by Length (FOSbL), and BoW achieves the best performance (with a precision of 0.640, a recall of 0.932, and an F1-score of 0.759), compared with other configurations when identifying decisions from the mailing list.",0,0,0,1
276,Context: The vast majority of software engineering research is reported independently of the application domain: techniques and tools usage is reported without any domain context.,1,0,0,0
277,"As reported in previous research, this has not always been so: early in the computing era, the research focus was frequently application domain specific (for example, scientific and data processing).",1,0,0,0
278,Objective: We believe determining the research context is often important.,0,0,1,0
279,"Therefore we propose a code-based approach to identify the application domain of a software system, via its lexicon.",0,0,1,0
280,We compare its use against the plain textual description attached to the same system.,0,0,1,0
281,"Method: Using a sample of 50 Java projects, we obtained i) the description of each project (e.g., its ReadMe file), ii) the lexicon extracted from its source code, and iii) a list of its main topics extracted with the Latent Dirichlet Allocation (LDA) modelling technique.",0,1,0,0
282,"We assigned a random subset of these data items to different researchers (i.e., 'experts'), and asked them to assign each item to one (or more) application domain.",0,1,0,0
283,We then evaluated the precision and accuracy of the three techniques.,0,1,0,0
284,"Results: Using the agreement levels between experts, We observed that the 'baseline' dataset (i.e., the ReadMe files) obtained the highest average in terms of agreement between experts, but we also observed that the three techniques had the same mode and median agreement levels.",0,0,0,1
285,"Additionally, in the cases where no agreement was reached for the baseline dataset, the two other techniques provided sufficient additional support.",0,0,0,1
286,"Conclusions: We conclude that the source code is sufficient for determining the application domain, so that classification is possible without special documentation requirements.",0,0,0,1
287,Many companies have turned towards globally distributed software development in their quest for access to more development capacity.,1,0,0,0
288,"This paper investigates how a company onboarded distributed teams in a global project, and report experience on how to study such distributed projects.",0,0,1,0
289,Onboarding is the process of helping new team members adapt to the existing team and ways of working.,1,0,0,0
290,The goal of the studied onboarding program was to integrate Portuguese developers into two existing Norwegian teams.,0,1,0,0
291,"Further, due to the growing trend in utilizing globally distributed projects, and the challenge of conducting studies in distributed organizations, it is crucial to find good practices for researching such projects.",1,0,0,0
292,"We collected qualitative data from interviews, observations, Slack conversations and documents, and quantitative data on Slack activity.",0,0,0,1
293,"We report experiences on different onboarding practices and techniques, and we suggest guidelines to help other researchers conduct qualitative studies in globally distributed projects.",0,0,0,1
294,Expanding abbreviations in source code to their full meanings is very useful for software maintainers to comprehend the source code.,1,0,0,0
295,"The existing approaches, however, focus on expanding an abbreviation to a single word, i.e., unigram.",1,0,0,0
296,They do not perform well when dealing with abbreviations of phrases that consist of multiple unigrams.,1,0,0,0
297,This paper proposes a bigram-based approach for retrieving abbreviated phrases automatically.,0,0,1,0
298,Key to this approach is a bigram-based inference model for choosing the best phrase from all candidates.,0,1,0,0
299,It utilizes the statistical properties of unigrams and bigrams as prior knowledge and a bigram language model for estimating the likelihood of each candidate phrase of a given abbreviation.,0,1,0,0
300,"We have applied the bigram-based approach to 100 phrase abbreviations, randomly selected from eight open source projects.",0,1,0,0
301,The experiment results show that it has correctly retrieved 78% of the abbreviations by using the unigram and bigram properties of a source code repository.,0,0,0,1
302,This is 9% more accurate than the unigram-based approach and much better than other existing approaches.,0,0,0,1
303,The bigram-based approach is also less biased towards specific phrase sizes than the unigram-based approach.,0,0,0,1
304,"There is a rapidly increasing amount of Artificial Intelligence (AI) systems developed in recent years, with much expectation on its capacity of innovation and business value generation.",1,0,0,0
305,"However, the promised value of AI systems in specific business contexts might not be understood, and further integrated into the development processes.",1,0,0,0
306,"We wanted to understand how software engineering processes and practices can be applied to develop AI systems in a fast-faced, business-driven manner.",0,0,1,0
307,"As the first step, we explored contextual factors of AI development and the connections between AI developments to business opportunities.",0,1,0,0
308,"We conducted 12 semi-structured interviews in seven companies in Brazil, Norway and Southeast Asia.",0,1,0,0
309,Our investigation revealed different types of AI systems and different AI development approaches.,0,0,0,1
310,"However, it is common that business opportunities involving with AI systems are not validated and there is lack of business-driven metrics that guide the development of AI systems.",0,0,0,1
311,The findings have implications for future research on business-driven AI development and supporting tools and practices.,0,0,0,1
312,Conclusions that are drawn from experiments are subject to varying degrees of uncertainty.,1,0,0,0
313,"For example, they might rely on small data sets, employ statistical techniques that make assumptions that are hard to verify, or there may be unknown confounding factors.",1,0,0,0
314,"In this paper we propose an alternative but complementary mechanism to explicitly incorporate these various sources of uncertainty into reasoning about empirical findings, by applying Subjective Logic.",0,0,1,0
315,"To do this we show how typical traditional results can be encoded as ""subjective opinions"" -- the building blocks of Subjective Logic.",0,1,0,0
316,We demonstrate the value of the approach by using Subjective Logic to aggregate empirical results from two large published studies that explore the relationship between programming languages and defects or failures.,0,0,0,1
317,Background: Machine Learning (ML) has been widely used as a powerful tool to support Software Engineering (SE).,1,0,0,0
318,The fundamental assumptions of data characteristics required for specific ML methods have to be carefully considered prior to their applications in SE.,1,0,0,0
319,"Within the context of Continuous Integration (CI) and Continuous Deployment (CD) practices, there are two vital characteristics of data prone to be violated in SE research.",1,0,0,0
320,"First, the logs generated during CI/CD for training are imbalanced data, which is contrary to the principles of common balanced classifiers; second, these logs are also time-series data, which violates the assumption of cross-validation.",1,0,0,0
321,Objective: We aim to systematically study the two data characteristics and further provide a comprehensive evaluation for predictive CI/CD with the data from real projects.,0,0,1,0
322,Method: We conduct an experimental study that evaluates 67 CI/CD predictive models using both cross-validation and time-series-validation.,0,1,0,0
323,"Results: Our evaluation shows that cross-validation makes the evaluation of the models optimistic in most cases, there are a few counter-examples as well.",0,0,0,1
324,"The performance of the top 10 imbalanced models are better than the balanced models in the predictions of failed builds, even for balanced data.",0,0,0,1
325,The degree of data imbalance has a negative impact on prediction performance.,0,0,0,1
326,"Conclusion: In research and practice, the assumptions of the various ML methods should be seriously considered for the validity of research.",0,0,0,1
327,"Even if it is used to compare the relative performance of models, cross-validation may not be applicable to the problems with time-series features.",0,0,0,1
328,The research community need to revisit the evaluation results reported in some existing research.,0,0,0,1
329,Context: Software testing plays an important role in assuring the reliability of systems.,1,0,0,0
330,Assessing the efficacy of testing remains challenging with few established test effectiveness metrics.,1,0,0,0
331,Those metrics that have been used (e.g. coverage and mutation analysis) have been criticised for insufficiently differentiating between the faults detected by tests.,1,0,0,0
332,Objective: We investigate how effective tests are at detecting different types of faults and whether some types of fault evade tests more than others.,0,0,1,0
333,Our aim is to suggest to developers specific ways in which their tests need to be improved to increase fault detection.,0,0,1,0
334,Method: We investigate seven fault types and analyse how often each goes undetected in 10 open source systems.,0,1,0,0
335,We statistically look for any relationship between the test set and faults.,0,1,0,0
336,"Results: Our results suggest that the fault detection rates of unit tests are relatively low, typically finding only about a half of all faults.",0,0,0,1
337,"In addition, conditional boundary and method call removals are less well detected by tests than other fault types.",0,0,0,1
338,Conclusions: We conclude that the testing of these open source systems needs to be improved across the board.,0,0,0,1
339,"In addition, despite boundary cases being long known to attract faults, tests covering boundaries need particular improvement.",0,0,0,1
340,"Overall, we recommend that developers do not rely only on code coverage and mutation score to measure the effectiveness of their tests.",0,0,0,1
341,Software testing is a crucial activity to check the internal quality of a software.,1,0,0,0
342,"During testing, developers often create tests for the normal behavior of a particular functionality (e.g., was this file properly uploaded to the cloud?).",1,0,0,0
343,"However, little is known whether developers also create tests for the exceptional behavior (e.g., what happens if the network fails during the file upload?).",1,0,0,0
344,"To minimize this knowledge gap, in this paper we design and perform a mixed-method study to understand how 417 open source Java projects are testing the exceptional behavior using the JUnit and TestNG frameworks, and the AssertJ library.",0,0,1,0
345,We found that 254 (60.91%) projects have at least one test method dedicated to test the exceptional behavior.,0,0,0,1
346,We also found that the number of test methods for exceptional behavior with respect to the total number of test methods lies between 0% and 10% in 317 (76.02%) projects.,0,0,0,1
347,"Also, 239 (57.31%) projects test only up to 10% of the used exceptions in the System Under Test (SUT).",0,0,0,1
348,"When it comes to mobile apps, we found that, in general, developers pay less attention to exceptional behavior tests when compared to desktop/server and multi-platform developers.",0,0,0,1
349,"In general, we found more test methods covering custom exceptions (the ones created in the own project) when compared to standard exceptions available in the Java Development Kit (JDK) or in third-party libraries.",0,0,0,1
350,"To triangulate the results, we conduct a survey with 66 developers from the projects we study.",0,1,0,0
351,"In general, the survey results confirm our findings.",0,0,0,1
352,"In particular, the majority of the respondents agrees that developers often neglect exceptional behavior tests.",0,0,0,1
353,"As implications, our numbers might be important to alert developers that more effort should be placed on creating tests for the exceptional behavior.",0,0,0,1
354,"Driven by the need for faster time-to-market and reduced development lead-time, large-scale systems engineering companies are adopting agile methods in their organizations.",1,0,0,0
355,This agile transformation is challenging and it is common that adoption starts bottom-up with agile software teams within the context of traditional company structures.,1,0,0,0
356,This creates the challenge of agile teams working within a document-centric and plan-driven (or waterfall) environment.,1,0,0,0
357,"While it may be desirable to take the best of both worlds, it is not clear how that can be achieved especially with respect to managing requirements in large-scale systems.",1,0,0,0
358,This paper presents an exploratory case study focusing on two departments of a large-scale systems engineering company (automotive) that is in the process of company-wide agile adoption.,0,1,0,0
359,We present challenges that agile teams face while working within a larger plan-driven context and propose potential strategies to mitigate the challenges.,0,0,0,1
360,"Challenges relate to, e.g., development teams not being aware of the high-level requirements, difficulties to manage change of these requirements as well as their relationship to backlog items such as user stories.",0,0,0,1
361,"While we found strategies for solving most of the challenges, they remain abstract and empirical research on their effectiveness is currently lacking.",0,0,0,1
362,"Background: Practitioners would like to take action based on software metrics, as long as they find them reliable.",1,0,0,0
363,"Existing literature explores how metrics can be made reliable, but remains unclear if there are other conditions necessary for a metric to be actionable.",1,0,0,0
364,"Context & Method: In the context of a European H2020 Project, we conducted a multiple case study to study metrics' use in four companies, and identified instances where these metrics influenced actions.",0,1,0,0
365,We used an online questionnaire to enquire about the project participants' views on actionable metrics.,0,1,0,0
366,"Next, we invited one participant from each company to elaborate on the identified metrics' use for taking actions and the questionnaire responses (N=17).",0,1,0,0
367,"Result: We learned that a metric that is practical, contextual, and exhibits high data quality characteristics is actionable.",0,0,0,1
368,"Even a non-actionable metric can be useful, but an actionable metric mostly requires interpretation.",0,0,0,1
369,"However, the more these metrics are simple and reflect the software development context accurately, the less interpretation required to infer actionable information from the metric.",0,0,0,1
370,Company size and project characteristics can also influence the type of metric that can be actionable.,0,0,0,1
371,Conclusion: This exploration of industry's views on actionable metrics help characterize actionable metrics in practical terms.,0,0,0,1
372,This awareness of what characteristics constitute an actionable metric can facilitate their definition and development right from the start of a software metrics program.,0,0,0,1
373,Context: Quality requirements (QRs) have a significant role in the success of software projects.,1,0,0,0
374,"In agile software development (ASD), where working software is valued over comprehensive documentation, QRs are often under-specified or not documented.",1,0,0,0
375,"Consequently, they may be handled improperly and result in degraded software quality and increased maintenance costs.",1,0,0,0
376,"Investigating the documentation of QRs in ASD, would provide evidence on existing practices, tools and aspects considered in ASD that other practitioners might utilize to improve documentation and management of QRs in ASD.",1,0,0,0
377,"Although there are some studies examining documentation in ASD, those that specifically investigate the documentation of QRs in depth are lacking.",1,0,0,0
378,"Method: we conducted a multiple case study by interviewing 15 practitioners of four ASD cases, to provide empirical evidence on documentation of QRs in ASD.",0,1,0,0
379,"We also run workshops with two of the cases, to identify important aspects that ASD practitioners consider when documenting QRs in requirements management repositories.",0,1,0,0
380,Result and conclusions: ASD companies approach documentation of QRs to fit the needs of their context.,0,0,0,1
381,"They used tools, backlogs, iterative prototypes, and artifacts such as epic, and stories to document QRs, or utilized face-face communication without documenting QRs.",0,0,0,1
382,"We observed that documentation of QRs in ASD is affected by factors such as context (e.g. product domain, and size) and the experience of practitioners.",0,0,0,1
383,"Some tools used to document QRs also enhanced customer collaboration, enabling customers report and document QRs.",0,0,0,1
384,"Aspects such as levels of abstraction, the traceability of QRs, optimal details of information of QRs and verification and validation are deemed important when documenting QRs in ASD requirements management repositories.",0,0,0,1
385,"Modelling is a fundamental activity in software engineering, which is often performed in collaboration.",1,0,0,0
386,"For this purpose, on-line tools running on the cloud are frequently used.",1,0,0,0
387,"However, recent advances in Natural Language Processing have fostered the emergence of chatbots, which are increasingly used for all sorts of software engineering tasks, including modelling.",1,0,0,0
388,"To evaluate to what extent chatbots are suitable for collaborative modelling, we conducted an experimental study with 54 participants, to evaluate the usability of a modelling chatbot called SOCIO, comparing it with the on-line tool Creately.",0,0,1,0
389,We employed a within-subjects cross-over design of 2 sequences and 2 periods.,0,1,0,0
390,"Usability was determined by attributes of efficiency, effectiveness, satisfaction and quality of the results.",0,1,0,0
391,We found that SOCIO saved time and reduced communication effort over Creately.,0,0,0,1
392,"SOCIO satisfied users to a greater extent than Creately, while in effectiveness results were similar.",0,0,0,1
393,"With respect to diagram quality, SOCIO outperformed Creately in terms of precision, while solutions with Creately had better recall and perceived success.",0,0,0,1
394,"However, in terms of accuracy and error scores, both tools were similar.",0,0,0,1
395,Pull requests are a method to facilitate review and management of contribution in distributed software development.,1,0,0,0
396,"Software developers author commits, and present them in a pull request to be inspected by maintainers and reviewers.",1,0,0,0
397,"The success and sustainability of communities depends on ongoing contributions, but rejections decrease motivation of contributors.",1,0,0,0
398,We carried out a a qualitative study to understand the mechanisms of evaluating PRs in open source software (FOSS) communities from developers and maintainers perspective.,0,0,1,0
399,We interviewed 30 participants from five different FOSS communities.,0,1,0,0
400,"The data shows that acceptance of contributions depends not only on technical criteria, but also significantly on social and strategic aspects.",0,0,0,1
401,"This paper identifies three PR governance styles found in the studied communities: (1) protective, (2) equitable and (3) lenient.",0,0,0,1
402,Each one of these styles has its particularities.,0,0,0,1
403,"While the protective style values trustworthiness and reliability of the contributor, the lenient style believes in creating a positive and welcoming environment where contributors are mentored to evolve contributions until they meet the community standards.",0,0,0,1
404,"Despite the differences, these governance styles have a commonality, they all safeguard the quality of the software.",0,0,0,1
405,Background: Using design metrics to predict fault-prone elements of a software design can help to focus attention on classes that need redesign and more extensive testing.,1,0,0,0
406,"However, some design metrics have been pointed out to be theoretically invalid, and the usefulness of some metrics is questioned.",1,0,0,0
407,"Aim: To identify a set of object-oriented metrics that are theoretically valid, and useful for identifying fault-prone classes in a design.",0,0,1,0
408,"Method: Drawing on four well-known sets of design metrics (CK, LK, MOOD and QMOOD), we propose a consolidated set of metrics that covers many aspects of object-oriented software design.",0,1,0,0
409,"We conduct two experiments, first using a single large system and then considering successive releases of that system, to compare the usefulness of the consolidated set with the other four sets for within-project prediction of fault-prone classes.",0,1,0,0
410,"Results: Both experiments suggest the consolidated set is effective at identifying fault-prone classes, outperforming the other metric sets (though at a cost of more false alarms).",0,0,0,1
411,"Conclusion: This paper adds to knowledge about the usefulness of existing sets of design metrics for within-project defect prediction, and identifies a consolidated set of metrics that is more effective than the existing sets at identifying fault-prone classes.",0,0,0,1
412,Background: Little is known about the practices used for technical debt (TD) payment.,1,0,0,0
413,"The study of payment practices, as well as the reasons for not applying them, can help practitioners to control and manage TD items.",1,0,0,0
414,"Aims: To investigate, from the point of view of software practitioners, if TD items have been paid off in software projects, the practices that have been used to pay off TD and the reasons that hamper the implementation of these practices.",0,0,1,0
415,"Method: We analyzed - both quantitatively and qualitatively - a corpus of responses from a survey of 432 practitioners, from four countries, about the possibility of TD payment.",0,1,0,0
416,"Results: We found that, for most of the cases, TD items have not been eliminated from software projects.",0,0,0,1
417,"The main reasons for not paying off TD are lack of organizational interest, low priority on the debt, focus on short-term goals, cost, and lack of time.",0,0,0,1
418,"On the other hand, we identified that code refactoring, design refactoring, and update system documentation are the most used practices for TD payment.",0,0,0,1
419,"Practitioners also cited practices related to the prevention, prioritization, and creation of a favorable setting as part of TD payment initiatives.",0,0,0,1
420,Conclusion: This paper summarizes the identified practices and reasons for not paying off debt items in a map.,0,0,0,1
421,Our map reveals that the majority of payment practices are of a technical nature while the majority of reasons for not paying off debts are associated with non-technical issues.,0,0,0,1
422,Open source software (OSS) communities are often able to produce high quality software comparable to proprietary software.,1,0,0,0
423,"The success of an open source software development (OSSD) community is often attributed to the underlying governance model, and a key component of these models is the decision-making (DM) process.",1,0,0,0
424,"While there have been studies on the decision-making processes publicized by OSS communities (e.g., through published process diagrams), little has been done to study decision-making processes that can be extracted using a bottom-up, data-driven approach, which can then be used to assess whether the publicized processes conform to the extracted processes.",1,0,0,0
425,"To bridge this gap, we undertook a large-scale data-driven study to understand how decisions are made in an OSSD community, using the case study of Python Enhancement Proposals (PEPs), which embody decisions made during the evolution of the Python language.",0,1,0,0
426,"Our main contributions are:

(a) the design and development of a framework using information retrieval and natural language processing techniques to analyze the Python email archives (comprising 1.48 million emails), and

(b) the extraction of decision-making processes that reveal activities that are neither explicitly mentioned in documentation published by the Python community nor identified in prior research work.",0,0,1,0
427,Our results provide insights into the actual decision-making process employed by the Python community.,0,0,0,1
428,"
Traceability plays an essential role in assuring that software and systems are safe to use.",1,0,0,0
429,Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links.,1,0,0,0
430,"To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants.",0,0,1,0
431,"A mutant is killed if its model checking fails; otherwise, it is survived.",0,0,1,0
432,"We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links.",0,0,1,0
433,Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art.,0,0,0,1
434,"
Software developers use a mix of source code and natural language text to communicate with each other: Stack Overflow and Developer mailing lists abound with this mixed text.",1,0,0,0
435,"Tagging this mixed text is essential for making progress on two seminal software engineering problems --- traceability, and reuse via precise extraction of code snippets from mixed text.",1,0,0,0
436,"In this paper, we borrow code-switching techniques from Natural Language Processing and adapt them to apply to mixed text to solve two problems: language identification and token tagging.",0,0,1,0
437,"Our technique, POSIT, simultaneously provides abstract syntax tree tags for source code tokens, part-of-speech tags for natural language words, and predicts the source language of a token in mixed text.",0,0,1,0
438,"To realize POSIT, we trained a biLSTM network with a Conditional Random Field output layer using abstract syntax tree tags from the CLANG compiler and part-of-speech tags from the Standard Stanford part-of-speech tagger.",0,1,0,0
439,POSIT improves the state-of-the-art on language identification by 10.6% and PoS/AST tagging by 23.7% in accuracy.,0,0,0,1
440,"
Static analyses have problems modelling dynamic language features soundly while retaining acceptable precision.",1,0,0,0
441,"The problem is well-understood in theory, but there is little evidence on how this impacts the analysis of real-world programs.",1,0,0,0
442,"We have studied this issue for call graph construction on a set of 31 real-world Java programs using an oracle of actual program behaviour recorded from executions of built-in and synthesised test cases with high coverage, have measured the recall that is being achieved by various static analysis algorithms and configurations, and investigated which language features lead to static analysis false negatives.",0,0,1,0
443,"We report that (1) the median recall is 0.884 suggesting that standard static analyses have significant gaps with respect to the proportion of the program modelled (2) built-in tests are significantly better to expose dynamic program behaviour than synthesised tests (3) adding precision to the static analysis has little impact on recall indicating that those are separate concerns (4) state-of-the-art support for dynamic language features can significantly improve recall (the median observed is 0.935), but it comes with a hefty performance penalty, and (5) the main sources of unsoundness are not reflective method invocations, but objects allocated or accessed via native methods, and invocations initiated by the JVM, without matching call sites in the program under analysis.",0,0,0,1
444,These results provide some novel insights into the interaction between static and dynamic program analyses that can be used to assess the utility of static analysis results and to guide the development of future static and hybrid analyses.,0,0,0,1
445,"
Although deep neural networks (DNNs) have demonstrated astonishing performance in many applications, there are still concerns on their dependability.",1,0,0,0
446,"One desirable property of DNN for applications with societal impact is fairness (i.e., non-discrimination).",1,0,0,0
447,"In this work, we propose a scalable approach for searching individual discriminatory instances of DNN.",0,0,1,0
448,"Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which makes it significantly more scalable than existing methods.",0,0,1,0
449,Experimental results show that our approach explores the search space more effectively (9 times) and generates much more individual discriminatory instances (25 times) using much less time (half to 1/7).,0,0,0,1
450,"
During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality.",1,0,0,0
451,"Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing.",1,0,0,0
452,"Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects.",1,0,0,0
453,This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects.,0,0,1,0
454,"Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft.",0,0,1,0
455,"We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects.",0,0,0,1
456,"Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls.",0,0,1,0
457,Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78% without empirically affecting the frequency of their flaky-test failures.,0,1,0,0
458,"Lastly, our study finds several cases where developers claim they ""fixed"" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures.",0,1,0,0
459,"Future studies should be more cautious when basing their results on changes that developers claim to be ""fixes"".",0,0,0,1
460,"
Metamodels play a significant role to describe and analyze the relations between domain concepts.",1,0,0,0
461,They are also cornerstone to build a software language (SL) for a domain and its associated tooling.,1,0,0,0
462,Metamodel definition generally drives code generation of a core API.,1,0,0,0
463,"The latter is further enriched by developers with additional code implementing advanced functionalities, e.g., checkers, recommenders, etc. When a SL is evolved to the next version, the metamodels are evolved as well before to re-generate the core API code.",1,0,0,0
464,"As a result, the developers added code both in the core API and the SL toolings may be impacted and thus may need to be co-evolved accordingly.",1,0,0,0
465,Many approaches support the co-evolution of various artifacts when metamodels evolve.,1,0,0,0
466,"However, not the co-evolution of code.",1,0,0,0
467,This paper fills this gap.,0,0,1,0
468,We propose a semi-automatic co-evolution approach based on change propagation.,0,0,1,0
469,The premise is that knowledge of the metamodel evolution changes can be propagated by means of resolutions to drive the code co-evolution.,0,0,1,0
470,Our approach leverages on the abstraction level of metamodels where a given metamodel element has often different usages in the code.,0,0,1,0
471,It supports alternative co-evaluations to meet different developers needs.,0,0,1,0
472,"Our work is evaluated on three Eclipse SL implementations, namely OCL, Modisco, and Papyrus over several evolved versions of metamodels and code.",0,1,0,0
473,"In response to five different evolved metamodels, we co-evolved 976 impacts over 18 projects.A comparison of our co-evolved code with the versioned ones shows the usefulness of our approach.",0,0,0,1
474,Our approach was able to reach a weighted average of 87.4% and 88.9% respectively of precision and recall while supporting useful alternative co-evolution that developers have manually performed.,0,0,0,1
475,"
Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects.",1,0,0,0
476,Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers.,1,0,0,0
477,"In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impacton the development process.",0,0,1,0
478,"We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR.",0,1,0,0
479,We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%.,0,0,0,1
480,We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%.,0,0,0,1
481,"We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk.",0,0,1,0
482,"In this way, we are able to simultaneously increase expertise during review with a ΔExpertise of 6%, with a negligible impact on workload of ΔCoreWorkload of 0.09%, and reduce the files at risk by ΔFaR -28%.",0,0,0,1
483,"Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or ""learner"" based on the context of the review.",0,0,0,1
484,We release the Sofia bot as well as the code and data for replication purposes.,0,0,0,1
485,"
Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research.",1,0,0,0
486,"Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research.",1,0,0,0
487,Method: We used a mixed-methods approach for this research.,0,1,0,0
488,"We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research.",0,1,0,0
489,"Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL.",0,0,0,1
490,We also obtained 20 replies from the GL users and 24 replies from the invited SE experts.,0,0,0,1
491,Conclusion: There is no common understanding of the meaning of GL in SE.,0,0,0,1
492,Researchers define the scopes and the definitions of GL in a variety of ways.,0,0,0,1
493,We found five main reasons of using GL in SE research.,0,0,0,1
494,The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle.,0,0,0,1
495,There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL.,0,0,0,1
496,"The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research.",0,0,0,1
497,"
Performance is one of the important aspects of software quality.",1,0,0,0
498,"Performance issues exist widely in software systems, and the process of fixing the performance issues is an essential step in the release cycle of software systems.",1,0,0,0
499,"Although performance testing is widely adopted in practice, it is still expensive and time-consuming.",1,0,0,0
500,"In particular, the performance testing is usually conducted after the system is built in a dedicated testing environment.",1,0,0,0
501,The challenges of performance testing make it difficult to fit into the common DevOps process in software development.,1,0,0,0
502,"On the other hand, there exist a large number of tests readily available, that are executed regularly within the release pipeline during software development.",1,0,0,0
503,"In this paper, we perform an exploratory study to determine whether such readily available tests are capable of serving as performance tests.",0,0,1,0
504,"In particular, we would like to see whether the performance of these tests can demonstrate performance improvements obtained from fixing real-life performance issues.",0,0,1,0
505,"We collect 127 performance issues from Hadoop and Cassandra, and evaluate the performance of the readily available tests from the commits before and after the performance issue fixes.",0,1,0,0
506,We find that most of the improvements from the fixes to performance issues can be demonstrated using the readily available tests in the release pipeline.,0,0,0,1
507,"However, only a very small portion of the tests can be used for demonstrating the improvements.",0,0,0,1
508,"By manually examining the tests, we identify eight reasons that a test cannot demonstrate performance improvements even though it covers the changed source code of the issue fix.",0,0,0,1
509,"Finally, we build random forest classifiers determining the important metrics influencing the readily available tests (not) being able to demonstrate performance improvements from issue fixes.",0,0,0,1
510,"We find that the test code itself and the source code covered by the test are important factors, while the factors related to the code changes in the performance issues fixes have a low importance.",0,0,0,1
511,"Practitioners may focus on designing and improving the tests, instead of fine-tuning tests for different performance issues fixes.",0,0,0,1
512,Our findings can be used as a guideline for practitioners to reduce the amount of effort spent on leveraging and designing tests that run in the release pipeline for performance assurance activities.,0,0,0,1
513,"
Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications.",1,0,0,0
514,RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values.,1,0,0,0
515,It is well known that problematic word embeddings can lead to low model accuracy.,1,0,0,0
516,"In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples.",0,0,1,0
517,We then leverage the diagnosis results as guidance to harden/repair the embeddings.,0,0,1,0
518,"Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.",0,0,0,1
519,"
Mobile apps are an integral component of our daily life.",1,0,0,0
520,"Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities.",1,0,0,0
521,This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives.,0,0,1,0
522,"First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps.",0,1,0,0
523,"We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people.",0,0,0,1
524,We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues.,0,1,0,0
525,"We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility.",0,0,0,1
526,We finally investigate user ratings and comments on app stores.,0,1,0,0
527,"We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps.",0,0,0,1
528,We conclude the paper with several observations that form the foundation for future research and development.,0,0,0,1
529,"
Existing coverage-based fuzzers usually use the individual control flow graph (CFG) edge coverage to guide the fuzzing process, which has shown great potential in finding vulnerabilities.",1,0,0,0
530,"However, CFG edge coverage is not effective in discovering vulnerabilities such as use-after-free (UaF).",1,0,0,0
531,"This is because, to trigger UaF vulnerabilities, one needs not only to cover individual edges, but also to traverse some (long) sequence of edges in a particular order, which is challenging for existing fuzzers.",1,0,0,0
532,"To this end, we propose to model UaF vulnerabilities as typestate properties, and develop a typestate-guided fuzzer, named UAFL, for discovering vulnerabilities violating typestate properties.",0,0,1,0
533,"Given a typestate property, we first perform a static typestate analysis to find operation sequences potentially violating the property.",0,1,0,0
534,Our fuzzing process is then guided by the operation sequences in order to progressively generate test cases triggering property violations.,0,1,0,0
535,"In addition, we also employ an information flow analysis to improve the efficiency of the fuzzing process.",0,1,0,0
536,We have performed a thorough evaluation of UAFL on 14 widely-used real-world programs.,0,1,0,0
537,"The experiment results show that UAFL substantially outperforms the state-of-the-art fuzzers, including AFL, AFLFast, FairFuzz, MOpt, Angora and QSYM, in terms of the time taken to discover vulnerabilities.",0,0,0,1
538,"We have discovered 10 previously unknown vulnerabilities, and received 5 new CVEs.",0,0,0,1
539,"
Merge conflicts are inevitable in collaborative software development and are disruptive.",1,0,0,0
540,"When they occur, developers have to stop their current work, understand the conflict and the surrounding code, and plan an appropriate resolution.",1,0,0,0
541,"However, not all conflicts are equally problematic---some can be easily fixed, while others might be complicated enough to need multiple people.",1,0,0,0
542,"Currently, there is not much support to help developers plan their conflict resolution.",1,0,0,0
543,"In this work, we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution.",0,0,1,0
544,The ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts.,1,0,0,0
545,"In this work, we investigate the characteristics of difficult merge conflicts, and automatically classify them.",0,0,1,0
546,"We analyzed 6,380 conflicts across 128 java projects and found that merge conflict difficulty can be accurately predicted (AUC of 0.76) through machine learning algorithms, such as bagging.",0,1,0,1
547,"
Generating high-quality system call sequences is not only important to testing file system implementations, but also challenging due to the astronomically large input space.",1,0,0,0
548,This paper introduces a new approach to the workload generation problem by building layered models and abstract workloads refinement.,0,0,1,0
549,This approach is instantiated as a three-layer file system model for file system workload generation.,0,0,1,0
550,"In a short-period experiment run, sequential workloads (system call sequences) manifested over a thousand crashes in mainline Linux Kernel file systems, with 12 previously unknown bugs being reported.",0,1,0,0
551,We also provide evidence that such workloads benefit other domain-specific testing techniques including crash consistency testing and concurrency testing.,0,0,0,1
552,"
Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs.",1,0,0,0
553,"Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs.",1,0,0,0
554,What challenges should automated repair tools address?,1,0,0,0
555,What are the repair patterns whose automation could help developers?,1,0,0,0
556,Which repair patterns should be assigned a higher priority for building automated bug repair tools?,1,0,0,0
557,This work presents a comprehensive study of bug fix patterns to address these questions.,0,0,1,0
558,"We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns.",0,1,0,0
559,"Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs.",0,0,0,1
560,"We also contribute a benchmark of 667 DNN (bug, repair) instances.",0,0,0,1
561,"
Modern static analyzers often need to simultaneously check a few dozen or even hundreds of value-flow properties, causing serious scalability issues when high precision is required.",1,0,0,0
562,"A major factor to this deficiency, as we observe, is that the core static analysis engine is oblivious of the mutual synergy among the properties being checked, thus inevitably losing many optimization opportunities.",1,0,0,0
563,Our work is to leverage the inter-property awareness and to capture redundancies and inconsistencies when many properties are considered at the same time.,0,0,1,0
564,We have evaluated our approach by checking twenty value-flow properties in standard benchmark programs and ten real-world software systems.,0,1,0,0
565,The results demonstrate that our approach is more than 8× faster than existing ones but consumes only 1/7 of the memory.,0,0,0,1
566,"Such substantial improvement in analysis efficiency is not achieved by sacrificing the effectiveness: at the time of writing, thirty-nine bugs found by our approach have been fixed by developers and four of them have been assigned CVE IDs due to their security impact.",0,0,0,1
567,"
This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems.",0,0,1,0
568,TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles).,0,0,1,0
569,"It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies.",0,0,1,0
570,"Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations.",0,1,0,1
571,"With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs.",0,1,0,1
572,Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer.,0,0,0,1
573,Grey-box repair fixes 30% bugs on average for Transformer.,0,0,0,1
574,"Manual inspection indicates that the translations repaired by our approach improve consistency in 87% of cases (degrading it in 2%), and that our repairs have better translation acceptability in 27% of the cases (worse in 8%).",0,0,0,1
575,"
In contemporary code review, the comments put by reviewers on a specific code change are immediately visible to the other reviewers involved.",1,0,0,0
576,"Could this visibility prime new reviewers' attention (due to the human's proneness to availability bias), thus biasing the code review outcome?",1,0,0,0
577,"In this study, we investigate this topic by conducting a controlled experiment with 85 developers who perform a code review and a psychological experiment.",0,1,1,0
578,"With the psychological experiment, we find that ≈70% of participants are prone to availability bias.",0,1,0,1
579,"However, when it comes to the code review, our experiment results show that participants are primed only when the existing code review comment is about a type of bug that is not normally considered; when this comment is visible, participants are more likely to find another occurrence of this type of bug.",0,0,0,1
580,"Moreover, this priming effect does not influence reviewers' likelihood of detecting other types of bugs.",0,0,0,1
581,"Our findings suggest that the current code review practice is effective because existing review comments about bugs in code changes are not negative primers, rather positive reminders for bugs that would otherwise be overlooked during code review.",0,0,0,1
582,"
Many data-driven software engineering tasks such as discovering programming patterns, mining API specifications, etc., perform source code analysis over control flow graphs (CFGs) at scale.",1,0,0,0
583,Analyzing millions of CFGs can be expensive and performance of the analysis heavily depends on the underlying CFG traversal strategy.,1,0,0,0
584,State-of-the-art analysis frameworks use a fixed traversal strategy.,1,0,0,0
585,We argue that a single traversal strategy does not fit all kinds of analyses and CFGs and propose bespoke control flow analysis (BCFA).,0,0,1,0
586,"Given a control flow analysis (CFA) and a large number of CFGs, BCFA selects the most efficient traversal strategy for each CFG.",1,0,0,0
587,"BCFA extracts a set of properties of the CFA by analyzing the code of the CFA and combines it with properties of the CFG, such as branching factor and cyclicity, for selecting the optimal traversal strategy.",0,0,0,1
588,"We have implemented BCFA in Boa, and evaluated BCFA using a set of representative static analyses that mainly involve traversing CFGs and two large datasets containing 287 thousand and 162 million CFGs.",0,1,0,0
589,Our results show that BCFA can speedup the large scale analyses by 1%-28%.,0,0,0,1
590,"Further, BCFA has low overheads; less than 0.2%, and low misprediction rate; less than 0.01%.",0,0,0,1
591,"
Grey-box fuzzing is an evolutionary process, which maintains and evolves a population of test cases with the help of a fitness function.",1,0,0,0
592,Fitness functions used by current grey-box fuzzers are not informative in that they cannot distinguish different program executions as long as those executions achieve the same coverage.,1,0,0,0
593,"The problem is that current fitness functions only consider a union of data, but not their combination.",1,0,0,0
594,"As such, fuzzers often get stuck in a local optimum during their search.",1,0,0,0
595,"In this paper, we introduce Ankou, the first grey-box fuzzer that recognizes different combinations of execution information, and present several scalability challenges encountered while designing and implementing Ankou.",0,0,1,0
596,"Our experimental results show that Ankou is 1.94× and 8.0× more effective in finding bugs than AFL and Angora, respectively.",0,0,0,1
597,"
Bottom-up program analysis has been traditionally easy to parallelize because functions without caller-callee relations can be analyzed independently.",1,0,0,0
598,"However, such function-level parallelism is significantly limited by the calling dependence - functions with caller-callee relations have to be analyzed sequentially because the analysis of a function depends on the analysis results, a.k.a., function summaries, of its callees.",1,0,0,0
599,"We observe that the calling dependence can be relaxed in many cases and, as a result, the parallelism can be improved.",1,0,0,0
600,"In this paper, we present Coyote, a framework of bottom-up data flow analysis, in which the analysis task of each function is elaborately partitioned into multiple sub-tasks to generate pipelineable function summaries.",0,0,1,0
601,"These sub-tasks are pipelined and run in parallel, even though the calling dependence exists.",0,0,1,0
602,We formalize our idea under the IFDS/IDE framework and have implemented an application to checking null-dereference bugs and taint issues in C/C++ programs.,0,0,1,0
603,"We evaluate Coyote on a series of standard benchmark programs and open-source software systems, which demonstrates significant speedup over a conventional parallel design.",0,1,0,1
604,"
Static analysis is a proven technique for catching bugs during software development.",1,0,0,0
605,"However, analysis tooling must approximate, both theoretically and in the interest of practicality.",1,0,0,0
606,False positives are a pervading manifestation of such approximations---tool configuration and customization is therefore crucial for usability and directing analysis behavior.,1,0,0,0
607,"To suppress false positives, developers readily disable bug checks or insert comments that suppress spurious bug reports.",1,0,0,0
608,Existing work shows that these mechanisms fall short of developer needs and present a significant pain point for using or adopting analyses.,1,0,0,0
609,We draw on the insight that an analysis user always has one notable ability to influence analysis behavior regardless of analyzer options and implementation: modifying their program.,1,0,0,0
610,"We present a new technique for automated, generic, and temporary code changes that tailor to suppress spurious analysis errors.",0,0,1,0
611,"We adopt a rule-based approach where simple, declarative templates describe general syntactic changes for code patterns that are known to be problematic for the analyzer.",0,1,0,0
612,Our technique promotes program transformation as a general primitive for improving the fidelity of analysis reports (we treat any given analyzer as a black box).,0,1,0,0
613,"We evaluate using five different static analyzers supporting three different languages (C, Java, and PHP) on large, real world programs (up to 800KLOC).",0,1,0,0
614,We show that our approach is effective in sidestepping long-standing and complex issues in analysis implementations.,0,0,0,1
615,"
As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers.",1,0,0,0
616,"To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms.",1,0,0,0
617,"As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt.",1,0,0,0
618,"However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them.",1,0,0,0
619,"In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist).",0,1,1,0
620,"Actually, it is in general very challenging to determine if developers have put in counter-measure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair.",1,0,0,0
621,"To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques.",0,0,1,0
622,Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively.,0,0,0,1
623,"With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.",0,0,0,1
624,"
Message passing is the standard paradigm of programming in high-performance computing.",1,0,0,0
625,"However, verifying Message Passing Interface (MPI) programs is challenging, due to the complex program features (such as non-determinism and non-blocking operations).",1,0,0,0
626,"In this work, we present MPI symbolic verifier (MPI-SV), the first symbolic execution based tool for automatically verifying MPI programs with non-blocking operations.",0,0,1,0
627,MPI-SV combines symbolic execution and model checking in a synergistic way to tackle the challenges in MPI program verification.,0,0,1,0
628,The synergy improves the scalability and enlarges the scope of verifiable properties.,0,0,0,1
629,We have implemented MPI-SV1 and evaluated it with 111 real-world MPI verification tasks.,0,1,0,0
630,"The pure symbolic execution-based technique successfully verifies 61 out of the 111 tasks (55%) within one hour, while in comparison, MPI-SV verifies 100 tasks (90%).",0,0,0,1
631,"On average, compared with pure symbolic execution, MPI-SV achieves 19x speedups on verifying the satisfaction of the critical property and 5x speedups on finding violations.",0,0,0,1
632,"
Identifying and optimizing open participation is essential to the success of open software development.",1,0,0,0
633,Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to detect more bugs with fewer workers.,1,0,0,0
634,"However, these studies mainly focus on one-time recommendations with respect to the initial context at the beginning of a new task.",1,0,0,0
635,This paper argues the need for in-process crowdtesting worker recommendation.,0,0,1,0
636,"We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task.",0,1,0,0
637,"This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened.",0,0,0,1
638,"To that end, this paper proposes a context-aware in-process crowdworker recommendation approach, iRec, to detect more bugs earlier and potentially shorten the non-yielding windows.",0,0,0,1
639,"It consists of three main components: 1) the modeling of dynamic testing context, 2) the learning-based ranking component, and 3) the diversity-based re-ranking component.",0,0,0,1
640,"The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec in improving the cost-effectiveness of crowdtesting by saving the cost and shortening the testing process.",0,1,0,1
641,"
The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance.",1,0,0,0
642,In this paper we introduce a large taxonomy of faults in deep learning (DL) systems.,0,0,1,0
643,"We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts.",0,1,0,0
644,Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources.,0,1,0,0
645,"Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.",0,1,0,1
646,"
Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs.",1,0,0,0
647,"Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures.",1,0,0,0
648,"However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts.",1,0,0,0
649,"In this paper, we illustrate how these limitations can be overcome through the use of a tailored probabilistic model.",0,0,1,0
650,"To this end, we design and implement a HierarchiCal PrObabilistic Model for SoftwarE Traceability (Comet) that is able to infer candidate trace links.",0,0,1,0
651,Comet is capable of modeling relationships between artifacts by combining the complementary observational prowess of multiple measures of textual similarity.,0,0,1,0
652,"Additionally, our model can holistically incorporate information from a diverse set of sources, including developer feedback and transitive (often implicit) relationships among groups of software artifacts, to improve inference accuracy.",0,0,1,0
653,We conduct a comprehensive empirical evaluation of Comet that illustrates an improvement over a set of optimally configured baselines of ≈14% in the best case and ≈5% across all subjects in terms of average precision.,0,1,0,0
654,"The comparative effectiveness of Comet in practice, where optimal configuration is typically not possible, is likely to be higher.",0,0,0,1
655,"Finally, we illustrate Comet's potential for practical applicability in a survey with developers from Cisco Systems who used a prototype Comet Jenkins plugin.",0,0,0,1
656,"
API usage directives in official API documentation describe the contracts, constraints and guidelines for using APIs in natural language.",1,0,0,0
657,"Through the investigation of API misuse scenarios on Stack Overflow, we identify three barriers that hinder the understanding of the API usage directives, i.e., lack of specific usage context, indirect relationships to cooperative APIs, and confusing APIs with subtle differences.",1,0,0,0
658,"To overcome these barriers, we develop a text mining approach to discover the crowdsourced API misuse scenarios on Stack Overflow and extract from these scenarios erroneous code examples and patches, as well as related API and confusing APIs to construct demystification reports to help developers understand the official API usage directives described in natural language.",0,0,1,0
659,We apply our approach to API usage directives in official Android API documentation and android-tagged discussion threads on Stack Overflow.,0,1,0,0
660,"We extract 159,116 API misuse scenarios for 23,969 API usage directives of 3138 classes and 7471 methods, from which we generate the demystification reports.",0,1,0,0
661,Our manual examination confirms that the extracted information in the generated demystification reports are of high accuracy.,0,0,0,1
662,"By a user study of 14 developers on 8 API-misuse related error scenarios, we show that our demystification reports help developer understand and debug API-misuse related program errors faster and more accurately, compared with reading only plain API usage-directive sentences.",0,0,0,1
663,"
Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration.",1,0,0,0
664,"A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate.",1,0,0,0
665,"Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale.",1,0,0,0
666,"In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python).",0,0,1,0
667,"To our knowledge, these are the largest NLMs for code that have been reported.",1,0,0,0
668,"All datasets, code, and trained models used in this work are publicly available.",0,0,0,1
669,"
Timing side channels arise in software when a program's execution time can be correlated with security-sensitive program input.",1,0,0,0
670,Recent results on software side-channel detection focus on analysis of program's source code.,1,0,0,0
671,"However, runtime behavior, in particular optimizations introduced during just-in-time (JIT) compilation, can impact or even introduce timing side channels in programs.",1,0,0,0
672,"In this paper, we present a technique for automatically detecting such JIT-induced timing side channels in Java programs.",0,0,1,0
673,We first introduce patterns to detect partitions of secret input potentially separable by side channels.,0,0,1,0
674,Then we present an automated approach for exploring behaviors of the Java Virtual Machine (JVM) to identify states where timing channels separating these partitions arise.,0,0,1,0
675,We evaluate our technique on three datasets used in recent work on side-channel detection.,0,1,0,0
676,"We find that many code variants labeled ""safe"" with respect to side-channel vulnerabilities are in fact vulnerable to JIT-induced timing side channels.",0,0,0,1
677,Our results directly contradict the conclusions of four separate state-of-the-art program analysis tools for side-channel detection and demonstrate that JIT-induced side channels are prevalent and can be detected automatically.,0,0,0,1
678,"
To battle the ever-increasing Android malware, malware family classification, which classifies malware with common features into a malware family, has been proposed as an effective malware analysis method.",1,0,0,0
679,Several machine-learning based approaches have been proposed for the task of malware family classification.,1,0,0,0
680,"Our study shows that malware families suffer from several data imbalance, with many families with only a small number of malware applications (referred to as few shot malware families in this work).",0,0,1,0
681,"Unfortunately, this issue has been overlooked in existing approaches.",1,0,0,0
682,"Although existing approaches achieve high classification performance at the overall level and for large malware families, our experiments show that they suffer from poor performance and generalizability for few shot malware families, and traditionally downsampling method cannot solve the problem.",0,0,0,1
683,"To address the challenge in few shot malware family classification, we propose a novel siamese-network based learning method, which allows us to train an effective MultiLayer Perceptron (MLP) network for embedding malware applications into a real-valued, continuous vector space by contrasting the malware applications from the same or different families.",0,0,1,0
684,"In the embedding space, the performance of malware family classification can be significantly improved for all scales of malware families, especially for few shot malware families, which also leads to the significant performance improvement at the overall level.",0,0,0,1
685,"
Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code.",1,0,0,0
686,"Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets.",1,0,0,0
687,Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks.,1,0,0,0
688,The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones.,1,0,0,0
689,"In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set.",0,0,1,0
690,Our approach can take advantages of both neural and retrieval-based techniques.,0,0,1,0
691,"Specifically, we first train an attentional encoder-decoder model based on the code snippets and the summaries in the training set; Second, given one input code snippet for testing, we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics, respectively; Third, we encode the input and two retrieved code snippets, and predict the summary by fusing them during decoding.",0,1,0,0
692,We conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state-of-the-art methods.,0,1,0,1
693,"
In object-oriented languages, constructors often have a combination of required and optional formal parameters.",1,0,0,0
694,It is tedious and inconvenient for programmers to write a constructor by hand for each combination.,1,0,0,0
695,"The multitude of constructors is error-prone for clients, and client code is difficult to read due to the large number of constructor arguments.",1,0,0,0
696,"Therefore, programmers often use design patterns that enable more flexible object construction---the builder pattern, dependency injection, or factory methods.",1,0,0,0
697,"However, these design patterns can be too flexible: not all combinations of logical parameters lead to the construction of well-formed objects.",1,0,0,0
698,"When a client uses the builder pattern to construct an object, the compiler does not check that a valid set of values was provided.",1,0,0,0
699,"Incorrect use of builders can lead to security vulnerabilities, run-time crashes, and other problems.",1,0,0,0
700,"This work shows how to statically verify uses of object construction, such as the builder pattern.",0,0,1,0
701,"Using a simple specification language, programmers specify which combinations of logical arguments are permitted.",0,0,0,1
702,Our compile-time analysis detects client code that may construct objects unsafely.,0,0,0,1
703,"Our analysis is based on a novel special case of typestate checking, accumulation analysis, that modularly reasons about accumulations of method calls.",0,1,0,0
704,"Because accumulation analysis does not require precise aliasing information for soundness, our analysis scales to industrial programs.",0,1,0,0
705,"We evaluated it on over 9 million lines of code, discovering defects which included previously-unknown security vulnerabilities and potential null-pointer violations in heavily-used open-source codebases.",0,1,0,0
706,Our analysis has a low false positive rate and low annotation burden.,0,0,0,1
707,Our implementation and experimental data are publicly available.,0,0,0,1
708,"
Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, specification inference, and code synthesis.",1,0,0,0
709,"However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks.",1,0,0,0
710,"In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments.",0,0,1,0
711,"For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments.",0,0,1,0
712,"Developers usually comment on different aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties.",1,0,0,0
713,"To more effectively utilize comments, a fine-grained and elaborated taxonomy of comments and a reliable classifier to automatically categorize a comment are needed.",1,0,0,0
714,"In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments.",0,0,1,0
715,"We develop a prototype CPC, and evaluate it on 5 projects.",0,1,0,0
716,The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy.,0,0,0,1
717,"Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code.",0,0,0,1
718,"Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been confirmed and fixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments.",0,0,0,1
719,This demonstrates the effectiveness of our approach.,0,0,0,1
720,Our user study confirms propagated comments align well with existing comments in terms of quality.,0,1,0,1
721,"
Application programming interfaces (APIs) have become ubiquitous in software development.",1,0,0,0
722,"However, external APIs are not guaranteed to contain every desirable feature, nor are they immune to software defects.",1,0,0,0
723,"Therefore, API users will sometimes be faced with situations where a current API does not satisfy all of their requirements, but migrating to another API is costly.",1,0,0,0
724,"In these cases, due to the lack of communication channels between API developers and users, API users may intentionally bypass an existing API after inquiring into workarounds for their API problems with online communities.",1,0,0,0
725,"This mechanism takes the API developer out of the conversation, potentially leaving API defects unreported and desirable API features undiscovered.",1,0,0,0
726,In this paper we explore API workaround inquiries from API users on Stack Overflow.,0,0,1,0
727,"We uncover general reasons why API users inquire about API workarounds, and general solutions to API workaround requests.",0,0,1,0
728,"Furthermore, using workaround implementations in Stack Overflow answers, we develop three API workaround implementation patterns.",0,0,1,1
729,We identify instances of these patterns in real-life open source projects and determine their value for API developers from their responses to feature requests based on the identified API workarounds.,0,0,0,1
730,"
While CUDA has become a mainstream parallel computing platform and programming model for general-purpose GPU computing, how to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem.",1,0,0,0
731,"In this paper, we propose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs.",0,0,1,0
732,"To evaluate the effectiveness and efficiency of Simulee, we construct a benchmark with 7 popular CUDA-related projects from GitHub, upon which we conduct an extensive set of experiments.",0,1,0,0
733,"The experimental results suggest that Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers.",0,0,0,1
734,"Furthermore, Simulee significantly outperforms state-of-the-art approaches for CUDA synchronization bug detection.",0,0,0,1
735,"
Self-adaptive systems often employ dynamic programming or similar techniques to select optimal adaptations at run-time.",1,0,0,0
736,"These techniques suffer from the ""curse of dimensionality"", increasing the cost of run-time adaptation decisions.",1,0,0,0
737,We propose a novel approach that improves upon the state-of-the-art proactive self-adaptation techniques to reduce the number of possible adaptations that need be considered for each run-time adaptation decision.,0,0,1,0
738,"The approach, realized in a tool called Thallium, employs a combination of automated formal modeling techniques to (i) analyze a structural model of the system showing which configurations are reachable from other configurations and (ii) compute the utility that can be generated by the optimal adaptation over a bounded horizon in both the best- and worst-case scenarios.",0,0,1,0
739,"It then constructs triangular possibility values using those optimized bounds to automatically compare adjacent adaptations for each configuration, keeping only the alternatives with the best range of potential results.",0,0,1,0
740,"The experimental results corroborate Thallium's ability to significantly reduce the number of states that need to be considered with each adaptation decision, freeing up vital resources at run-time.",0,0,0,1
741,"
Mobile banking apps, belonging to the most security-critical app category, render massive and dynamic transactions susceptible to security risks.",1,0,0,0
742,"Given huge potential financial loss caused by vulnerabilities, existing research lacks a comprehensive empirical study on the security risks of global banking apps to provide useful insights and improve the security of banking apps.",1,0,0,0
743,"Since data-related weaknesses in banking apps are critical and may directly cause serious financial loss, this paper first revisits the state-of-the-art available tools and finds that they have limited capability in identifying data-related security weaknesses of banking apps.",0,0,1,0
744,"To complement the capability of existing tools in data-related weakness detection, we propose a three-phase automated security risk assessment system, named Ausera, which leverages static program analysis techniques and sensitive keyword identification.",0,0,1,0
745,"By leveraging Ausera, we collect 2,157 weaknesses in 693 real-world banking apps across 83 countries, which we use as a basis to conduct a comprehensive empirical study from different aspects, such as global distribution and weakness evolution during version updates.",0,1,0,0
746,We find that apps owned by subsidiary banks are always less secure than or equivalent to those owned by parent banks.,0,0,0,1
747,"In addition, we also track the patching of weaknesses and receive much positive feedback from banking entities so as to improve the security of banking apps in practice.",0,1,0,0
748,We further find that weaknesses derived from outdated versions of banking apps or third-party libraries are highly prone to being exploited by attackers.,0,0,0,1
749,"To date, we highlight that 21 banks have confirmed the weaknesses we reported (including 126 weaknesses in total).",0,0,0,1
750,"We also exchange insights with 7 banks, such as HSBC in UK and OCBC in Singapore, via in-person or online meetings to help them improve their apps.",0,1,0,0
751,"We hope that the insights developed in this paper will inform the communities about the gaps among multiple stakeholders, including banks, academic researchers, and third-party security companies.",0,0,0,1
752,"
Floating point is widely used in software to emulate arithmetic over reals.",1,0,0,0
753,"Unfortunately, floating point leads to rounding errors that propagate and accumulate during execution.",1,0,0,0
754,Generating inputs to maximize the numerical error is critical when evaluating the accuracy of floating-point code.,1,0,0,0
755,"In this paper, we formulate the problem of generating high error-inducing floating-point inputs as a code coverage maximization problem solved using symbolic execution.",0,0,1,0
756,"Specifically, we define inaccuracy checks to detect large precision loss and cancellation.",0,0,1,0
757,"We inject these checks at strategic program locations to construct specialized branches that, when covered by a given input, are likely to lead to large errors in the result.",0,1,0,0
758,"We apply symbolic execution to generate inputs that exercise these specialized branches, and describe optimizations that make our approach practical.",0,1,0,0
759,We implement a tool named FPGen and present an evaluation on 21 numerical programs including matrix computation and statistics libraries.,0,1,0,0
760,"We show that FPGen exposes errors for 20 of these programs and triggers errors that are, on average, over 2 orders of magnitude larger than the state of the art.",0,0,0,1
761,"
Software testing is an essential part of the software lifecycle and requires a substantial amount of time and effort.",1,0,0,0
762,It has been estimated that software developers spend close to 50% of their time on testing the code they write.,1,0,0,0
763,"For these reasons, a long standing goal within the research community is to (partially) automate software testing.",1,0,0,0
764,"While several techniques and tools have been proposed to automatically generate test methods, recent work has criticized the quality and usefulness of the assert statements they generate.",1,0,0,0
765,"Given a test method and a focal method (i.e., the main method under test), Atlas can predict a meaningful assert statement to assess the correctness of the focal method.",0,0,1,0
766,We applied Atlas to thousands of test methods from GitHub projects and it was able to predict the exact assert statement manually written by developers in 31% of the cases when only considering the top-1 predicted assert.,0,1,0,0
767,"When considering the top-5 predicted assert statements, Atlas is able to predict exact matches in 50% of the cases.",0,0,0,1
768,"These promising results hint to the potential usefulness of our approach as (i) a complement to automatic test case generation techniques, and (ii) a code completion support for developers, who can benefit from the recommended assert statements while writing test code.",0,0,0,1
769,"
Multithreaded programs can have deadlocks, even after deployment, so users may want to run deadlock tools on deployed programs.",1,0,0,0
770,"However, current deadlock predictors such as MagicLock and UnDead have large overheads that make them impractical for end-user deployment and confine their use to development time.",1,0,0,0
771,Such overhead stems from running an exponential-time algorithm on a large execution trace.,1,0,0,0
772,"In this paper, we present the first low-overhead deadlock predictor, called AirLock, that is fit for both in-house testing and deployed programs.",0,0,1,0
773,"AirLock maintains a small predictive lock reachability graph, searches the graph for cycles, and runs an exponential-time algorithm only for each cycle.",0,0,1,0
774,This approach lets AirLock find the same deadlocks as MagicLock and UnDead but with much less overhead because the number of cycles is small in practice.,0,0,1,0
775,"Our experiments with real-world benchmarks show that the average time overhead of AirLock is 3.5%, which is three orders of magnitude less than that of MagicLock and UnDead.",0,1,0,1
776,AirLock's low overhead makes it suitable for use with fuzz testers like AFL and on-the-fly after deployment.,0,0,0,1
777,"
Mobile application (app) developers commonly utilize analytic services to analyze their app users' behavior to support debugging, improve service quality, and facilitate advertising.",1,0,0,0
778,"Anonymization and aggregation can reduce the sensitivity of such behavioral data, therefore analytic services often encourage the use of such protections.",1,0,0,0
779,"However, these protections are not directly enforced so it is possible for developers to misconfigure the analytic services and expose personal information, which may cause greater privacy risks.",1,0,0,0
780,"Since people use apps in many aspects of their daily lives, such misconfigurations may lead to the leaking of sensitive personal information such as a users' real-time location, health data, or dating preferences.",1,0,0,0
781,"To study this issue and identify potential privacy risks due to such misconfigurations, we developed a semi-automated approach, Privacy-Aware Analytics Misconfiguration Detector (PAMDroid), which enables our empirical study on mis-configurations of analytic services.",0,0,1,0
782,"This paper describes a study of 1,000 popular apps using top analytic services in which we found misconfigurations in 120 apps.",0,1,0,0
783,"In 52 of the 120 apps, misconfigurations lead to a violation of either the analytic service providers' terms of service or the app's own privacy policy.",0,0,0,1
784,"
Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data.",1,0,0,0
785,"This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from.",1,0,0,0
786,"Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair.",1,0,0,0
787,"Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness.",0,0,1,0
788,Our technique casts the DNN data augmentation problem as an optimization problem.,0,0,1,0
789,"It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances.",0,0,1,0
790,"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.",0,1,0,0
791,"Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average.",0,0,0,1
792,"Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.",0,0,0,1
793,"
SMT solvers are at the basis of many applications, such as program verification, program synthesis, and test case generation.",1,0,0,0
794,"For all these applications to provide reliable results, SMT solvers must answer queries correctly.",1,0,0,0
795,"However, since they are complex, highly-optimized software systems, ensuring their correctness is challenging.",1,0,0,0
796,"In particular, state-of-the-art testing techniques do not reliably detect when an SMT solver is unsound.",1,0,0,0
797,"In this paper, we present an automatic approach for generating test cases that reveal soundness errors in the implementations of string solvers, as well as potential completeness and performance issues.",0,0,1,0
798,We synthesize input formulas that are satisfiable or unsatisfiable by construction and use this ground truth as test oracle.,0,0,1,0
799,"We automatically apply satisfiability-preserving transformations to generate increasingly-complex formulas, which allows us to detect many errors with simple inputs and, thus, facilitates debugging.",0,0,1,0
800,"The experimental evaluation shows that our technique effectively reveals bugs in the implementation of widely-used SMT solvers and applies also to other types of solvers, such as automata-based solvers.",0,1,0,1
801,"We focus on strings here, but our approach carries over to other theories and their combinations.",0,0,0,1
802,"
Property-based testing is a popular approach for validating the logic of a program.",1,0,0,0
803,An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver.,1,0,0,0
804,"However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs.",1,0,0,0
805,Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver.,1,0,0,0
806,"However, collecting such information reduces the speed at which tests can be executed.",1,0,0,0
807,"In this paper, we propose and study a black-box approach for generating valid test inputs.",0,0,1,0
808,We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs.,0,0,1,0
809,This formalization highlights the role of a guide which governs the space of choices within a random input generator.,0,0,1,0
810,"We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator.",0,0,1,0
811,"We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks.",0,1,0,0
812,"We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.",0,0,0,1
813,"
Highly-configurable software systems can have thousands of interdependent configuration options across different subsystems.",1,0,0,0
814,"In the resulting configuration space, discovering a valid product configuration for some selected options can be complex and error prone.",1,0,0,0
815,"The configuration space can be organized using a feature model, fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem.",1,0,0,0
816,We propose a method for lazy product discovery in large fragmented feature models with interdependent features.,0,0,1,0
817,We formalize the method and prove its soundness and completeness.,0,0,1,0
818,The evaluation explores an industrial-size configuration space.,0,0,1,0
819,"The results show that lazy product discovery has significant performance benefits compared to standard product discovery, which in contrast to our method requires all fragments to be composed to analyze the feature model.",0,0,0,1
820,"Furthermore, the method succeeds when more efficient, heuristics-based engines fail to find a valid configuration.",0,0,0,1
821,"
Data scientists frequently analyze data by writing scripts.",1,0,0,0
822,"We conducted a contextual inquiry with interdisciplinary researchers, which revealed that parameter tuning is a highly iterative process and that debugging is time-consuming.",0,1,0,0
823,"As analysis scripts evolve and become more complex, analysts have difficulty conceptualizing their workflow.",1,0,0,0
824,"In particular, after editing a script, it becomes difficult to determine precisely which code blocks depend on the edit.",1,0,0,0
825,"Consequently, scientists frequently re-run entire scripts instead of re-running only the necessary parts.",1,0,0,0
826,"We present ProvBuild, a tool that leverages language-level provenance to streamline the debugging process by reducing programmer cognitive load and decreasing subsequent runtimes, leading to an overall reduction in elapsed debugging time.",0,0,1,0
827,ProvBuild uses provenance to track dependencies in a script.,0,0,1,0
828,"When an analyst debugs a script, ProvBuild generates a simplifed script that contains only the information necessary to debug a particular problem.",0,0,1,0
829,We demonstrate that debugging the simplified script lowers a programmer's cognitive load and permits faster re-execution when testing changes.,0,0,0,1
830,The combination of reduced cognitive load and shorter runtime reduces the time necessary to debug a script.,0,0,0,1
831,"We quantitatively and qualitatively show that even though ProvBuild introduces overhead during a script's first execution, it is a more efficient way for users to debug and tune complex workflows.",0,0,0,1
832,"ProvBuild demonstrates a novel use of language-level provenance, in which it is used to proactively improve programmer productively rather than merely providing a way to retroactively gain insight into a body of code.",0,0,0,1
833,"
Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains.",1,0,0,0
834,The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success.,1,0,0,0
835,"However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances.",1,0,0,0
836,We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable.,0,0,1,0
837,We present an approach to testing image classifier robustness based on class property violations.,0,0,1,0
838,We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others.,0,0,0,1
839,These bugs usually violate some class properties of one or more of those classes.,0,0,0,1
840,"Most DNN testing techniques focus on perimage violations, so fail to detect class-level confusions or biases.",1,0,0,0
841,We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software.,0,0,0,1
842,"We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg.",0,0,0,1
843,"72.6%) for confusion errors, and up to 84.3% (avg.",0,0,0,1
844,66.8%) for bias errors.,0,0,0,1
845,"DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.",0,0,0,1
846,"
CPU cache is a limited but crucial storage component in modern processors, whereas the cache timing side-channel may inadvertently leak information through the physically measurable timing variance.",1,0,0,0
847,"Speculative execution, an essential processor optimization, and a source of such variances, can cause severe detriment on deliberate branch mispredictions.",1,0,0,0
848,"Despite static analysis could qualitatively verify the timing-leakage-free property under speculative execution, it is incapable of producing endorsements including inputs and speculated flows to diagnose leaks in depth.",1,0,0,0
849,"This work proposes a new symbolic execution based method, SpecuSym, for precisely detecting cache timing leaks introduced by speculative execution.",0,0,1,0
850,"Given a program (leakage-free in non-speculative execution), SpecuSym systematically explores the program state space, models speculative behavior at conditional branches, and accumulates the cache side effects along with subsequent path explorations.",0,0,1,0
851,"During the dynamic execution, SpecuSym constructs leak predicates for memory visits according to the specified cache model and conducts a constraint-solving based cache behavior analysis to inspect the new cache behaviors.",0,0,1,0
852,We have implemented SpecuSym atop KLEE and evaluated it against 15 open-source benchmarks.,0,1,0,0
853,Experimental results show that SpecuSym successfully detected from 2 to 61 leaks in 6 programs under 3 different cache settings and identified false positives in 2 programs reported by recent work.,0,0,0,1
854,"
Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs.",1,0,0,0
855,"While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them.",1,0,0,0
856,"Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour---the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern.",1,0,0,0
857,"The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision.",0,0,1,0
858,"We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions).",0,1,0,0
859,"We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers.",0,0,0,1
860,These indicate a shallow understanding of the underlying technology that empower such systems.,0,0,0,1
861,We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.,0,0,0,1
862,"
Deep learning has made significant achievements in many application areas.",1,0,0,0
863,"To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform.",1,0,0,0
864,"However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.",1,0,0,0
865,This paper presents the first comprehensive empirical study on program failures of deep learning jobs.,0,0,1,0
866,4960 real failures are collected from a deep learning platform in Microsoft.,0,1,0,0
867,We manually examine their failure messages and classify them into 20 categories.,0,1,0,0
868,"In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures.",0,0,0,1
869,"To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews.",0,1,0,0
870,"Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools.",0,0,0,1
871,"Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.",0,0,0,1
872,"
Open Source Software (OSS) has come to play a critical role in the software industry.",1,0,0,0
873,"Some large ecosystems enjoy the participation of large numbers of companies, each of which has its own focus and goals.",1,0,0,0
874,"Indeed, companies that otherwise compete, may become collaborators within the OSS ecosystem they participate in.",1,0,0,0
875,"Prior research has largely focused on commercial involvement in OSS projects, but there is a scarcity of research focusing on company collaborations within OSS ecosystems.",1,0,0,0
876,"Some of these ecosystems have become critical building blocks for organizations worldwide; hence, a clear understanding of how companies collaborate within large ecosystems is essential.",1,0,0,0
877,"This paper presents the results of an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions.",0,0,1,0
878,"Based on a detailed analysis, we identify clusters of collaborations, and identify four strategies that companies adopt to engage with the OpenStack ecosystem.",0,0,0,1
879,"We alsofind that companies may engage in intentional or passive collaborations, or may work in an isolated fashion.",0,0,0,1
880,"Further, wefi nd that a company's position in the collaboration network is positively associated with its productivity in OpenStack.",0,0,0,1
881,"Our study sheds light on how large OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem.",0,0,0,1
882,"
Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging.",1,0,0,0
883,"While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development.",1,0,0,0
884,"With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source.",0,0,1,0
885,"Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes.",0,0,0,1
886,"We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project.",0,0,0,1
887,"In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.",0,0,0,1
888,"
The assessment of information flows is an essential part of analyzing Android apps, and is frequently supported by static taint analysis.",1,0,0,0
889,"Its precision, however, can suffer from the analysis not being able to precisely determine what elements a pointer can (and cannot) point to.",1,0,0,0
890,"Recent advances in static analysis suggest that incorporating dynamic heap snapshots, taken at one point at runtime, can significantly improve general static analysis.",1,0,0,0
891,"In this paper, we investigate to what extent this also holds for taint analysis, and how various design decisions, such as when and how many snapshots are collected during execution, and how exactly they are used, impact soundness and precision.",0,0,1,0
892,"We have extended FlowDroid to incorporate heap snapshots, yielding our prototype Heapster, and evaluated it on DroidMacroBench, a novel benchmark comprising real-world Android apps that we also make available as an artifact.",0,1,0,0
893,"The results show (1) the use of heap snapshots lowers analysis time and memory consumption while increasing precision; (2) a very good trade-off between precision and recall is achieved by a mixed mode in which the analysis falls back to static points-to relations for objects for which no dynamic data was recorded; and (3) while a single heap snapshot (ideally taken at the end of the execution) suffices to improve performance and precision, a better trade-off can be obtained by using multiple snapshots.",0,0,0,1
894,"
Automatic test generation typically aims to generate inputs that explore new paths in the program under test in order to find bugs.",1,0,0,0
895,"Existing work has, therefore, focused on guiding the exploration toward program parts that are more likely to contain bugs by using an offline static analysis.",1,0,0,0
896,"In this paper, we introduce a novel technique for targeted greybox fuzzing using an online static analysis that guides the fuzzer toward a set of target locations, for instance, located in recently modified parts of the program.",0,0,1,0
897,This is achieved by first semantically analyzing each program path that is explored by an input in the fuzzer's test suite.,0,0,1,0
898,"The results of this analysis are then used to control the fuzzer's specialized power schedule, which determines how often to fuzz inputs from the test suite.",0,0,1,0
899,"We implemented our technique by extending a state-of-the-art, industrial fuzzer for Ethereum smart contracts and evaluate its effectiveness on 27 real-world benchmarks.",0,1,0,0
900,Using an online analysis is particularly suitable for the domain of smart contracts since it does not require any code instrumentation---adding instrumentation to contracts changes their semantics.,1,0,0,0
901,Our experiments show that targeted fuzzing significantly outperforms standard greybox fuzzing for reaching 83% of the challenging target locations (up to 14x of median speed-up).,0,0,0,1
902,"
GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications.",1,0,0,0
903,"These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes.",1,0,0,0
904,"However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can ""see"" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines.",1,0,0,0
905,"In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier.",0,0,1,0
906,"Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem.",0,0,1,0
907,"Our autoencoder learns to group similar GUI animations by ""seeing"" lots of unlabeled real-application GUI animations and learning to generate them.",0,1,0,0
908,"As the first work of its kind, we build the datasets of synthetic and real-world GUI animations.",0,1,0,0
909,"Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.",0,0,0,1
910,"
Detecting regression bugs in software evolution, analyzing side-channels in programs and evaluating robustness in deep neural networks (DNNs) can all be seen as instances of differential software analysis, where the goal is to generate diverging executions of program paths.",1,0,0,0
911,"Two executions are said to be diverging if the observable program behavior differs, e.g., in terms of program output, execution time, or (DNN) classification.",1,0,0,0
912,"The key challenge of differential software analysis is to simultaneously reason about multiple program paths, often across program variants.",1,0,0,0
913,"This paper presents HyDiff, the first hybrid approach for differential software analysis.",0,0,1,0
914,HyDiff integrates and extends two very successful testing techniques: Feedback-directed greybox fuzzing for efficient program testing and shadow symbolic execution for systematic program exploration.,0,0,1,0
915,HyDiff extends greybox fuzzing with divergence-driven feedback based on novel cost metrics that also take into account the control flow graph of the program.,0,0,1,0
916,Furthermore HyDiff extends shadow symbolic execution by applying four-way forking in a systematic exploration and still having the ability to incorporate concrete inputs in the analysis.,0,0,1,0
917,"HyDiff applies divergence revealing heuristics based on resource consumption and control-flow information to efficiently guide the symbolic exploration, which allows its efficient usage beyond regression testing applications.",0,0,1,0
918,"We introduce differential metrics such as output, decision and cost difference, as well as patch distance, to assist the fuzzing and symbolic execution components in maximizing the execution divergence.",0,0,0,1
919,We implemented our approach on top of the fuzzer AFL and the symbolic execution framework Symbolic PathFinder.,0,0,0,1
920,"Weillustrate HyDiff on regression and side-channel analysis for Java bytecode programs, and further show how to use HyDiff for robustness analysis of neural networks.",0,0,0,1
921,"
Database-backed web applications manipulate large amounts of persistent data, and such applications often contain constraints that restrict data length, data value, and other data properties.",1,0,0,0
922,Such constraints are critical in ensuring the reliability and usability of these applications.,1,0,0,0
923,"In this paper, we present a comprehensive study on where data constraints are expressed, what they are about, how often they evolve, and how their violations are handled.",0,0,1,0
924,"The results show that developers struggle with maintaining consistent data constraints and checking them across different components and versions of their web applications, leading to various problems.",0,0,0,1
925,"Guided by our study, we developed checking tools and API enhancements that can automatically detect such problems and improve the quality of such applications.",0,0,0,1
926,"
Many automated test generation techniques have been proposed for finding crashes in Android apps.",1,0,0,0
927,"Despite recent advancement in these approaches, a study shows that Android app developers prefer reading test cases written in natural language.",1,0,0,0
928,"Meanwhile, there exist redundancies in bug reports (written in natural language) across different apps that have not been previously reused.",1,0,0,0
929,"We propose collaborative bug finding, a novel approach that uses bugs in other similar apps to discover bugs in the app under test.",0,0,1,0
930,"We design three settings with varying degrees of interactions between programmers: (1) bugs from programmers who develop a different app, (2) bugs from manually searching for bug reports in GitHub repositories, (3) bugs from a bug recommendation system, Bugine.",0,0,1,0
931,Our studies of the first two settings in a software testing course show that collaborative bug finding helps students who are novice Android app testers to discover 17 new bugs.,0,0,0,1
932,"As students admit that searching for relevant bug reports could be time-consuming, we introduce Bugine, an approach that automatically recommends relevant GitHub issues for a given app.",0,0,0,1
933,"Bugine uses (1) natural language processing to find GitHub issues that mention common UI components shared between the app under test and other apps in our database, and (2) a ranking algorithm to select GitHub issues that are of the best quality.",0,0,0,1
934,Our results show that Bugine is able to find 34 new bugs.,0,0,0,1
935,"In total, collaborative bug finding helps us find 51 new bugs, in which eight have been confirmed and 11 have been fixed by the developers.",0,0,0,1
936,These results confirm our intuition that our proposed technique is useful in discovering new bugs for Android apps.,0,0,0,1
937,"
In recent years, machine translation software has increasingly been integrated into our daily lives.",1,0,0,0
938,"People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language.",1,0,0,0
939,"However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee.",1,0,0,0
940,"Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts.",1,0,0,0
941,"Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.",1,0,0,0
942,"To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software.",0,0,1,0
943,"Our key insight is that the translation results of ""similar"" source sentences should typically exhibit similar sentence structures.",0,0,1,0
944,"Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold.",0,0,0,1
945,"To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively.",0,1,0,1
946,"The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.",0,0,0,1
947,"
Misleading names of the methods in a project or the APIs in a software library confuse developers about program functionality and API usages, leading to API misuses and defects.",1,0,0,0
948,"In this paper, we introduce MNire, a machine learning approach to check the consistency between the name of a given method and its implementation.",0,0,1,0
949,MNire first generates a candidate name and compares the current name against it.,0,0,1,0
950,"If the two names are sufficiently similar, we consider the method as consistent.",0,0,1,0
951,"To generate the method name, we draw our ideas and intuition from an empirical study on the nature of method names in a large dataset.",0,1,0,0
952,"Our key finding is that high proportions of the tokens of method names can be found in the three contexts of a given method including its body, the interface (the method's parameter types and return type), and the enclosing class' name.",0,0,0,1
953,"Even when such tokens are not there, MNire uses the contexts to predict the tokens due to the high likelihoods of their co-occurrences.",0,0,0,1
954,Our unique idea is to treat the name generation as an abstract summarization on the tokens collected from the names of the program entities in the three above contexts.,0,0,0,1
955,We conducted several experiments to evaluate MNire in method name consistency checking and in method name recommending on large datasets with +14M methods.,0,1,0,0
956,"In detecting inconsistency method names, MNire improves the state-of-the-art approach by 10.4% and 11% relatively in recall and precision, respectively.",0,0,0,1
957,"In method name recommendation, MNire improves relatively over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher).",0,0,0,1
958,"To assess MNire's usefulness, we used it to detect inconsistent methods and suggest new names in several active, GitHub projects.",0,1,0,0
959,We made 50 pull requests (PRs) and received 42 responses.,0,1,0,0
960,"Among them, five PRs were merged into the main branch, and 13 were approved for later merging.",0,0,0,1
961,"In total, in 31/42 cases, the developer teams agree that our suggested names are more meaningful than the current names, showing MNire's usefulness.",0,0,0,1
962,"
Developers build programs based on software libraries to reduce coding effort.",1,0,0,0
963,"If a program inappropriately sets an API parameter, the program may exhibit unexpected runtime behaviors.",1,0,0,0
964,"To help developers correctly use library APIs, researchers built tools to mine API parameter rules.",1,0,0,0
965,"However, it is still unknown (1) what types of parameter rules there are, and (2) how these rules distribute inside documents and source files.",1,0,0,0
966,"In this paper, we conducted an empirical study to investigate the above-mentioned questions.",0,1,1,0
967,"To analyze as many parameter rules as possible, we took a hybrid approach that combines automatic localization of constrained parameters with manual inspection.",0,1,0,0
968,"Our automatic approach---PaRu---locates parameters that have constraints either documented in Javadoc (i.e., document rules) or implied by source code (i.e., code rules).",0,1,0,0
969,"Our manual inspection (1) identifies and categorizes rules for the located parameters, and (2) establishes mapping between document and code rules.",0,0,0,1
970,"By applying PaRu to 9 widely used libraries, we located 5,334 parameters with either document or code rules.",0,0,0,1
971,"Interestingly, there are only 187 parameters that have both types of rules, and 79 pairs of these parameter rules are unmatched.",0,0,0,1
972,"Additionally, PaRu extracted 1,688 rule sentences from Javadoc and code.",0,0,0,1
973,"We manually classified these sentences into six categories, two of which are overlooked by prior approaches.",0,0,0,1
974,We found that 86.2% of parameters have only code rules; 10.3% of parameters have only document rules; and only 3.5% of parameters have both document and code rules.,0,0,0,1
975,Our research reveals the challenges for automating parameter rule extraction.,0,0,0,1
976,"Based on our findings, we discuss the potentials of prior approaches and present our insights for future tool design.",0,0,0,1
977,"
The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions.",1,0,0,0
978,It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities.,1,0,0,0
979,"Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project.",1,0,0,0
980,"In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts.",0,0,1,0
981,"We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers.",0,0,1,0
982,"Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities.",0,0,0,1
983,"Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.",0,0,0,1
984,"
Test-to-code traceability links model the relationships between test artefacts and code artefacts.",1,0,0,0
985,"When utilised during the development process, these links help developers to keep test code in sync with tested code, reducing the rate of test failures and missed faults.",1,0,0,0
986,"Test-to-code traceability links can also help developers to maintain an accurate mental model of the system, reducing the risk of architectural degradation when making changes.",1,0,0,0
987,"However, establishing and maintaining these links manually places an extra burden on developers and is error-prone.",1,0,0,0
988,"This paper presents TCtracer, an approach and implementation for the automatic establishment of test-to-code traceability links.",0,0,1,0
989,"Unlike existing work, TCtracer operates at both the method level and the class level, allowing us to establish links between tests and functions, as well as between test classes and tested classes.",0,0,1,0
990,We improve over existing techniques by combining an ensemble of new and existing techniques and exploiting a synergistic flow of information between the method and class levels.,0,0,1,0
991,"An evaluation of TCtracer using four large, well-studied open source systems demonstrates that, on average, we can establish test-to-function links with a mean average precision (MAP) of 78% and test-class-to-class links with an MAP of 93%.",0,0,0,1
992,"
Large-scale open source communities, such as the Linux kernel, have gone through decades of development, substantially growing in scale and complexity.",1,0,0,0
993,"In the traditional workflow, maintainers serve as ""gatekeepers"" for the subsystems that they maintain.",1,0,0,0
994,"As the number of patches and authors significantly increases, maintainers come under considerable pressure, which may hinder the operation and even the sustainability of the community.",1,0,0,0
995,A few subsystems have begun to use new workflows to address these issues.,1,0,0,0
996,"However, it is unclear to what extent these new workflows are successful, or how to apply them.",1,0,0,0
997,"Therefore, we conduct an empirical study on the multiple-committer model (MCM) that has provoked extensive discussion in the Linux kernel community.",0,0,1,0
998,"We explore the effect of the model on the i915 subsystem with respect to four dimensions: pressure, latency, complexity, and quality assurance.",0,0,1,0
999,"We find that after this model was adopted, the burden of the i915 maintainers was significantly reduced.",0,0,0,1
1000,"Also, the model scales well to allow more committers.",0,0,0,1
1001,"After analyzing the online documents and interviewing the maintainers of i915, we propose that overloaded subsystems which have trustworthy candidate committers are suitable for adopting the model.",0,0,0,1
1002,"We further suggest that the success of the model is closely related to a series of measures for risk mitigation---sufficient precommit testing, strict review process, and the use of tools to simplify work and reduce errors.",0,0,0,1
1003,We employ a network analysis approach to locate candidate committers for the target subsystems and validate this approach and contextual success factors through email interviews with their maintainers.,0,1,0,0
1004,"To the best of our knowledge, this is the first study focusing on how to scale open source communities.",0,1,0,0
1005,We expect that our study will help the rapidly growing Linux kernel and other similar communities to adapt to changes and remain sustainable.,0,0,0,1
1006,Static analysis is increasingly used by companies and individual code developers to detect and fix bugs and security vulnerabilities.,1,0,0,0
1007,"As programs grow more complex, the analyses have to support new code concepts, frameworks and libraries.",1,0,0,0
1008,"However, static-analysis code itself is also prone to bugs.",1,0,0,0
1009,"While more complex analyses are written and used in production systems every day, the cost of debugging and fixing them also increases tremendously.",1,0,0,0
1010,"To understand the difficulties of debugging static analysis, we surveyed 115 static-analysis writers.",0,1,1,0
1011,"From their responses, we determined the core requirements to build a debugger for static analyses, which revolve around two main issues: abstracting from both the analysis code and the code it analyses at the same time, and tracking the analysis internal state throughout both code bases.",0,0,0,1
1012,Most tools used by our survey participants lack the capabilities to address both issues.,0,0,0,1
1013,"Focusing on those requirements, we introduce Visuflow, a debugging environment for static data-flow analysis.",0,0,1,0
1014,Visuflow features graph visualizations and custom breakpoints that enable users to view the state of an analysis at any time.,0,0,1,0
1015,"In a user study on 20 static-analysis writers, Visuflow helped identify 25 and fix 50 percent more errors in the analysis code compared to the standard Eclipse debugging environment.",0,1,0,1
1016,Selecting reviewers for code changes is a critical step for an efficient code review process.,1,0,0,0
1017,Recent studies propose automated reviewer recommendation algorithms to support developers in this task.,1,0,0,0
1018,"However, the evaluation of recommendation algorithms, when done apart from their target systems and users (i.e., code review tools and change authors), leaves out important aspects: perception of recommendations, influence of recommendations on human choices, and their effect on user experience.",1,0,0,0
1019,This study is the first to evaluate a reviewer recommender in vivo.,0,0,1,0
1020,"We compare historical reviewers and recommendations for over 21,000 code reviews performed with a deployed recommender in a company environment and set out to measure the influence of recommendations on users' choices, along with other performance metrics.",0,1,0,0
1021,"Having found no evidence of influence, we turn to the users of the recommender.",0,0,0,1
1022,"Through interviews and a survey we find that, though perceived as relevant, reviewer recommendations rarely provide additional value for the respondents.",0,1,0,1
1023,We confirm this finding with a larger study at another company.,0,0,0,1
1024,The confirmation of this finding brings up a case for more user-centric approaches to designing and evaluating the recommenders.,0,0,0,1
1025,"Finally, we investigate information needs of developers during reviewer selection and discuss promising directions for the next generation of reviewer recommendation tools.",0,0,1,0
1026,Abbreviations are widely used in identifiers.,1,0,0,0
1027,"However, they have severe negative impact on program comprehension and IR-based software maintenance activities, e.g., concept location, software clustering, and recovery of traceability links.",1,0,0,0
1028,"Consequently, a number of efficient approaches have been proposed successfully to expand abbreviations in identifiers.",1,0,0,0
1029,"Most of such approaches rely heavily on dictionaries, and rarely exploit the specific and fine-grained context of identifiers.",1,0,0,0
1030,"As a result, such approaches are less accurate in expanding abbreviations (especially short ones) that may match multiple dictionary words.",1,0,0,0
1031,"To this end, in this paper we propose an automatic approach to improve the accuracy of abbreviation expansion by exploiting the specific and fine-grained context.",0,0,1,0
1032,"It focuses on a special but common category of abbreviations (abbreviations in parameter names), and thus it can exploit the specific and fine-grained context, i.e., the type of the enclosing parameter as well the corresponding formal (or actual) parameter name.",0,0,1,0
1033,The recent empirical study on parameters suggest that actual parameters are often lexically similar to their corresponding formal parameters.,0,1,0,0
1034,"Consequently, it is likely that an abbreviation in a formal parameter can find its full terms in the corresponding actual parameter, and vice versa.",0,0,1,0
1035,"Based on this assumption, a series of heuristics are proposed to look for full terms from the corresponding actual (or formal) parameter names.",0,1,0,0
1036,"To the best of our knowledge, we are the first to expand abbreviations by exploiting the lexical similarity between actual and formal parameters.",0,1,0,0
1037,We also search for full terms in the data type of the enclosing parameter.,0,0,1,0
1038,"Only if all such heuristics fail, the approach turns to the traditional abbreviation dictionaries.",0,0,1,0
1039,We evaluate the proposed approach on seven well known open-source projects.,0,1,0,0
1040,"Evaluation results suggest that when only parameter abbreviations are involved, the proposed approach can improve the precision from 26 to 95 percent and recall from 26 to 65 percent compared against the state-of-the-art general purpose approach.",0,0,0,1
1041,"Consequently, the proposed approach could be employed as a useful supplement to existing approaches to expand parameter abbreviations.",0,0,0,1
1042,Test adequacy criteria are widely used to guide test creation.,1,0,0,0
1043,"However, many of these criteria are sensitive to statement structure or the choice of test oracle.",1,0,0,0
1044,"This is because such criteria ensure that execution reaches the element of interest, but impose no constraints on the execution path after this point.",1,0,0,0
1045,We are not guaranteed to observe a failure just because a fault is triggered.,1,0,0,0
1046,"To address this issue, we have proposed the concept of observability-an extension to coverage criteria based on Boolean expressions that combines the obligations of a host criterion with an additional path condition that increases the likelihood that a fault encountered will propagate to a monitored variable.",0,0,1,0
1047,"Our study, conducted over five industrial systems and an additional forty open-source systems, has revealed that adding observability tends to improve efficacy over satisfaction of the traditional criteria, with average improvements of 125.98 percent in mutation detection with the common output-only test oracle and per-model improvements of up to 1760.52 percent.",0,1,0,1
1048,"Ultimately, there is merit to our hypothesis-observability reduces sensitivity to the choice of oracle and to the program structure.",0,0,0,1
1049,"We present Supa, a value-flow-based demand-driven flow- and context-sensitive pointer analysis with strong updates for C and C++ programs.",0,0,1,0
1050,"Supa enables computing points-to information via value-flow refinement, in environments with small time and memory budgets.",0,0,1,0
1051,"We formulate Supa by solving a graph-reachability problem on an inter-procedural value-flow graph representing a program's def-use chains, which are pre-computed efficiently but over-approximately.",0,0,1,0
1052,"To answer a client query (a request for a variable's points-to set), Supa reasons about the flow of values along the pre-computed def-use chains sparsely (rather than across all program points), by performing only the work necessary for the query (rather than analyzing the whole program).",0,0,1,0
1053,"In particular, strong updates are performed to filter out spurious def-use chains through value-flow refinement as long as the total budget is not exhausted.",0,0,1,0
1054,"We have implemented Supa on top of LLVM (4.0.0) together with a comprehensive micro-benchmark suite after a years-long effort (consisting of around 400 test cases, including hand-written ones and the ones extracted from real programs).",0,0,1,0
1055,"We have evaluated Supa by choosing uninitialized pointer detection and C++ virtual table resolution as two major clients, using 24 real-world programs including 18 open-source C programs and 6 large CPU2000/2006 C++ benchmarks.",0,1,0,0
1056,"For uninitialized pointer client, Supa achieves improved precision as the analysis budget increases, with its flow-sensitive (context-insensitive) analysis reaching 97.4 percent of that achieved by whole-program Sparse Flow-Sensitive analysis (SFS) by consuming about 0.18 seconds and 65 KB of memory per query, on average (with a budget of at most 10,000 value-flow edges per query).",0,0,0,1
1057,"With context-sensitivity also considered, Supa becomes more precise for some programs but also incurs more analysis times.",0,0,0,1
1058,"To further demonstrate the effectiveness of Supa, we have also evaluated Supa in resolving C++ virtual tables by querying the function pointers at every virtual callsite.",0,1,0,0
1059,"Compared to analysis without strong updates for heap objects, Supa's demand-driven context-sensitive strong update analysis reduces 7.35 percent spurious virtual table targets with only 0.4 secs per query, on average.",0,0,0,1
1060,"Developers always focus on delivering high-quality updates to improve, or maintain the rating of their apps.",1,0,0,0
1061,Prior work has studied user reviews by analyzing all reviews of an app.,1,0,0,0
1062,"However, this app-level analysis misses the point that users post reviews to provide their feedback on a certain update.",1,0,0,0
1063,"For example, two bad updates of an app with a history of good updates would not be spotted using app-level analysis.",1,0,0,0
1064,"In this paper, we examine reviews at the update-level to better understand how users perceive bad updates.",0,0,1,0
1065,"We focus our study on the top 250 bad updates (i.e., updates with the highest increase in the percentage of negative reviews relative to the prior updates of the app) from 26,726 updates of 2,526 top free-to-download apps in the Google Play Store.",0,1,0,0
1066,We find that feature removal and UI issues have the highest increase in the percentage of negative reviews.,0,0,0,1
1067,Bad updates with crashes and functional issues are the most likely to be fixed by a later update.,1,0,0,0
1068,"However, developers often do not mention these fixes in the release notes.",1,0,0,0
1069,Our work demonstrates the necessity of an update-level analysis of reviews to capture the feelings of an app's user-base about a particular update.,0,0,0,1
1070,Bug reports play an important role in the process of debugging and fixing bugs.,1,0,0,0
1071,"To reduce the burden of bug report managers and facilitate the process of bug fixing, a great amount of software engineering research has been invested toward automated bug report management techniques.",1,0,0,0
1072,"However, the verdict is still open whether such techniques are actually required and applicable outside the domain of theoretical research.",1,0,0,0
1073,"To fill this gap, we conducted a survey among 327 practitioners to gain their insights into various categories of automated bug report management techniques.",0,1,0,0
1074,"Specifically, we asked the respondents to rate the importance of such techniques and provide the rationale.",0,1,0,0
1075,"To get deeper insights into practitioners' perspective, we conducted follow-up interviews with 25 interviewees selected from the survey respondents.",0,1,0,0
1076,"Through the survey and the interviews, we gained a better understanding of the perceived usefulness (or its lack) of different categories of automated bug report management techniques.",0,0,0,1
1077,"Based on our findings, we summarized some potential research directions in developing techniques to help developers better manage bug reports.",0,0,0,1
1078,Finding good configurations of a software system is often challenging since the number of configuration options can be large.,1,0,0,0
1079,"Software engineers often make poor choices about configuration or, even worse, they usually use a sub-optimal configuration in production, which leads to inadequate performance.",1,0,0,0
1080,"To assist engineers in finding the better configuration, this article introduces Flash, a sequential model-based method that sequentially explores the configuration space by reflecting on the configurations evaluated so far to determine the next best configuration to explore.",0,0,1,0
1081,Flash scales up to software systems that defeat the prior state-of-the-art model-based methods in this area.,0,0,1,0
1082,Flash runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems.,0,0,1,0
1083,The central insight of this article is to use the prior knowledge of the configuration space (gained from prior runs) to choose the next promising configuration.,0,0,1,0
1084,"This strategy reduces the effort (i.e., number of measurements) required to find the better configuration.",0,0,1,0
1085,We evaluate Flash using 30 scenarios based on 7 software systems to demonstrate that Flash saves effort in 100 and 80 percent of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to state-of-the-art techniques.,0,1,0,1
1086,A linter is a static analysis tool that warns software developers about possible code errors or violations to coding standards.,1,0,0,0
1087,"By using such a tool, errors can be surfaced early in the development process when they are cheaper to fix.",1,0,0,0
1088,"For a linter to be successful, it is important to understand the needs and challenges of developers when using a linter.",1,0,0,0
1089,"In this paper, we examine developers' perceptions on JavaScript linters.",0,0,1,0
1090,We study why and how developers use linters along with the challenges they face while using such tools.,0,0,1,0
1091,"For this purpose we perform a case study on ESLint, the most popular JavaScript linter.",0,1,0,0
1092,"We collect data with three different methods where we interviewed 15 developers from well-known open source projects, analyzed over 9,500 ESLint configuration files, and surveyed 337 developers from the JavaScript community.",0,1,0,0
1093,Our results provide practitioners with reasons for using linters in their JavaScript projects as well as several configuration strategies and their advantages.,0,0,0,1
1094,"We also provide a list of linter rules that are often enabled and disabled, which can be interpreted as the most important rules to reason about when configuring linters.",0,0,0,1
1095,"Finally, we propose several feature suggestions for tool makers and future work for researchers.",0,0,0,1
1096,"When multiple developers change a software system in parallel, these concurrent changes need to be merged to all appear in the software being developed.",1,0,0,0
1097,"Numerous merge techniques have been proposed to support this task, but none of them can fully automate the merge process.",1,0,0,0
1098,"Indeed, it has been reported that as much as 10 to 20 percent of all merge attempts result in a merge conflict, meaning that a developer has to manually complete the merge.",1,0,0,0
1099,"To date, we have little insight into the nature of these merge conflicts.",1,0,0,0
1100,"What do they look like, in detail?",1,0,0,0
1101,How do developers resolve them?,1,0,0,0
1102,Do any patterns exist that might suggest new merge techniques that could reduce the manual effort?,1,0,0,0
1103,"This paper contributes an in-depth study of the merge conflicts found in the histories of 2,731 open source Java projects.",0,1,1,0
1104,"Seeded by the manual analysis of the histories of five projects, our automated analysis of all 2,731 projects: (1) characterizes the merge conflicts in terms of number of chunks, size, and programming language constructs involved, (2) classifies the manual resolution strategies that developers use to address these merge conflicts, and (3) analyzes the relationships between various characteristics of the merge conflicts and the chosen resolution strategies.",0,1,1,0
1105,"Our results give rise to three primary recommendations for future merge techniques, that - when implemented - could on one hand help in automatically resolving certain types of conflicts and on the other hand provide the developer with tool-based assistance to more easily resolve other types of conflicts that cannot be automatically resolved.",0,0,0,1
1106,"With the advent of multicore processors, there is a great need to write parallel programs to take advantage of parallel computing resources.",1,0,0,0
1107,"However, due to the nondeterminism of parallel execution, the malware behaviors sensitive to thread scheduling are extremely difficult to detect.",1,0,0,0
1108,Dynamic taint analysis is widely used in security problems.,1,0,0,0
1109,"By serializing a multithreaded execution and then propagating taint tags along the serialized schedule, existing dynamic taint analysis techniques lead to under-tainting with respect to other possible interleavings under the same input.",1,0,0,0
1110,"In this paper, we propose an approach called DSTAM that integrates symbolic analysis and guided execution to systematically detect tainted instances on all possible executions under a given input.",0,0,1,0
1111,"Symbolic analysis infers alternative interleavings of an executed trace that cover new tainted instances, and computes thread schedules that guide future executions.",1,0,0,0
1112,Guided execution explores new execution traces that drive future symbolic analysis.,1,0,0,0
1113,"We have implemented a prototype as part of an educational tool that teaches secure C programming, where accuracy is more critical than efficiency.",0,0,1,0
1114,"To the best of our knowledge, DSTAM is the first algorithm that addresses the challenge of taint analysis for multithreaded program under fixed inputs.",0,0,1,0
1115,Successful software products evolve through a process of continual change.,1,0,0,0
1116,"However, this process may weaken the design of the software and make it unnecessarily complex, leading to significantly reduced productivity and increased fault-proneness.",1,0,0,0
1117,"Refactoring improves the software design while preserving overall functionality and behavior, and is an important technique in managing the growing complexity of software systems.",1,0,0,0
1118,Most of the existing work on software refactoring uses either an entirely manual or a fully automated approach.,1,0,0,0
1119,"Manual refactoring is time-consuming, error-prone and unsuitable for large-scale, radical refactoring.",1,0,0,0
1120,"On the other hand, fully automated refactoring yields a static list of refactorings which, when applied, leads to a new and often hard to comprehend design.",1,0,0,0
1121,"Furthermore, it is difficult to merge these refactorings with other changes performed in parallel by developers.",1,0,0,0
1122,"In this paper, we propose a refactoring recommendation approach that dynamically adapts and interactively suggests refactorings to developers and takes their feedback into consideration.",0,0,1,0
1123,Our approach uses NSGA-II to find a set of good refactoring solutions that improve software quality while minimizing the deviation from the initial design.,0,0,1,0
1124,These refactoring solutions are then analyzed to extract interesting common features between them such as the frequently occurring refactorings in the best non-dominated solutions.,0,0,1,0
1125,"Based on this analysis, the refactorings are ranked and suggested to the developer in an interactive fashion as a sequence of transformations.",0,0,1,0
1126,"The developer can approve, modify or reject each of the recommended refactorings, and this feedback is then used to update the proposed rankings of recommended refactorings.",0,0,1,0
1127,"After a number of introduced code changes and interactions with the developer, the interactive NSGA-II algorithm is executed again on the new modified system to repair the set of refactoring solutions based on the new changes and the feedback received from the developer.",0,0,1,0
1128,We evaluated our approach on a set of eight open source systems and two industrial projects provided by an industrial partner.,0,1,0,0
1129,Statistical analysis of our experiments shows that our dynamic interactive refactoring approach performed significantly better than four existing search-based refactoring techniques and one fully-automated refactoring tool not based on heuristic search.,0,0,0,1
1130,"Free/Libre and Open Source Software (FLOSS) communities are composed, in part, of volunteers, many of whom contribute infrequently.",1,0,0,0
1131,"However, these infrequent volunteers contribute to the sustainability of FLOSS projects, and should ideally be encouraged to continue participating, even if they cannot be persuaded to contribute regularly.",1,0,0,0
1132,"Infrequent contributions are part of a trend which has been widely observed in other sectors of volunteering, where it has been termed “episodic volunteering” (EV).",1,0,0,0
1133,"Previous FLOSS research has focused on the Onion model, differentiating core and peripheral developers, with the latter considered as a homogeneous group.",1,0,0,0
1134,"We argue this is too simplistic, given the size of the periphery group and the myriad of valuable activities they perform beyond coding.",0,0,1,0
1135,Our exploratory qualitative survey of 13 FLOSS communities investigated what episodic volunteering looks like in a FLOSS context.,0,1,0,0
1136,"EV is widespread in FLOSS communities, although not specifically managed.",0,0,0,1
1137,We suggest several recommendations for managing EV based on a framework drawn from the volunteering literature.,0,0,0,1
1138,"Also, episodic volunteers make a wide range of value-added contributions other than code, and they should neither be expected nor coerced into becoming habitual volunteers.",0,0,0,1
1139,"With the thriving of mobile app markets, third-party libraries are pervasively used in Android applications.",1,0,0,0
1140,"The libraries provide functionalities such as advertising, location, and social networking services, making app development much more productive.",1,0,0,0
1141,"However, the spread of vulnerable and harmful third-party libraries can also hurt the mobile ecosystem, leading to various security problems.",1,0,0,0
1142,"Therefore, third-party library identification has emerged as an important problem, being the basis of many security applications such as repackaging detection, vulnerability identification, and malware analysis.",1,0,0,0
1143,"Previously, we proposed a novel approach to identifying third-party Android libraries at a massive scale.",1,0,0,0
1144,Our method uses the internal code dependencies of an app to recognize library candidates and further classify them.,0,0,1,0
1145,"With a fine-grained feature hashing strategy, we can better handle code whose package and method names are obfuscated than historical work.",0,0,1,0
1146,"We have developed a prototypical tool called LibD and evaluated it with an up-to-date dataset containing 1,427,395 Android apps.",0,1,0,0
1147,"Our experiment results show that LibD outperforms existing tools in detecting multi-package third-party libraries with the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.",0,0,0,1
1148,"In this paper, we extend our early work by investigating the possibility of employing effective and scalable library detection to boost the performance of large-scale app analyses in the real world.",0,0,1,0
1149,We show that the technique of LibD can be used to accelerate whole-app Android vulnerability detection and quickly identify variants of vulnerable third-party libraries.,0,0,0,1
1150,This extension paper sheds light on the practical value of our previous research.,0,0,0,1
1151,Application Programming Interfaces (APIs) represent key tools for software developers to build complex software systems.,1,0,0,0
1152,"However, several studies have revealed that even major API providers tend to have incomplete or inconsistent API documentation.",1,0,0,0
1153,"This can severely hamper the API comprehension and, as a consequence, the quality of the software built on them.",1,0,0,0
1154,"In this paper, we propose DRONE (Detect and Repair of dOcumentatioN dEfects), a framework to automatically detect and repair defects from API documents by leveraging techniques from program analysis, natural language processing, and constraint solving.",0,0,1,0
1155,"Specifically, we target at the directives of API documents, which are related to parameter constraints and exception handling declarations.",0,0,1,0
1156,"Furthermore, in presence of defects, we also provide a prototypical repair recommendation system.",0,0,1,0
1157,We evaluate our approach on parts of the well-documented APIs of JDK 1.8 APIs (including javaFX) and Android 7.0 (level 24).,0,1,0,0
1158,"Across the two empirical studies, our approach can detect API defects with an average F-measure of 79.9, 71.7, and 81.4 percent, respectively.",0,1,0,1
1159,The API repairing capability has also been evaluated on the generated recommendations in a further experiment.,0,1,0,0
1160,User judgments indicate that the constraint information is addressed correctly and concisely in the rendered directives.,0,0,0,1
1161,"To ensure the quality of its shared knowledge, Stack Overflow encourages users to revise answers through a badge system, which is based on quantitative measures (e.g., a badge is awarded after revising more than 500 answers).",1,0,0,0
1162,"Prior studies show that badges can positively steer the user behavior on Stack Overflow (e.g., increasing user participation).",1,0,0,0
1163,"However, little is known whether revision-related badges have a negative impact on the quality of revisions since some studies show that certain users may game incentive systems to gain rewards.",1,0,0,0
1164,"In this study, we analyze 3,871,966 revision records that are collected from 2,377,692 Stack Overflow answers.",0,1,0,0
1165,We find that: 1) Users performed a much larger than usual revisions on the badge-awarding days compared to normal days; 25% of the users did not make any more revisions once they received their first revision-related badge.,0,0,0,1
1166,"2) Performing more revisions than usual in a single day increased the likelihood of such revisions being rolled back (e.g., due to undesired or incorrect revisions).",0,0,0,1
1167,3) Users were more likely to perform text and small revisions if they performed many revisions in a single day.,0,0,0,1
1168,"Our findings are concurred by the Stack Overflow community, and they highlight the need for changes to the current badge system in order to provide a better balance between the quality and quantity of revisions.",0,0,0,1
1169,Automated program repair is the problem of automatically fixing bugs in programs in order to significantly reduce the debugging costs and improve the software quality.,1,0,0,0
1170,"To address this problem, test-suite based repair techniques regard a given test suite as an oracle and modify the input buggy program to make the entire test suite pass.",1,0,0,0
1171,"GenProg is well recognized as a prominent repair approach of this kind, which uses genetic programming (GP) to rearrange the statements already extant in the buggy program.",1,0,0,0
1172,"However, recent empirical studies show that the performance of GenProg is not fully satisfactory, particularly for Java.",1,0,0,0
1173,"In this paper, we propose ARJA, a new GP based repair approach for automated repair of Java programs.",0,0,1,0
1174,"To be specific, we present a novel lower-granularity patch representation that properly decouples the search subspaces of likely-buggy locations, operation types and potential fix ingredients, enabling GP to explore the search space more effectively.",0,0,1,0
1175,"Based on this new representation, we formulate automated program repair as a multi-objective search problem and use NSGA-II to look for simpler repairs.",0,0,1,0
1176,"To reduce the computational effort and search space, we introduce a test filtering procedure that can speed up the fitness evaluation of GP and three types of rules that can be applied to avoid unnecessary manipulations of the code.",0,0,1,0
1177,"Moreover, we also propose a type matching strategy that can create new potential fix ingredients by exploiting the syntactic patterns of existing statements.",0,0,1,0
1178,We conduct a large-scale empirical evaluation of ARJA along with its variants on both seeded bugs and real-world bugs in comparison with several state-of-the-art repair approaches.,0,1,0,0
1179,Our results verify the effectiveness and efficiency of the search mechanisms employed in ARJA and also show its superiority over the other approaches.,0,0,0,1
1180,"In particular, compared to jGenProg (an implementation of GenProg for Java), an ARJA version fully following the redundancy assumption can generate a test-suite adequate patch for more than twice the number of bugs (from 27 to 59), and a correct patch for nearly four times of the number (from 5 to 18), on 224 real-world bugs considered in Defects4J.",0,0,0,1
1181,"Furthermore, ARJA is able to correctly fix several real multi-location bugs that are hard to be repaired by most of the existing repair approaches.",0,0,0,1
1182,The standard approach to applying text retrieval models to code repositories is to train models on documents representing program elements.,1,0,0,0
1183,"However, code changes lead to model obsolescence and to the need to retrain the model from the latest snapshot.",1,0,0,0
1184,"To address this, we previously introduced an approach that trains a model on documents representing changesets from a repository and demonstrated its feasibility for feature location.",1,0,0,0
1185,"In this paper, we expand our work by investigating: a second task (developer identification), the effects of including different changeset parts in the model, the repository characteristics that affect the accuracy of our approach, and the effects of the time invariance assumption on evaluation results.",0,0,1,0
1186,"Our results demonstrate that our approach is as accurate as the standard approach for projects with most changes localized to a subset of the code, but less accurate when changes are highly distributed throughout the code.",0,0,0,1
1187,"Moreover, our results demonstrate that context and messages are key to the accuracy of changeset-based models and that the time invariance assumption has a statistically significant effect on evaluation results, providing overly-optimistic results.",0,0,0,1
1188,"Our findings indicate that our approach is a suitable alternative to the standard approach, providing comparable accuracy while eliminating retraining costs.",0,0,0,1
1189,Developers increasingly rely on text matching tools to analyze the relation between natural language words and APIs.,1,0,0,0
1190,"However, semantic gaps, namely textual mismatches between words and APIs, negatively affect these tools.",1,0,0,0
1191,"Previous studies have transformed words or APIs into low-dimensional vectors for matching; however, inaccurate results were obtained due to the failure of modeling words and APIs simultaneously.",1,0,0,0
1192,"To resolve this problem, two main challenges are to be addressed: the acquisition of massive words and APIs for mining and the alignment of words and APIs for modeling.",1,0,0,0
1193,"Therefore, this study proposes Word2API to effectively estimate relatedness of words and APIs.",0,0,1,0
1194,Word2API collects millions of commonly used words and APIs from code repositories to address the acquisition challenge.,0,1,0,0
1195,"Then, a shuffling strategy is used to transform related words and APIs into tuples to address the alignment challenge.",0,0,1,0
1196,"Using these tuples, Word2API models words and APIs simultaneously.",0,0,1,0
1197,Word2API outperforms baselines by 10-49.6 percent of relatedness estimation in terms of precision and NDCG.,0,0,0,1
1198,"Word2API is also effective on solving typical software tasks, e.g., query expansion and API documents linking.",0,0,0,1
1199,A simple system with Word2API-expanded queries recommends up to 21.4 percent more related APIs for developers.,0,0,0,1
1200,"Meanwhile, Word2API improves comparison algorithms by 7.9-17.4 percent in linking questions in Question&Answer communities to API documents.",0,0,0,1
1201,"Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources.",1,0,0,0
1202,Various features to build defect prediction models have been proposed and evaluated.,1,0,0,0
1203,"Among them, process metrics are one important category.",1,0,0,0
1204,"Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution.",1,0,0,0
1205,Are the change sequences derived from such information useful to characterize buggy program modules?,1,0,0,0
1206,How can we leverage such sequences to build good defect prediction models?,1,0,0,0
1207,"Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length.",1,0,0,0
1208,This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers.,1,0,0,0
1209,"To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically.",0,0,1,0
1210,"In this paper, we propose a novel approach called Fences, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis.",0,0,1,0
1211,It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN.,0,0,1,0
1212,Our evaluations on 10 open source projects show that Fences can predict defects with high performance.,0,1,0,1
1213,"In particular, our approach achieves an average F-measure of 0.657, which improves the prediction models built on traditional metrics significantly.",0,0,0,1
1214,The improvements vary from 31.6 to 46.8 percent on average.,0,0,0,1
1215,"In terms of AUC, Fences achieves an average value of 0.892, and the improvements over baselines vary from 4.2 to 16.1 percent.",0,0,0,1
1216,Fences also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.,0,0,0,1
1217,Developers frequently discuss aspects of the systems they are developing online.,1,0,0,0
1218,The comments they post to discussions form a rich information source about the system.,1,0,0,0
1219,"Intention mining, a process introduced by Di Sorbo et al., classifies sentences in developer discussions to enable further analysis.",1,0,0,0
1220,"As one example of use, intention mining has been used to help build various recommenders for software developers.",1,0,0,0
1221,The technique introduced by Di Sorbo et al. to categorize sentences is based on linguistic patterns derived from two projects.,1,0,0,0
1222,"The limited number of data sources used in this earlier work introduces questions about the comprehensiveness of intention categories and whether the linguistic patterns used to identify the categories are generalizable to developer discussion recorded in other kinds of software artifacts (e.g., issue reports).",1,0,0,0
1223,"To assess the comprehensiveness of the previously identified intention categories and the generalizability of the linguistic patterns for category identification, we manually created a new dataset, categorizing 5,408 sentences from issue reports of four projects in GitHub.",0,1,0,0
1224,"Based on this manual effort, we refined the previous categories.",0,0,1,0
1225,"We assess Di Sorbo et al.'s patterns on this dataset, finding that the accuracy rate achieved is low (0.31).",0,0,0,1
1226,"To address the deficiencies of Di Sorbo et al.'s patterns, we propose and investigate a convolution neural network (CNN)-based approach to automatically classify sentences into different categories of intentions.",0,0,1,0
1227,"Our approach optimizes CNN by integrating batch normalization to accelerate the training speed, and an automatic hyperparameter tuning approach to tune appropriate hyperparameters of CNN.",0,0,1,0
1228,"Our approach achieves an accuracy of 0.84 on the new dataset, improving Di Sorbo et al.'s approach by 171 percent.",0,0,0,1
1229,"We also apply our approach to improve an automated software engineering task, in which we use our proposed approach to rectify misclassified issue reports, thus reducing the bias introduced by such data to other studies.",0,0,1,0
1230,"A case study on four open source projects with 2,076 issue reports shows that our approach achieves an average AUC score of 0.687, which improves other baselines by at least 16 percent.",0,0,0,1
1231,"Modern information technology paradigms, such as online services and off-the-shelf products, often involve a wide variety of users with different or even conflicting objectives.",1,0,0,0
1232,"Every software output may satisfy some users, but may also fail to satisfy others.",1,0,0,0
1233,"Furthermore, users often do not know the internal working mechanisms of the systems.",1,0,0,0
1234,"This situation is quite different from bespoke software, where developers and users typically know each other.",1,0,0,0
1235,"This paper proposes an approach to help users to better understand the software that they use, and thereby more easily achieve their objectives-even when they do not fully understand how the system is implemented.",0,0,1,0
1236,"Our approach borrows the concept of metamorphic relations from the field of metamorphic testing (MT), using it in an innovative way that extends beyond MT.",0,0,1,0
1237,We also propose a “symmetry” metamorphic relation pattern and a “change direction” metamorphic relation input pattern that can be used to derive multiple concrete metamorphic relations.,0,0,1,0
1238,"Empirical studies reveal previously unknown failures in some of the most popular applications in the world, and show how our approach can help users to better understand and better use the systems.",1,1,0,1
1239,"The empirical results provide strong evidence of the simplicity, applicability, and effectiveness of our methodology.",0,1,0,1
1240,Android ecosystem is heavily fragmented.,1,0,0,0
1241,"The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps, and thus various compatibility issues arise.",1,0,0,0
1242,"Unfortunately, little is known on the characteristics of such fragmentation-induced compatibility issues.",1,0,0,0
1243,No mature tools exist to help developers quickly diagnose and fix these issues.,1,0,0,0
1244,"To bridge the gap, we conducted an empirical study on 220 real-world compatibility issues collected from five popular open-source Android apps.",0,1,0,0
1245,We further interviewed Android practitioners and conducted an online survey to gain insights from real practices.,0,1,0,0
1246,"Via the studies, we characterized compatibility issues, investigated common practices to handle compatibility issues, and disclosed that these issues exhibit common patterns.",0,0,1,0
1247,"With these findings, we propose a technique, FicFinder, to automatically detect compatibility issues in Android apps.",0,0,1,0
1248,FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues can be triggered.,0,0,1,0
1249,FicFinder reports actionable debugging information to developers when it detects potential issues.,0,0,0,1
1250,We evaluated FicFinder with 53 large-scale open-source Android apps.,0,1,0,0
1251,The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.,0,0,0,1
1252,"Defect models that are trained on class imbalanced datasets (i.e., the proportion of defective and clean modules is not equally represented) are highly susceptible to produce inaccurate prediction models.",1,0,0,0
1253,"Prior research compares the impact of class rebalancing techniques on the performance of defect models but arrives at contradictory conclusions due to the use of different choice of datasets, classification techniques, and performance measures.",1,0,0,0
1254,Such contradictory conclusions make it hard to derive practical guidelines for whether class rebalancing techniques should be applied in the context of defect models.,1,0,0,0
1255,"In this paper, we investigate the impact of class rebalancing techniques on the performance measures and interpretation of defect models.",0,0,1,0
1256,We also investigate the experimental settings in which class rebalancing techniques are beneficial for defect models.,0,0,1,0
1257,"Through a case study of 101 datasets that span across proprietary and open-source systems, we conclude that the impact of class rebalancing techniques on the performance of defect prediction models depends on the used performance measure and the used classification techniques.",0,1,0,0
1258,"We observe that the optimized SMOTE technique and the under-sampling technique are beneficial when quality assurance teams wish to increase AUC and Recall, respectively, but they should be avoided when deriving knowledge and understandings from defect models.",0,0,0,1
1259,Discrete event controllers are at the heart of many software systems that require continuous operation.,1,0,0,0
1260,Changing these controllers at runtime to cope with changes in its execution environment or system requirements change is a challenging open problem.,1,0,0,0
1261,In this paper we address the problem of dynamic update of controllers in reactive systems.,0,0,1,0
1262,"We present a general approach to specifying correctness criteria for dynamic update and a technique for automatically computing a controller that handles the transition from the old to the new specification, assuring that the system will reach a state in which such a transition can correctly occur and in which the underlying system architecture can reconfigure.",0,0,1,0
1263,Our solution uses discrete event controller synthesis to automatically build a controller that guarantees both progress towards update and safe update.,0,0,0,1
1264,"Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts.",1,0,0,0
1265,Traditional defect prediction features often fail to capture the semantic differences between different programs.,1,0,0,0
1266,This degrades the performance of the prediction models built on these traditional features.,1,0,0,0
1267,"Thus, the capability to capture the semantics in programs is required to build accurate prediction models.",1,0,0,0
1268,"To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes.",0,0,1,0
1269,"Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models).",0,0,1,0
1270,"We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction).",0,1,0,0
1271,Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks.,0,0,0,1
1272,"Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.",0,0,0,1
1273,Software systems fail.,1,0,0,0
1274,"These failures are often reported to issue tracking systems, where they are prioritized and assigned to responsible developers to be investigated.",1,0,0,0
1275,"When developers debug software, they need to reproduce the reported failure in order to verify whether their fix actually prevents the failure from happening again.",1,0,0,0
1276,"Since manually reproducing each failure could be a complex task, several automated techniques have been proposed to tackle this problem.",1,0,0,0
1277,"Despite showing advancements in this area, the proposed techniques showed various types of limitations.",1,0,0,0
1278,"In this paper, we present EvoCrash, a new approach to automated crash reproduction based on a novel evolutionary algorithm, called Guided Genetic Algorithm (GGA).",0,0,1,0
1279,"We report on our empirical study on using EvoCrash to reproduce 54 real-world crashes, as well as the results of a controlled experiment, involving human participants, to assess the impact of EvoCrash tests in debugging.",0,1,0,0
1280,"Based on our results, EvoCrash outperforms state-of-the-art techniques in crash reproduction and uncovers failures that are undetected by classical coverage-based unit test generation tools.",0,0,0,1
1281,"In addition, we observed that using EvoCrash helps developers provide fixes more often and take less time when debugging, compared to developers debugging and fixing code without using EvoCrash tests.",0,0,0,1
1282,Defect prediction has been an active research area for over four decades.,1,0,0,0
1283,"Despite numerous studies on defect prediction, the potential value of defect prediction in practice remains unclear.",1,0,0,0
1284,"To address this issue, we performed a mixed qualitative and quantitative study to investigate what practitioners think, behave and expect in contrast to research findings when it comes to defect prediction.",0,1,0,0
1285,"We collected hypotheses from open-ended interviews and a literature review of defect prediction papers that were published at ICSE, ESEC/FSE, ASE, TSE and TOSEM in the last 6 years (2012-2017).",0,1,0,0
1286,We then conducted a validation survey where the hypotheses became statements or options of our survey questions.,0,1,0,0
1287,We received 395 responses from practitioners from over 33 countries across five continents.,0,1,0,0
1288,Some of our key findings include: 1) Over 90 percent of respondents are willing to adopt defect prediction techniques.,0,0,0,1
1289,2) There exists a disconnect between practitioners' perceptions and well supported research evidence regarding defect density distribution and the relationship between file size and defectiveness.,0,0,0,1
1290,3) 7.2 percent of the respondents reveal an inconsistency between their behavior and perception regarding defect prediction.,0,0,0,1
1291,4) Defect prediction at the feature level is the most preferred level of granularity by practitioners.,0,0,0,1
1292,"5) During bug fixing, more than 40 percent of the respondents acknowledged that they would make a “work-around” fix rather than correct the actual error-causing code.",0,0,0,1
1293,"Through a qualitative analysis of free-form text responses, we identified reasons why practitioners are reluctant to adopt defect prediction tools.",0,1,0,0
1294,We also noted features that practitioners expect defect prediction tools to deliver.,0,0,0,1
1295,"Based on our findings, we highlight future research directions and provide recommendations for practitioners.",0,0,0,1
1296,One source of software project challenges and failures is the systematic errors introduced by human cognitive biases.,1,0,0,0
1297,"Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research.",1,0,0,0
1298,"This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state-of-the-art research and provide guidelines for future research and practise.",0,0,1,0
1299,"Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases.",0,1,0,0
1300,"Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases.",0,0,0,1
1301,"Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.",0,0,0,1
1302,"Previous approaches to dynamic taint analysis for JavaScript are implemented directly in a browser or JavaScript engine, limiting their applicability to a single platform and requiring ongoing maintenance as platforms evolve, or they require nontrivial program transformations.",1,0,0,0
1303,We present an approach that relies on instrumentation to encode taint propagation as instructions for an abstract machine.,0,0,1,0
1304,"Our approach has two key advantages: it is platform-independent and can be used with any existing JavaScript engine, and it can track taint on primitive values without requiring the introduction of wrapper objects.",0,0,1,0
1305,"Furthermore, our technique enables multiple deployment scenarios by varying when and where the generated instructions are executed and it supports indirect taint sources, i.e., situations where taint enters an application via arguments passed to dynamically registered event-listener functions.",0,0,1,0
1306,"We implemented the technique for the ECMAScript 5 language in a tool called Ichnaea, and evaluated it on 22 NPM modules containing several types of injection vulnerabilities, including 4 modules containing vulnerabilities that were not previously discovered and reported.",0,1,0,0
1307,"On these modules, run-time overheads range from 3.17x to 38.42x, which is significantly better than a previous transformation-based technique.",0,0,0,1
1308,We also report on a case study that shows how Ichnaea can be used to detect privacy leaks in a Tizen web application for the Samsung Gear S2 smart watch.,0,1,0,0
1309,Numerous software companies are adopting value-based decision making.,1,0,0,0
1310,"However, what does value mean for key stakeholders making decisions?",1,0,0,0
1311,How do different stakeholder groups understand value?,1,0,0,0
1312,"Without an explicit understanding of what value means, decisions are subject to ambiguity and vagueness, which are likely to bias them.",1,0,0,0
1313,This case study provides an in-depth analysis of key stakeholders' value propositions when selecting features for a large telecommunications company's software-intensive product.,0,0,1,0
1314,"Stakeholders' value propositions were elicited via interviews, which were analyzed using Grounded Theory coding techniques (open and selective coding).",0,1,0,0
1315,"Thirty-six value propositions were identified and classified into six dimensions: customer value, market competitiveness, economic value/profitability, cost efficiency, technology & architecture, and company strategy.",0,0,0,1
1316,"Our results show that although propositions in the customer value dimension were those mentioned the most, the concept of value for feature selection encompasses a wide range of value propositions.",0,0,0,1
1317,"Moreover, stakeholder groups focused on different and complementary value dimensions, calling to the importance of involving all key stakeholders in the decision making process.",0,0,0,1
1318,"Although our results are particularly relevant to companies similar to the one described herein, they aim to generate a learning process on value-based feature selection for practitioners and researchers in general.",0,0,0,1
1319,One of the major trends in research on Self-Protecting Systems is to use a model of the system to be protected to predict its evolution.,1,0,0,0
1320,"However, very often, devising the model requires special knowledge of mathematical frameworks, that prevents the adoption of this technique outside of the academic environment.",1,0,0,0
1321,"Furthermore, some of the proposed approaches suffer from the curse of dimensionality, as their complexity is exponential in the size of the protected system.",1,0,0,0
1322,"In this paper, we introduce a model-integrated approach for the design of Self-Protecting Systems, which automatically generates and solves Markov Decision Processes (MDPs) to obtain optimal defense strategies for systems under attack.",0,0,1,0
1323,"MDPs are created in such a way that the size of the state space does not depend on the size of the system, but on the scope of the attack, which allows us to apply it to systems of arbitrary size.",0,0,1,0
1324,"Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities.",1,0,0,0
1325,"They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities.",1,0,0,0
1326,"However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way.",1,0,0,0
1327,"To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB.",0,1,1,0
1328,We investigate elite developers’ contributing activities and their impacts on project outcomes.,0,0,1,0
1329,"Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator).",0,0,0,1
1330,"These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes.",0,0,0,1
1331,The results also provide implications for supporting these elite developers.,0,0,0,1
1332,The behavioural comparison of systems is an important concern of software engineering research.,1,0,0,0
1333,"For example, the areas of specification discovery and specification mining are concerned with measuring the consistency between a collection of execution traces and a program specification.",1,0,0,0
1334,This problem is also tackled in process mining with the help of measures that describe the quality of a process specification automatically discovered from execution logs.,1,0,0,0
1335,"Though various measures have been proposed, it was recently demonstrated that they neither fulfil essential properties, such as monotonicity, nor can they handle infinite behaviour.",1,0,0,0
1336,"In this article, we address this research problem by introducing a new framework for the definition of behavioural quotients.",0,0,1,0
1337,We prove that corresponding quotients guarantee desired properties that existing measures have failed to support.,0,0,0,1
1338,We demonstrate the application of the quotients for capturing precision and recall measures between a collection of recorded executions and a system specification.,0,0,0,1
1339,We use a prototypical implementation of these measures to contrast their monotonic assessment with measures that have been defined in prior research.,0,1,0,0
1340,"Coverage criteria provide a useful and widely used means to guide software testing; however, indiscriminately pursuing full coverage may not always be convenient or meaningful, as not all entities are of interest in any usage context.",1,0,0,0
1341,We aim at introducing a more meaningful notion of coverage that takes into account how the software is going to be used.,0,0,1,0
1342,Entities that are not going to be exercised by the user should not contribute to the coverage ratio.,0,0,1,0
1343,"We revisit the definition of coverage measures, introducing a notion of relative coverage.",0,0,1,0
1344,"According to this notion, we provide a definition and a theoretical framework of relative coverage, within which we discuss implications on testing theory and practice.",0,1,1,0
1345,"Through the evaluation of three different instances of relative coverage, we could observe that relative coverage measures provide a more effective strategy than traditional ones: we could reach higher coverage measures, and test cases selected by relative coverage could achieve higher reliability.",0,0,0,1
1346,We hint at several other useful implications of relative coverage notion on different aspects of software testing.,0,0,0,1
1347,"Automated system test generation for web/enterprise systems requires either a sequence of actions on a GUI (e.g., clicking on HTML links and form buttons) or direct HTTP calls when dealing with web services (e.g., REST and SOAP).",1,0,0,0
1348,"When doing white-box testing of such systems, their code can be analyzed, and the same type of heuristics (e.g., the branch distance) used in search-based unit testing can be employed to improve performance.",1,0,0,0
1349,"However, web/enterprise systems do often interact with a database.",1,0,0,0
1350,"To obtain higher coverage and find new faults, the state of the databases needs to be taken into account when generating white-box tests.",1,0,0,0
1351,"In this work, we present a novel heuristic to enhance search-based software testing of web/enterprise systems, which takes into account the state of the accessed databases.",0,0,1,0
1352,"Furthermore, we enable the generation of SQL data directly from the test cases.",0,0,1,0
1353,This is useful when it is too difficult or time consuming to generate the right sequence of events to put the database in the right state.,0,0,1,0
1354,"Also, it is useful when dealing with databases that are “read-only” for the system under test, and the actual data are generated by other services.",0,0,1,0
1355,"We implemented our technique as an extension of EVOMASTER, where system tests are generated in the JUnit format.",0,1,0,0
1356,"Experiments on six RESTful APIs (five open-source and one industrial) show that our novel techniques improve coverage significantly (up to +16.5%), finding seven new faults in those systems.",0,1,0,1
1357,UI design is an integral part of software development.,1,0,0,0
1358,"For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications.",1,0,0,0
1359,"However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs.",1,0,0,0
1360,"In this article, we propose a deep-learning-based UI design search engine to fill in the gap.",0,0,1,0
1361,"The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs.",0,0,1,0
1362,"We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results.",0,1,0,0
1363,Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks.,0,0,0,1
1364,Test-suite minimization is one key technique for optimizing the software testing process.,1,0,0,0
1365,"Due to the need to balance multiple factors, multi-criteria test-suite minimization (MCTSM) becomes a popular research topic in the recent decade.",1,0,0,0
1366,The MCTSM problem is typically modeled as integer linear programming (ILP) problem and solved with weighted-sum single objective approach.,1,0,0,0
1367,"However, there is no existing approach that can generate sound (i.e., being Pareto-optimal) and complete (i.e., covering the entire Pareto front) Pareto-optimal solution set, to the knowledge of the authors.",1,0,0,0
1368,"In this work, we first prove that the ILP formulation can accurately model the MCTSM problem and then propose the multi-objective integer programming (MOIP) approaches to solve it.",0,0,1,0
1369,"We apply our MOIP approaches on three specific MCTSM problems and compare the results with those of the cutting-edge methods, namely, NonlinearFormulation_LinearSolver (NF_LS) and two Multi-Objective Evolutionary Algorithms (MOEAs).",0,0,1,0
1370,"The results show that our MOIP approaches can always find sound and complete solutions on five subject programs, using similar or significantly less time than NF_LS and two MOEAs do.",0,0,0,1
1371,"The current experimental results are quite promising, and our approaches have the potential to be applied for other similar search-based software engineering problems.",0,0,0,1
1372,"Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills.",1,0,0,0
1373,The source code in programming screencasts is an important and valuable information for developers.,1,0,0,0
1374,"But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts.",1,0,0,0
1375,"Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily.",1,0,0,0
1376,"However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups.",1,0,0,0
1377,"Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images.",1,0,0,0
1378,"The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts.",1,0,0,0
1379,"In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts.",0,0,1,0
1380,"First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames.",0,0,1,0
1381,"Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor.",0,0,1,0
1382,"Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code.We conduct an experiment on 1,142 programming screencasts from YouTube.",0,0,1,0
1383,"We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames.",0,0,0,1
1384,We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words.,0,0,0,1
1385,"Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool.",0,0,1,0
1386,"Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the [email protected], 10, and 20 of 0.93, 0.81, and 0.63, respectively.",0,1,0,1
1387,We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants.,0,1,0,0
1388,This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.,0,0,0,1
1389,"Deep neural network (DNN) has become increasingly popular and DNN testing is very critical to guarantee the correctness of DNN, i.e., the accuracy of DNN in this work.",1,0,0,0
1390,"However, DNN testing suffers from a serious efficiency problem, i.e., it is costly to label each test input to know the DNN accuracy for the testing set, since labeling each test input involves multiple persons (even with domain-specific knowledge) in a manual way and the testing set is large-scale.",1,0,0,0
1391,"To relieve this problem, we propose a novel and practical approach, called PACE (which is short for Practical ACcuracy Estimation), which selects a small set of test inputs that can precisely estimate the accuracy of the whole testing set.",0,0,1,0
1392,"In this way, the labeling costs can be largely reduced by just labeling this small set of selected test inputs.",0,0,1,0
1393,"Besides achieving a precise accuracy estimation, to make PACE more practical it is also required that it is interpretable, deterministic, and as efficient as possible.",0,0,1,0
1394,"Therefore, PACE first incorporates clustering to interpretably divide test inputs with different testing capabilities (i.e., testing different functionalities of a DNN model) into different groups.",0,0,1,0
1395,"Then, PACE utilizes the MMD-critic algorithm, a state-of-the-art example-based explanation algorithm, to select prototypes (i.e., the most representative test inputs) from each group, according to the group sizes, which can reduce the impact of noise due to clustering.",0,0,1,0
1396,"Meanwhile, PACE also borrows the idea of adaptive random testing to select test inputs from the minority space (i.e., the test inputs that are not clustered into any group) to achieve great diversity under the required number of test inputs.",0,0,1,0
1397,"The two parallel selection processes (i.e., selection from both groups and the minority space) compose the final small set of selected test inputs.",0,0,1,0
1398,"We conducted an extensive study to evaluate the performance of PACE based on a comprehensive benchmark (i.e., 24 pairs of DNN models and testing sets) by considering different types of models (i.e., classification and regression models, high-accuracy and low-accuracy models, and CNN and RNN models) and different types of test inputs (i.e., original, mutated, and automatically generated test inputs).",0,1,0,0
1399,"The results demonstrate that PACE is able to precisely estimate the accuracy of the whole testing set with only 1.181%∼2.302% deviations, on average, significantly outperforming the state-of-the-art approaches.",0,0,0,1
1400,Software engineers get questions of “how much testing is enough” on a regular basis.,1,0,0,0
1401,"Existing approaches in software testing management employ experience-, risk-, or value-based analysis to prioritize and manage testing processes.",1,0,0,0
1402,"However, very few is applicable to the emerging crowdtesting paradigm to cope with extremely limited information and control over unknown, online crowdworkers.",1,0,0,0
1403,"In practice, deciding when to close a crowdtesting task is largely done by experience-based guesswork and frequently results in ineffective crowdtesting.",1,0,0,0
1404,"More specifically, it is found that an average of 32% testing cost was wasteful spending in current crowdtesting practice.",1,0,0,0
1405,"This article intends to address this challenge by introducing automated decision support for monitoring and determining appropriate time to close crowdtesting tasks.To that end, it first investigates the necessity and feasibility of close prediction of crowdtesting tasks based on an industrial dataset.",0,0,1,0
1406,"Next, it proposes a close prediction approach named iSENSE2.0, which applies incremental sampling technique to process crowdtesting reports arriving in chronological order and organizes them into fixed-sized groups as dynamic inputs.",0,0,1,0
1407,"Then, a duplicate tagger analyzes the duplicate status of received crowd reports, and a CRC-based (Capture-ReCapture) close estimator generates the close decision based on the dynamic bug arrival status.",0,0,1,0
1408,"In addition, a coverage-based sanity checker is designed to reinforce the stability and performance of close prediction.",0,0,1,0
1409,"Finally, the evaluation of iSENSE2.0 is conducted on 56,920 reports of 306 crowdtesting tasks from one of the largest crowdtesting platforms.",0,1,0,0
1410,The results show that a median of 100% bugs can be detected with 30% saved cost.,0,0,0,1
1411,"The performance of iSENSE2.0 does not demonstrate significant difference with the state-of-the-art approach iSENSE, while the later one relies on the duplicate tag, which is generally considered as time-consuming and tedious to obtain.",0,0,0,1
1412,Task-related conflict and person-related conflict in software testing are inevitable and can impact the effectiveness and efficiency of the software development process.,1,0,0,0
1413,"The dimensionality of conflict in software testing is reasonably well understood, although in past research both types of conflict have frequently been modeled as reflective constructs that can obstruct the effectiveness of their use as organizational assessment and training tools.",1,0,0,0
1414,One contribution of this study is an empirical model of conflict sources in software engineering; such sources of conflict threaten to derail efficient software development outcomes in firms.,0,0,1,0
1415,A second contribution of this research is the development of a formative measurement model for purposes of development of assessing task conflict and person conflict in software teams.,0,0,1,0
1416,These validated measures can be utilized as training and development instruments for on-the-job remediation of development team conflict.,0,0,1,0
1417,"As is indicated in the organizational behavior and software engineering literature, deploying valid measures of workplace stressors such as conflict can lead to the managerial application of effective strategies and tactics to improve workplace morale and satisfaction, to the great benefit of productivity and retention.",1,0,0,0
1418,"Today, there are millions of third-party Android applications.",1,0,0,0
1419,Some of them are buggy or even malicious.,1,0,0,0
1420,"To identify such applications, novel frameworks for automated black-box testing and dynamic analysis are being developed by the Android community.",1,0,0,0
1421,Code coverage is one of the most common metrics for evaluating effectiveness of these frameworks.,1,0,0,0
1422,"Furthermore, code coverage is used as a fitness function for guiding evolutionary and fuzzy testing techniques.",1,0,0,0
1423,"However, there are no reliable tools for measuring fine-grained code coverage in black-box Android app testing.",1,0,0,0
1424,"We present the Android Code coVerage Tool, ACVTool for short, that instruments Android apps and measures code coverage in the black-box setting at class, method and instruction granularity.",0,0,1,0
1425,ACVTool has successfully instrumented 96.9% of apps in our experiments.,0,1,0,1
1426,"It introduces a negligible instrumentation time overhead, and its runtime overhead is acceptable for automated testing tools.",0,0,0,1
1427,"We demonstrate practical value of ACVTool in a large-scale experiment with Sapienz, a state-of-the-art automated testing tool.",0,1,0,0
1428,"Using ACVTool on the same cohort of apps, we have compared different coverage granularities applied by Sapienz in terms of the found amount of crashes.",0,1,0,0
1429,Our results show that none of the applied coverage granularities clearly outperforms others in this aspect.,0,0,0,1
1430,Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet.,1,0,0,0
1431,The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help.,1,0,0,0
1432,Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow.,1,0,0,0
1433,These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process.,1,0,0,0
1434,"Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach.",1,0,1,0
1435,"Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem.",0,0,1,0
1436,"We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation.",0,1,0,1
1437,We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.,0,0,1,0
1438,Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties.,1,0,0,0
1439,Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties.,1,0,0,0
1440,"Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks, which may enable attackers to steal valuable assets of involving parties.",1,0,0,0
1441,"There is, therefore, a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed.",1,0,0,0
1442,"In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware.",0,0,1,0
1443,Our repair method is search-based and searches among mutations of the buggy contract.,0,0,1,0
1444,Our method also considers the gas usage of the candidate patches by leveraging our novel notion of gas dominance relationship.,0,0,1,0
1445,"We have made our smart contract repair tool SCRepair available open-source, for investigation by the wider community.",0,0,1,0
1446,Refactoring aims at improving code non-functional attributes without modifying its external behavior.,1,0,0,0
1447,Previous studies investigated the motivations behind refactoring by surveying developers.,1,0,0,0
1448,"With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects.",0,1,1,0
1449,"First, we mine 287,813 refactoring operations performed in the history of 150 systems.",0,1,0,0
1450,"Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics.",0,1,0,0
1451,"Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication).",0,1,0,0
1452,"Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.",0,0,0,1
1453,"Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation.",1,0,0,0
1454,"It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another.",1,0,0,0
1455,"However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing.",1,0,0,0
1456,"In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation.",0,0,1,0
1457,This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead.,0,0,1,0
1458,We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool.,0,1,0,0
1459,"In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more).",0,0,0,1
1460,"The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.",0,0,1,0
1461,Learning representation for source code is a foundation of many program analysis tasks.,1,0,0,0
1462,"In recent years, neural networks have already shown success in this area, but most existing models did not make full use of the unique structural information of programs.",1,0,0,0
1463,"Although abstract syntax tree (AST)-based neural models can handle the tree structure in the source code, they cannot capture the richness of different types of substructure in programs.",1,0,0,0
1464,"In this article, we propose a modular tree network that dynamically composes different neural network units into tree structures based on the input AST.",0,0,1,0
1465,"Different from previous tree-structural neural network models, a modular tree network can capture the semantic differences between types of AST substructures.",0,0,1,0
1466,We evaluate our model on two tasks: program classification and code clone detection.,0,1,0,0
1467,"Our model achieves the best performance compared with state-of-the-art approaches in both tasks, showing the advantage of leveraging more elaborate structure information of the source code.",0,0,0,1
1468,"
The rise of highly configurable complex software and its widespread usage requires design of efficient testing methodology.",1,0,0,0
1469,t-wise coverage is a leading metric to measure the quality of the testing suite and the underlying test generation engine.,1,0,0,0
1470,"While uniform sampling-based test generation is widely believed to be the state of the art approach to achieve t-wise coverage in presence of constraints on the set of configurations, such a scheme often fails to achieve high t-wise coverage in presence of complex constraints.",1,0,0,0
1471,"In this work, we propose a novel approach Baital, based on adaptive weighted sampling using literal weighted functions, to generate test sets with high t-wise coverage.",0,0,1,0
1472,We demonstrate that our approach reaches significantly higher t-wise coverage than uniform sampling.,0,0,0,1
1473,"The novel usage of literal weighted sampling leaves open several interesting directions, empirical as well as theoretical, for future research.",0,0,0,1
1474,"
Although deep neural networks have been very successful in image-classification tasks, they are prone to adversarial attacks.",1,0,0,0
1475,"To generate adversarial inputs, there has emerged a wide variety of techniques, such as black- and whitebox attacks for neural networks.",1,0,0,0
1476,"In this paper, we present DeepSearch, a novel fuzzing-based, query-efficient, blackbox attack for image classifiers.",0,0,1,0
1477,"Despite its simplicity, DeepSearch is shown to be more effective in finding adversarial inputs than state-of-the-art blackbox approaches.",0,0,0,1
1478,DeepSearch is additionally able to generate the most subtle adversarial inputs in comparison to these approaches.,0,0,0,1
1479,"
Good documentation offers the promise of enabling developers to easily understand design decisions.",1,0,0,0
1480,"Unfortunately, in practice, design documents are often rarely updated, becoming inaccurate, incomplete, and untrustworthy.",1,0,0,0
1481,A better solution is to enable developers to write down design rules which are checked against code for consistency.,1,0,0,0
1482,"But existing rule checkers require learning specialized query languages or program analysis frameworks, creating a barrier to writing project-specific rules.",1,0,0,0
1483,We introduce two new techniques for authoring design rules: snippet-based authoring and semi-natural-language authoring.,0,0,1,0
1484,"In snippet-based authoring, developers specify characteristics of elements to match by writing partial code snippets.",0,0,0,1
1485,"In semi-natural language authoring, a textual representation offers a representation for understanding design rules and resolving ambiguities.",0,0,0,1
1486,We implemented these approaches in RulePad.,0,1,0,0
1487,"To evaluate RulePad, we conducted a between-subjects study with 14 participants comparing RulePad to the PMD Designer, a utility for writing rules in a popular rule checker.",0,1,0,0
1488,We found that those with RulePad were able to successfully author 13 times more query elements in significantly less time and reported being significantly more willing to use RulePad in their everyday work.,0,0,0,1
1489,"
The Android ecosystem offers different facilities to enable communication among app components and across apps to ensure that rich services can be composed through functionality reuse.",1,0,0,0
1490,"At the heart of this system is the Inter-component communication (ICC) scheme, which has been largely studied in the literature.",1,0,0,0
1491,"Less known in the community is another powerful mechanism that allows for direct inter-app code invocation which opens up for different reuse scenarios, both legitimate or malicious.",1,0,0,0
1492,"This paper exposes the general workflow for this mechanism, which beyond ICCs, enables app developers to access and invoke functionalities (either entire Java classes, methods or object fields) implemented in other apps using official Android APIs.",0,0,1,0
1493,"We experimentally showcase how this reuse mechanism can be leveraged to “plagiarize"" supposedly-protected functionalities.",0,1,0,0
1494,"Typically, we were able to leverage this mechanism to bypass security guards that a popular video broadcaster has placed for preventing access to its video database from outside its provided app.",0,0,0,1
1495,"We further contribute with a static analysis toolkit, named DICIDer, for detecting direct inter-app code invocations in apps.",0,0,1,0
1496,An empirical analysis of the usage prevalence of this reuse mechanism is then conducted.,0,1,0,0
1497,"Finally, we discuss the usage contexts as well as the implications of this studied reuse mechanism.",0,0,0,1
1498,"
Software systems are designed and implemented with assumptions about the environment.",1,0,0,0
1499,"However, once the system is deployed, the actual environment may deviate from its expected behavior, possibly undermining desired properties of the system.",1,0,0,0
1500,"To enable systematic design of systems that are robust against potential environmental deviations, we propose a rigorous notion of robustness for software systems.",0,0,1,0
1501,"In particular, the robustness of a system is defined as the largest set of deviating environmental behaviors under which the system is capable of guaranteeing a desired property.",1,0,0,0
1502,"We describe a new set of design analysis problems based on our notion of robustness, and a technique for automatically computing robustness of a system given its behavior description.",0,0,1,0
1503,We demonstrate potential applications of our robustness notion on two case studies involving network protocols and safety-critical interfaces.,0,1,0,0
1504,"
JavaScript is widely used for implementing client-side web applications, and it is common to include JavaScript code from many different hosts.",1,0,0,0
1505,"However, in a web browser, all the scripts loaded in the same frame share a single global namespace.",1,0,0,0
1506,"As a result, a script may read or even overwrite the global objects or functions in other scripts, causing unexpected behaviors.",1,0,0,0
1507,"For example, a script can redefine a function in a different script as an object, so that any call of that function would cause an exception at run time.",1,0,0,0
1508,We systematically investigate the client-side JavaScript code integrity problem caused by JavaScript global identifier conflicts in this paper.,0,0,1,0
1509,"We developed a browser-based analysis framework, JSObserver, to collect and analyze the write operations to global memory locations by JavaScript code.",0,1,0,0
1510,"We identified three categories of conflicts using JSObserver on the Alexa top 100K websites, and detected 145,918 conflicts on 31,615 websites.",0,0,0,1
1511,We reveal that JavaScript global identifier conflicts are prevalent and could cause behavior deviation at run time.,0,0,0,1
1512,"In particular, we discovered that 1,611 redefined functions were called after being overwritten, and many scripts modified the value of cookies or redefined cookie-related functions.",0,0,0,1
1513,Our research demonstrated that JavaScript global identifier conflict is an emerging threat to both the web users and the integrity of web applications.,0,0,0,1
1514,"
Keeping a good influx of newcomers is critical for open source software projects' survival, while newcomers face many barriers to contributing to a project for the first time.",1,0,0,0
1515,"To support newcomers onboarding, GitHub encourages projects to apply labels such as good first issue (GFI) to tag issues suitable for newcomers.",1,0,0,0
1516,"However, many newcomers still fail to contribute even after many attempts, which not only reduces the enthusiasm of newcomers to contribute but makes the efforts of project members in vain.",1,0,0,0
1517,"To better support the onboarding of newcomers, this paper reports a preliminary study on this mechanism from its application status, effect, problems, and best practices.",0,0,1,0
1518,"By analyzing 9,368 GFIs from 816 popular GitHub projects and conducting email surveys with newcomers and project members, we obtain the following results.",0,1,0,0
1519,"We find that more and more projects are applying this mechanism in the past decade, especially the popular projects.",0,0,0,1
1520,"Compared to common issues, GFIs usually need more days to be solved.",0,0,0,1
1521,"While some newcomers really join the projects through GFIs, almost half of GFIs are not solved by newcomers.",0,0,0,1
1522,"We also discover a series of problems covering mechanism (e.g., inappropriate GFIs), project (e.g., insufficient GFIs) and newcomer (e.g., uneven skills) that makes this mechanism ineffective.",0,0,0,1
1523,"We discover the practices that may address the problems, including identifying GFIs that have informative description and available support, and require limited scope and skill, etc. Newcomer onboarding is an important but challenging question in open source projects and our work enables a better understanding of GFI mechanism and its problems, as well as highlights ways in improving them.",0,0,0,1
1524,"
Loop invariant generation has long been a challenging problem.",1,0,0,0
1525,Black-box learning has recently emerged as a promising method for inferring loop invariants.,1,0,0,0
1526,"However, the performance depends heavily on the quality of collected examples.",1,0,0,0
1527,"In many cases, only after tens or even hundreds of constraint queries, can a feasible invariant be successfully inferred.",1,0,0,0
1528,"To reduce the gigantic number of constraint queries and improve the performance of black-box learning, we introduce interval counterexamples into the learning framework.",0,0,1,0
1529,Each interval counterexample represents a set of counterexamples from constraint solvers.,0,0,0,1
1530,We propose three different generalization techniques to compute interval counterexamples.,0,0,0,1
1531,The existing decision tree algorithm is also improved to adapt interval counterexamples.,0,0,0,1
1532,We evaluate our techniques and report over 40% improvement on learning rounds and verification time over the state-of-the-art approach.,0,0,0,1
1533,"
Machine translation software has become heavily integrated into our daily lives due to the recent improvement in the performance of deep neural networks.",1,0,0,0
1534,"However, machine translation software has been shown to regularly return erroneous translations, which can lead to harmful consequences such as economic loss and political conflicts.",1,0,0,0
1535,"Additionally, due to the complexity of the underlying neural models, testing machine translation systems presents new challenges.",1,0,0,0
1536,"To address this problem, we introduce a novel methodology called PatInv.",0,0,1,0
1537,The main intuition behind PatInv is that sentences with different meanings should not have the same translation.,0,0,1,0
1538,"Under this general idea, we provide two realizations of PatInv that given an arbitrary sentence, generate syntactically similar but semantically different sentences by: (1) replacing one word in the sentence using a masked language model or (2) removing one word or phrase from the sentence based on its constituency structure.",0,1,0,0
1539,We then test whether the returned translations are the same for the original and modified sentences.,0,1,0,0
1540,We have applied PatInv to test Google Translate and Bing Microsoft Translator using 200 English sentences.,0,1,0,0
1541,Two language settings are considered: English-Hindi (En-Hi) and English-Chinese (En-Zh).,0,1,0,0
1542,"The results show that PatInv can accurately find 308 erroneous translations in Google Translate and 223 erroneous translations in Bing Microsoft Translator, most of which cannot be found by the state-of-the-art approaches.",0,0,0,1
1543,"
Mutation testing research has often used the number of mutants as a surrogate measure for the true execution cost of generating and executing mutants.",1,0,0,0
1544,This poses a potential threat to the validity of the scientific findings reported in the literature.,1,0,0,0
1545,"Out of 75 works surveyed in this paper, we found that 54 (72%) are vulnerable to this threat.",0,1,0,1
1546,"To investigate the magnitude of the threat, we conducted an empirical evaluation using 10 real-world programs.",0,1,1,0
1547,"The results reveal that: i) percentages of randomly sampled mutants differ from the true execution time, on average, by 44%, varying in difference from 19% to 91%; ii) errors arising from using the surrogate correlate with program size (ρ = 0.74) and number of mutants (ρ = 0.76), making the problem more pernicious for more realistic programs; iii) scientific findings concerning sampling strategies would have approximately 37% rank disagreement, indicating potentially dramatic impact on experiment validity.",0,0,0,1
1548,"To investigate whether this threat matters in practice, we reproduced a seminal study on Selective Mutation (widely relied upon for more than two decades).",0,1,0,0
1549,The impact is stark: an inconclusive scientific finding using the surrogate is transformed to an unequivocal finding when using the true execution cost.,0,0,0,1
1550,"
Robots that support humans by performing useful tasks (a.k.a., service robots) are booming worldwide.",1,0,0,0
1551,"In contrast to industrial robots, the development of service robots comes with severe software engineering challenges, since they require high levels of robustness and autonomy to operate in highly heterogeneous environments.",1,0,0,0
1552,"As a domain with critical safety implications, service robotics faces a need for sound software development practices.",1,0,0,0
1553,"In this paper, we present the first large-scale empirical study to assess the state of the art and practice of robotics software engineering.",1,0,1,0
1554,We conducted 18 semi-structured interviews with industrial practitioners working in 15 companies from 9 different countries and a survey with 156 respondents from 26 countries from the robotics domain.,0,1,0,0
1555,"Our results provide a comprehensive picture of (i) the practices applied by robotics industrial and academic practitioners, including processes, paradigms, languages, tools, frameworks, and reuse practices, (ii) the distinguishing characteristics of robotics software engineering, and (iii) recurrent challenges usually faced, together with adopted solutions.",0,0,0,1
1556,"The paper concludes by discussing observations, derived hypotheses, and proposed actions for researchers and practitioners.",0,0,0,1
1557,"
Equivalence checking techniques help establish whether two versions of a program exhibit the same behavior.",1,0,0,0
1558,The majority of popular techniques for formally proving/refuting equivalence relies on symbolic execution – a static analysis approach that reasons about program behaviors in terms of symbolic input variables.,1,0,0,0
1559,"Yet, symbolic execution is difficult to scale in practice due to complex programming constructs, such as loops and non-linear arithmetic.",1,0,0,0
1560,"This paper proposes an approach, named ARDiff, for improving the scalability of symbolic-execution-based equivalence checking techniques when comparing syntactically-similar versions of a program, e.g., for verifying the correctness of code upgrades and refactoring.",0,0,1,0
1561,"Our approach relies on a set of novel heuristics to determine which parts of the versions’ common code can be effectively pruned during the analysis, reducing the analysis complexity without sacrificing its effectiveness.",0,1,0,0
1562,"Furthermore, we devise a new equivalence checking benchmark, extending existing benchmarks with a set of real-life methods containing complex math functions and loops.",0,1,0,0
1563,"We evaluate the effectiveness and efficiency of ARDiff on this benchmark and show that it outperforms existing method-level equivalence checking techniques by solving 86% of all equivalent and 55% of non-equivalent cases, compared with 47% to 69% for equivalent and 38% to 52% for non-equivalent cases in related work.",0,0,0,1
1564,"
In large-scale cloud systems, unplanned service interruptions and outages may cause severe degradation of service availability.",1,0,0,0
1565,"Such incidents can occur in a bursty manner, which will deteriorate user satisfaction.",1,0,0,0
1566,Identifying incidents rapidly and accurately is critical to the operation and maintenance of a cloud system.,1,0,0,0
1567,"In industrial practice, incidents are typically detected through analyzing the issue reports, which are generated over time by monitoring cloud services.",1,0,0,0
1568,Identifying incidents in a large number of issue reports is quite challenging.,1,0,0,0
1569,An issue report is typically multi-dimensional: it has many categorical attributes.,1,0,0,0
1570,It is difficult to identify a specific attribute combination that indicates an incident.,1,0,0,0
1571,"Existing methods generally rely on pruning-based search, which is time-consuming given high-dimensional data, thus not practical to incident detection in large-scale cloud systems.",1,0,0,0
1572,"In this paper, we propose MID (Multi-dimensional Incident Detection), a novel framework for identifying incidents from large-amount, multi-dimensional issue reports effectively and efficiently.",0,0,1,0
1573,Key to the MID design is encoding the problem into a combinatorial optimization problem.,0,1,0,0
1574,"Then a specific-tailored meta-heuristic search method is designed, which can rapidly identify attribute combinations that indicate incidents.",0,1,0,0
1575,We evaluate MID with extensive experiments using both synthetic data and real-world data collected from a large-scale production cloud system.,0,1,0,0
1576,The experimental results show that MID significantly outperforms the current state-of-the-art methods in terms of effectiveness and efficiency.,0,0,0,1
1577,"Additionally, MID has been successfully applied to Microsoft's cloud systems and helped greatly reduce manual maintenance effort.",0,0,0,1
1578,"
We present HOMI, a new technique to enhance symbolic execution by maintaining only a small number of promising states.",0,0,1,0
1579,"In practice, symbolic execution typically maintains as many states as possible in a fear of losing important states.",1,0,0,0
1580,"In this paper, however, we show that only a tiny subset of the states plays a significant role in increasing code coverage or reaching bug points.",0,0,0,1
1581,"Based on this observation, HOMI aims to minimize the total number of states while keeping “promising” states during symbolic execution.",0,1,0,0
1582,We identify promising states by a learning algorithm that continuously updates the probabilistic pruning strategy based on data accumulated during the testing process.,0,1,0,0
1583,Experimental results show that HOMI greatly increases code coverage and the ability to find bugs of KLEE on open-source C programs.,0,0,0,1
1584,"
Fuzz testing has been proved its effectiveness in discovering software vulnerabilities.",1,0,0,0
1585,"Empowered its randomness nature along with a coverage-guiding feature, fuzzing has been identified a vast number of vulnerabilities in real-world programs.",1,0,0,0
1586,This paper begins with an observation that the design of the current state-of-the-art fuzzers is not well suited for a particular (but yet important) set of software programs.,1,0,0,0
1587,"Specifically, current fuzzers have limitations in fuzzing programs serving multiple purposes, where each purpose is controlled by extra options.",1,0,0,0
1588,"This paper proposes CrFuzz, which overcomes this limitation.",0,0,1,0
1589,CrFuzz designs a clustering analysis to automatically predict if a newly given input would be accepted or not by a target program.,0,1,0,0
1590,"Exploiting this prediction capability, CrFuzz is designed to efficiently explore the programs with multiple purposes.",0,1,0,0
1591,"We employed CrFuzz for three state-of-the-art fuzzers, AFL, QSYM, and MOpt, and CrFuzz-augmented versions have shown 19.3% and 5.68% better path and edge coverage on average.",0,0,0,1
1592,"More importantly, during two weeks of long-running experiments, CrFuzz discovered 277 previously unknown vulnerabilities where 212 of those are already confirmed and fixed by the respected vendors.",0,0,0,1
1593,"We would like to emphasize that many of these vulnerabilities were discoverd from FFMpeg, ImageMagick, and Graphicsmagick, all of which are targets of Google's OSS-Fuzz project and thus heavily fuzzed for last three years by far.",0,0,0,1
1594,"Nevertheless, CrFuzz identified a remarkable number of vulnerabilities, demonstrating its effectiveness of vulnerability finding capability.",0,0,0,1
1595,"
In large-scale online service systems, incidents occur frequently due to a variety of causes, from updates of software and hardware to changes in operation environment.",1,0,0,0
1596,These incidents could significantly degrade system’s availability and customers’ satisfaction.,1,0,0,0
1597,Some incidents are linked because they are duplicate or inter-related.,1,0,0,0
1598,The linked incidents can greatly help on-call engineers find mitigation solutions and identify the root causes.,1,0,0,0
1599,"In this work, we investigate the incidents and their links in a representative real-world incident management (IcM) system.",0,0,1,0
1600,"Based on the identified indicators of linked incidents, we further propose LiDAR (Linked Incident identification with DAta-driven Representation), a deep learning based approach to incident linking.",0,0,1,0
1601,"More specifically, we incorporate the textual description of incidents and structural information extracted from historical linked incidents to identify possible links among a large number of incidents.",0,0,1,0
1602,"To show the effectiveness of our method, we apply our method to a real-world IcM system and find that our method outperforms other state-of-the-art methods.",0,1,0,0
1603,"
Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage.",1,0,0,0
1604,NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite.,1,0,0,0
1605,"In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms.",0,0,1,0
1606,"We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions.",0,0,1,0
1607,"Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences.",0,0,0,1
1608,"Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.",0,0,0,1
1609,"
Smart contracts are computer programs allowing users to define and execute transactions automatically on top of the blockchain platform.",1,0,0,0
1610,Many of such smart contracts can be viewed as games.,1,0,0,0
1611,"A game-like contract accepts inputs from multiple participants, and upon ending, automatically derives an outcome while distributing assets according to some predefined rules.",1,0,0,0
1612,"Without clear understanding of the game rules, participants may suffer from fraudulent advertisements and financial losses.",1,0,0,0
1613,"In this paper, we present a framework to perform (semi-)automated verification of smart contract fairness, whose results can be used to refute false claims with concrete examples or certify contract implementations with respect to desired fairness properties.",0,0,1,0
1614,"We implement FairCon, which is able to check fairness properties including truthfulness, efficiency, optimality, and collusion-freeness for Ethereum smart contracts.",0,1,0,0
1615,We evaluate FairCon on a set of real-world benchmarks and the experiment result indicates that FairCon is effective in detecting property violations and able to prove fairness for common types of contracts.,0,1,0,1
1616,"
Exception handling is an effective mechanism to avoid unexpected runtime errors.",1,0,0,0
1617,"However, novice programmers might fail to handle exceptions properly, causing serious errors like system crashing or resource leaking.",1,0,0,0
1618,"In this paper, we introduce FuzzyCatch, a code recommendation tool for handling exceptions.",0,0,1,0
1619,"Based on fuzzy logic, FuzzyCatch can predict if a runtime exception would occur in a given code snippet and recommend code to handle that exception.",0,1,0,0
1620,FuzzyCatch is implemented as a plugin for Android Studio.,0,1,0,0
1621,The empirical evaluation suggests that FuzzyCatch is highly effective.,0,1,0,1
1622,"For example, it has top-1 accuracy of 77% on recommending what exception to catch in a try catch block and of 70% on recommending what method should be called when such an exception occurs.",0,0,0,1
1623,FuzzyCatch also achieves a high level of accuracy and outperforms baselines significantly on detecting and fixing real exception bugs.,0,0,0,1
1624,"
Software reuse lowers development costs and improves the quality of software systems.",1,0,0,0
1625,Two strategies are common: clone & own (copying and adapting a system) and platform-oriented reuse (building a configurable platform).,1,0,0,0
1626,"The former is readily available, flexible, and initially cheap, but does not scale with the frequency of reuse, imposing high maintenance costs.",1,0,0,0
1627,"The latter scales, but imposes high upfront investments for building the platform, and reduces flexibility.",1,0,0,0
1628,"As such, each strategy has distinctive advantages and disadvantages, imposing different development activities and software architectures.",1,0,0,0
1629,Deciding for one strategy is a core decision with long-term impact on an organization’s software development.,1,0,0,0
1630,"Unfortunately, the strategies’ costs are not well-understood - not surprisingly, given the lack of systematically elicited empirical data, which is difficult to collect.",1,0,0,0
1631,"We present an empirical study of the development activities, costs, cost factors, and benefits associated with either reuse strategy.",0,1,1,0
1632,"For this purpose, we combine quantitative and qualitative data that we triangulated from 26 interviews at a large organization and a systematic literature review covering 57 publications.",0,1,0,0
1633,Our study both confirms and refutes common hypotheses on software reuse.,0,0,0,1
1634,"For instance, we confirm that developing for platform-oriented reuse is more expensive, but simultaneously reduces reuse costs; and that platform-orientation results in higher code quality compared to clone & own.",0,0,0,1
1635,"Surprisingly, refuting common hypotheses, we find that change propagation can be more expensive in a platform, that platforms can facilitate the advancement into innovative markets, and that there is no strict distinction of clone & own and platform-oriented reuse in practice.",0,0,0,1
1636,"
A large percentage of real-world software configuration issues, such as misconfigurations, involve multiple interdependent configuration parameters.",1,0,0,0
1637,"However, existing techniques and tools either do not consider dependencies among configuration parameters— termed configuration dependencies—or rely on one or two dependency types and code patterns as input.",1,0,0,0
1638,"Without rigorous understanding of configuration dependencies, it is hard to deal with many resulting configuration issues.",1,0,0,0
1639,"This paper presents our study of software configuration dependencies in 16 widely-used cloud and datacenter systems, including dependencies within and across software components.",0,0,1,0
1640,"To understand types of configuration dependencies, we conduct an exhaustive search of descriptions in structured configuration metadata and unstructured user manuals.",0,1,0,0
1641,We find and manually analyze 521 configuration dependencies.,0,1,0,0
1642,We define five types of configuration dependencies and identify their common code patterns.,0,0,0,1
1643,We report on consequences of not satisfying these dependencies and current software engineering practices for handling the consequences.,0,0,0,1
1644,"We mechanize the knowledge gained from our study in a tool, cDep, which detects configuration dependencies.",0,0,0,1
1645,cDep automatically discovers five types of configuration dependencies from bytecode using static program analysis.,0,0,0,1
1646,We apply cDep to the eight Java and Scala software systems in our study.,0,1,0,0
1647,cDep finds 87.9% (275/313) of the related subset of dependencies from our study.,0,0,0,1
1648,"cDep also finds 448 previously undocumented dependencies, with a 6.0% average false positive rate.",0,0,0,1
1649,"Overall, our results show that configuration dependencies are more prevalent and diverse than previously reported and should henceforth be considered a first-class issue in software configuration engineering.",0,0,0,1
1650,"
Deep neural networks (DNNs) have been widely applied in the software development process to automatically learn patterns from massive data.",1,0,0,0
1651,"However, many applications still make decisions based on rules that are manually crafted and verified by domain experts due to safety or security concerns.",1,0,0,0
1652,"In this paper, we aim to close the gap between DNNs and rule-based systems by automating the rule generation process via extracting knowledge from well-trained DNNs.",0,0,1,0
1653,Existing techniques with similar purposes either rely on specific DNNs input instances or use inherently unstable random sampling of the input space.,1,0,0,0
1654,"Therefore, these approaches either limit the exploration area to a local decision-space of the DNNs or fail to converge to a consistent set of rules.",1,0,0,0
1655,The resulting rules thus lack representativeness and stability.,1,0,0,0
1656,"In this paper, we address the two aforementioned shortcomings by discovering a global property of the DNNs and use it to remodel the DNNs decision-boundary.",0,0,1,0
1657,"We name this property as the activation probability, and show that this property is stable.",0,0,0,1
1658,"With this insight, we propose an approach named DENAS including a novel rule-generation algorithm.",0,0,1,0
1659,Our proposed algorithm approximates the non-linear decision boundary of DNNs by iteratively superimposing a linearized optimization function.,0,0,1,0
1660,"We evaluate the representativeness, stability, and accuracy of DENAS against five state-of-the-art techniques (LEMNA, Gradient, IG, DeepTaylor, and DTExtract) on three software engineering and security applications: Binary analysis, PDF malware detection, and Android malware detection.",0,1,0,0
1661,Our results show that DENAS can generate more representative rules consistently in a more stable manner over other approaches.,0,0,0,1
1662,We further offer case studies that demonstrate the applications of DENAS such as debugging faults in the DNNs and generating signatures that can detect zero-day malware.,0,0,0,1
1663,"
We present a new framework and associated synthesis algorithms for program synthesis over noisy data, i.e., data that may contain incorrect/corrupted input-output examples.",0,0,1,0
1664,This framework is based on an extension of finite tree automata called state-weighted finite tree automata.,0,0,1,0
1665,We show how to apply this framework to formulate and solve a variety of program synthesis problems over noisy data.,0,1,0,0
1666,"Results from our implemented system running on problems from the SyGuS 2018 benchmark suite highlight its ability to successfully synthesize programs in the face of noisy data sets, including the ability to synthesize a correct program even when every input-output example in the data set is corrupted.",0,0,0,1
1667,"
The Intel Security Guard Extensions (SGX) architecture enables the abstraction of enclaved execution, using which an application can protect its code and data from powerful adversaries, including system software that executes with the highest processor privilege.",1,0,0,0
1668,"While the Intel SGX architecture exports an ISA with low-level instructions that enable applications to create enclaves, the task of writing applications using this ISA has been left to the software community.",1,0,0,0
1669,We consider the problem of porting legacy applications to SGX enclaves.,0,0,1,0
1670,"In the approximately four years to date since the Intel SGX became commercially available, the community has developed three different models to port applications to enclaves---the library OS, the library wrapper, and the instruction wrapper models.",1,0,0,0
1671,"In this paper, we conduct an empirical evaluation of the merits and costs of each model.",0,0,1,0
1672,"We report on our attempt to port a handful of real-world application benchmarks (including OpenSSL, Memcached, a Web server and a Python interpreter) to SGX enclaves using prototypes that embody each of the above models.",0,1,0,0
1673,"Our evaluation focuses on the merits and costs of each of these models from the perspective of the effort required to port code under each of these models, the effort to re-engineer an application to work with enclaves, the security offered by each model, and the runtime performance of the applications under these models.",0,1,0,0
1674,"
This paper discusses the problem of testing the performance of the adaptation layer in a self-adaptive system.",1,0,0,0
1675,"The problem is notoriously hard, due to the high degree of uncertainty and variability inherent in an adaptive software application.",1,0,0,0
1676,"In particular, providing any type of formal guarantee for this problem is extremely difficult.",1,0,0,0
1677,In this paper we propose the use of a rigorous probabilistic approach to overcome the mentioned difficulties and provide probabilistic guarantees on the software performance.,0,0,1,0
1678,We describe the set up needed for the application of a probabilistic approach.,0,1,0,0
1679,"We then discuss the traditional tools from statistics that could be applied to analyse the results, highlighting their limitations and motivating why they are unsuitable for the given problem.",0,0,1,0
1680,We propose the use of a novel tool – the scenario theory – to overcome said limitations.,0,0,1,0
1681,"We conclude the paper with a thorough empirical evaluation of the proposed approach, using two adaptive software applications: the Tele-Assistance Service and the Self-Adaptive Video Encoder.",0,1,0,0
1682,"With the first, we empirically expose the trade-off between data collection and confidence in the testing campaign.",0,0,0,1
1683,"With the second, we demonstrate how to compare different adaptation strategies.",0,0,0,1
1684,"
Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task.",1,0,0,0
1685,"It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation.",1,0,0,0
1686,"Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data.",1,0,0,0
1687,"Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task.",1,0,0,0
1688,"We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods.",0,0,1,0
1689,This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods.,0,0,1,0
1690,"We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.",0,1,0,1
1691,"
A program fails.",1,0,0,0
1692,Under which circumstances does the failure occur?,1,0,0,0
1693,Our Alhazenapproach starts with a run that exhibits a particular behavior and automatically determines input features associated with the behavior in question: (1) We use a grammar to parse the input into individual elements.,0,1,0,0
1694,(2) We use a decision tree learner to observe and learn which input elements are associated with the behavior in question.,0,1,0,0
1695,(3) We use the grammar to generate additional inputs to further strengthen or refute hypotheses as learned associations.,0,1,0,0
1696,"(4) By repeating steps 2 and 3, we obtain a theory that explains and predicts the given behavior.",0,1,0,0
1697,"In our evaluation using inputs for find, grep, NetHack, and a JavaScript transpiler, the theories produced by Alhazen predict and produce failures with high accuracy and allow developers to focus on a small set of input features: “grep fails whenever the --fixed-strings option is used in conjunction with an empty search string.”",0,0,0,1
1698,"
Mastering the knowledge about security-sensitive functions that can potentially result in bugs is valuable to detect them.",1,0,0,0
1699,"However, identifying this kind of functions is not a trivial task.",1,0,0,0
1700,Introducing machine learning-based techniques to do the task is a natural choice.,1,0,0,0
1701,"Unfortunately, the approach also requires considerable prior knowledge, e.g., sufficient labelled training samples.",1,0,0,0
1702,"In practice, the requirement is often hard to meet.",1,0,0,0
1703,"In this paper, to solve the problem, we propose a novel and practical method called SinkFinder to automatically discover function pairs that we are interested in, which only requires very limited prior knowledge.",0,0,1,0
1704,SinkFinder first takes just one pair of well-known interesting functions as the initial seed to infer enough positive and negative training samples by means of sub-word word embedding.,0,1,0,0
1705,"By using these samples, a support vector machine classifier is trained to identify more interesting function pairs.",0,1,0,0
1706,"Finally, checkers equipped with the obtained knowledge can be easily developed to detect bugs in target systems.",0,1,0,0
1707,"The experiments demonstrate that SinkFinder can successfully discover hundreds of interesting functions and detect dozens of previously unknown bugs from large-scale systems, such as Linux, OpenSSL and PostgreSQL.",0,0,0,1
1708,"
With the increasing adoption of Deep Learning (DL) for critical tasks, such as autonomous driving, the evaluation of the quality of systems that rely on DL has become crucial.",1,0,0,0
1709,"Once trained, DL systems produce an output for any arbitrary numeric vector provided as input, regardless of whether it is within or outside the validity domain of the system under test.",1,0,0,0
1710,"Hence, the quality of such systems is determined by the intersection between their validity domain and the regions where their outputs exhibit a misbehaviour.",1,0,0,0
1711,"In this paper, we introduce the notion of frontier of behaviours, i.e., the inputs at which the DL system starts to misbehave.",0,0,1,0
1712,"If the frontier of misbehaviours is outside the validity domain of the system, the quality check is passed.",0,1,0,0
1713,"Otherwise, the inputs at the intersection represent quality deficiencies of the system.",0,1,0,0
1714,"We developed DeepJanus, a search-based tool that generates frontier inputs for DL systems.",0,0,1,0
1715,"The experimental results obtained for the lane keeping component of a self-driving car show that the frontier of a well trained system contains almost exclusively unrealistic roads that violate the best practices of civil engineering, while the frontier of a poorly trained one includes many valid inputs that point to serious deficiencies of the system.",0,0,0,1
1716,"
Developers frequently change the type of a program element and update all its references for performance, security, concurrency,library migration, or better maintainability.",1,0,0,0
1717,"Despite type changes being a common program transformation, it is the least automated and the least studied.",1,0,0,0
1718,"With this knowledge gap, researchers miss opportunities to improve the state of the art in automation for software evolution, tool builders do not invest resources where automation is most needed, language and library designers can-not make informed decisions when introducing new types, and developers fail to use common practices when changing types.",1,0,0,0
1719,"To fill this gap, we present the first large-scale and most fine-grained empirical study on type changes in Java.",0,0,1,0
1720,"We develop state-of-the-art tools to statically mine 297,543 type changes and their subsequent code adaptations from a diverse corpus of 129 Java projects containing 416,652 commits.",0,0,1,0
1721,With this rich data set we answer research questions about the practice of type changes.,0,0,1,0
1722,"Among others, we found that type changes are actually more common than renaming,but the current research and tools for type changes are inadequate.Based on our extensive and reliable data, we present actionable,empirically-justified implications.",0,0,0,1
1723,"
Due to the lexical gap between functionality descriptions and user queries, documentation-based API retrieval often produces poor results.Verb phrases and their phrase patterns are essential in both describing API functionalities and interpreting user queries.",1,0,0,0
1724,Thus we hypothesize that API retrieval can be facilitated by explicitly recognizing and matching between the fine-grained structures of functionality descriptions and user queries.,0,0,1,0
1725,"To verify this hypothesis, we conducted a large-scale empirical study on the functionality descriptions of 14,733 JDK and Android API methods.",0,1,0,0
1726,"We identified 356 different functionality verbs from the descriptions, which were grouped into 87 functionality categories, and we extracted 523 phrase patterns from the verb phrases of the descriptions.",0,0,0,1
1727,"Building on these findings, we propose an API method recommendation approach based on explicit matching of functionality verb phrases in functionality descriptions and user queries, called PreMA.",0,0,1,0
1728,"Our evaluation shows that PreMA can accurately recognize the functionality categories (92.8%) and phrase patterns (90.4%) of functionality description sentences; and when used for API retrieval tasks, PreMA can help participants complete their tasks more accurately and with fewer retries compared to a baseline approach.",0,0,0,1
1729,"
The cloud runs on REST APIs.",1,0,0,0
1730,"In this paper, we study how to intelligently generate data payloads embedded in REST API requests in order to find data-processing bugs in cloud services.",0,0,1,0
1731,"We discuss how to leverage REST API specifications, which, by definition, contain data schemas for API request bodies.",0,0,1,0
1732,"We then propose and evaluate a range of data fuzzing techniques, including structural schema fuzzing rules, various rule combinations, search heuristics, extracting data values from examples included in REST API specifications, and learning data values on-the-fly from previous service responses.",0,1,0,0
1733,"After evaluating these techniques, we identify the top-performing combination and use this algorithm to fuzz several Microsoft Azure cloud services.",0,1,0,0
1734,"During our experiments, we found 100s of “Internal Server Error” service crashes, which we triaged into 17 unique bugs and reported to Azure developers.",0,0,0,1
1735,"All these bugs are reproducible, confirmed, and fixed or in the process of being fixed.",0,0,0,1
1736,"
Smart home devices provide the convenience of remotely control-ling and automating home appliances.",1,0,0,0
1737,"The most advanced smart home environments allow developers to write apps to make smart home devices work together to accomplish tasks, e.g., home security and energy conservation.",1,0,0,0
1738,A smart home app typically implements narrow functionality and thus to fully implement desired functionality homeowners may need to install multiple apps.,1,0,0,0
1739,These different apps can conflict with each other and these conflicts can result in undesired actions such as locking the door during a fire.,1,0,0,0
1740,"In this paper, we study conflicts between apps on Samsung SmartThings, the most popular platform for developing and deploying smart home IoT devices.",0,0,1,0
1741,"By collecting and studying 198 official and 69 third-party apps, we found significant app conflicts in 3 categories: (1) close to 60% of app pairs that access the same device, (2) more than 90% of app pairs with physical interactions, and (3) around 11% of app pairs that access the same global variable.",0,1,0,0
1742,Our results suggest that the problem of conflicts between smart home apps is serious and can create potential safety risks.,0,0,0,1
1743,We then developed a conflict detection tool that uses model checking to automatically detect up to 96% of the conflicts.,0,1,0,0
1744,"
Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field.",1,0,0,0
1745,"A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data.",1,0,0,0
1746,"To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system.",1,0,0,0
1747,Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models.,1,0,0,0
1748,We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data.,0,0,1,0
1749,The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression.,0,1,0,0
1750,"Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (>0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work",0,0,0,1
1751,"
UI testing is tedious and time-consuming due to the manual effort required.",1,0,0,0
1752,Recent research has explored opportunities for reusing existing UI tests from an app to automatically generate new tests for other apps.,1,0,0,0
1753,"However, the evaluation of such techniques currently remains manual, unscalable, and unreproducible, which can waste effort and impede progress in this emerging area.",1,0,0,0
1754,"We introduce FrUITeR, a framework that automatically evaluates UI test reuse in a reproducible way.",0,0,1,0
1755,"We apply FrUITeR to existing test-reuse techniques on a uniform benchmark we established, resulting in 11,917 test reuse cases from 20 apps.",0,1,0,0
1756,We report several key findings aimed at improving UI test reuse that are missed by existing work.,0,0,0,1
1757,"
Detecting bugs in deep learning software at the architecture level provides additional benefits that detecting bugs at the model level does not provide.",1,0,0,0
1758,This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level.,0,1,0,0
1759,We propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation.,0,0,1,0
1760,"Our approach mainly comprises two kinds of abstraction techniques, i.e., one for tensors and one for numerical values.",0,1,0,0
1761,"Moreover, to scale up while maintaining adequate detection precision, we propose two abstraction techniques: tensor partitioning and (elementwise) affine relation analysis to abstract tensors and numerical values, respectively.",0,0,1,0
1762,"We realize the combination scheme of tensor partitioning and affine relation analysis (together with interval analysis) as DEBAR, and evaluate it on two datasets: neural architectures with known bugs (collected from existing studies) and real-world neural architectures.",0,1,0,0
1763,The evaluation results show that DEBAR outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability.,0,0,0,1
1764,DEBAR successfully detects all known numerical bugs with no false positives within 1.7–2.3 seconds per architecture.,0,0,0,1
1765,"On the real-world architectures, DEBAR reports 529 warnings within 2.6–135.4 seconds per architecture, where 299 warnings are true positives.",0,0,0,1
1766,"
Deep learning is being incorporated in many modern software systems.",1,0,0,0
1767,"Deep learning approaches train a deep neural network (DNN) model using training examples, and then use the DNN model for prediction.",1,0,0,0
1768,"While the structure of a DNN model as layers is observable, the model is treated in its entirety as a monolithic component.",1,0,0,0
1769,"To change the logic implemented by the model, e.g. to add/remove logic that recognizes inputs belonging to a certain class, or to replace the logic with an alternative, the training examples need to be changed and the DNN needs to be retrained using the new set of examples.",1,0,0,0
1770,We argue that decomposing a DNN into DNN modules— akin to decomposing a monolithic software code into modules—can bring the benefits of modularity to deep learning.,0,0,1,0
1771,"In this work, we develop a methodology for decomposing DNNs for multi-class problems into DNN modules.",0,0,1,0
1772,"For four canonical problems, namely MNIST, EMNIST, FMNIST, and KMNIST, we demonstrate that such decomposition enables reuse of DNN modules to create different DNNs, enables replacement of one DNN module in a DNN with another without needing to retrain.",0,0,0,1
1773,The DNN models formed by composing DNN modules are at least as good as traditional monolithic DNNs in terms of test accuracy for our problems.,0,0,0,1
1774,"
Machine learning software is increasingly being used to make decisions that affect people's lives.",1,0,0,0
1775,"But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.).",1,0,0,0
1776,"This ""algorithmic discrimination"" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community.",1,0,0,0
1777,"There have been works done to find ""algorithmic bias"" or ""ethical bias"" in the software system.",1,0,0,0
1778,"Once the bias is detected in the AI software system, the mitigation of bias is extremely important.",1,0,0,0
1779,"In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model.",0,0,1,0
1780,"Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model.",0,0,0,1
1781,We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle.,0,0,0,1
1782,Fairway offers much support for these two purposes.,0,0,0,1
1783,"
Executing software microbenchmarks, a form of small-scale performance tests predominantly used for libraries and frameworks, is a costly endeavor.",1,0,0,0
1784,"Full benchmark suites take up to multiple hours or days to execute, rendering frequent checks, e.g., as part of continuous integration (CI), infeasible.",1,0,0,0
1785,"However, altering benchmark configurations to reduce execution time without considering the impact on result quality can lead to benchmark results that are not representative of the software’s true performance.",1,0,0,0
1786,We propose the first technique to dynamically stop software microbenchmark executions when their results are sufficiently stable.,0,0,1,0
1787,Our approach implements three statistical stoppage criteria and is capable of reducing Java Microbenchmark Harness (JMH) suite execution times by 48.4% to 86.0%.,0,0,0,1
1788,"At the same time it retains the same result quality for 78.8% to 87.6% of the benchmarks, compared to executing the suite for the default duration.",0,0,0,1
1789,"The proposed approach does not require developers to manually craft custom benchmark configurations; instead, it provides automated mechanisms for dynamic reconfiguration.",0,0,0,1
1790,"Hence, making dynamic reconfiguration highly effective and efficient, potentially paving the way to inclusion of JMH microbenchmarks in CI.",0,0,0,1
1791,"
Recommendations between colleagues are effective for encouraging developers to adopt better practices.",1,0,0,0
1792,"Research shows these peer interactions are useful for improving developer behaviors, or the adoption of activities to help software engineers complete programming tasks.",1,0,0,0
1793,"However, in-person recommendations between developers in the workplace are declining.",1,0,0,0
1794,"One form of online recommendations between developers are pull requests, which allow users to propose code changes and provide feedback on contributions.",1,0,0,0
1795,"GitHub, a popular code hosting platform, recently introduced the suggested changes feature, which allows users to recommend improvements for pull requests.",1,0,0,0
1796,"To better understand this feature and its impact on recommendations between developers, we report an empirical study of this system, measuring usage, effectiveness, and perception.",0,1,1,0
1797,Our results show that suggested changes support code review activities and significantly impact the timing and communication between developers on pull requests.,0,0,0,1
1798,"This work provides insight into the suggested changes feature and implications for improving future systems for automated developer recommendations, such as providing situated, concise, and actionable feedback.",0,0,0,1
1799,"
Fuzzing is a widely used technique for detecting software bugs and vulnerabilities.",1,0,0,0
1800,Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage.,1,0,0,0
1801,"Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations.",1,0,0,0
1802,"In recent years, machine learning (ML) based mutation strategies have reported promising results.",1,0,0,0
1803,"However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data.",1,0,0,0
1804,"As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage).",0,0,1,0
1805,The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high.,0,0,0,1
1806,MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2× more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs,0,0,0,1
1807,"
Code coverage analysis plays an important role in the software testing process.",1,0,0,0
1808,"More recently, the remarkable effectiveness of coverage feedback has triggered a broad interest in feedback-guided fuzzing.",1,0,0,0
1809,"In this work, we introduce bcov, a tool for binary-level coverage analysis.",0,0,1,0
1810,Our tool statically instruments x86-64 binaries in the ELF format without compiler support.,0,0,1,0
1811,We implement several techniques to improve efficiency and scale to large real-world software.,0,0,1,0
1812,"First, we bring Agrawal’s probe pruning technique to binary-level instrumentation and effectively leverage its superblocks to reduce overhead.",0,1,0,0
1813,"Second, we introduce sliced microexecution, a robust technique for jump table analysis which improves CFG precision and enables us to instrument jump table entries.",0,1,0,0
1814,"To address this challenge, we aggressively exploit padding bytes and systematically host detours in neighboring basic blocks.",0,1,0,0
1815,We evaluate bcov on a corpus of 95 binaries compiled from eight popular and well-tested packages like FFmpeg and LLVM.,0,1,0,0
1816,"Two instrumentation policies, with different edge-level precision, are used to patch all functions in this corpus - over 1.6 million functions.",0,1,0,0
1817,Our precise policy has average performance and memory overheads of 14% and 22% respectively.,0,0,0,1
1818,Instrumented binaries do not introduce any test regressions.,0,0,0,1
1819,The reported coverage is highly accurate with an average F-score of 99.86%.,0,0,0,1
1820,"Finally, our jump table analysis is comparable to that of IDA Pro on gcc binaries and outperforms it on clang binaries.",0,0,0,1
1821,"
Facing the limited resource of smartphones, asynchronous programming significantly improves the performance of Android applications.",1,0,0,0
1822,Android provides several packaged components to ease the development of asynchronous programming.,1,0,0,0
1823,"Among them, the AsyncTask component is widely used by developers since it is easy to implement.",1,0,0,0
1824,"However, the abuse of AsyncTask component can decrease responsiveness and even lead to crashes.",1,0,0,0
1825,"By investigating the Android Developer Documentation and technical forums, we summarize five misuse patterns about AsyncTask.",0,0,1,0
1826,"To detect them, we propose a flow, context, object and field-sensitive inter-procedural static analysis approach.",0,1,0,0
1827,"Specifically, the static analysis includes typestate analysis, reference analysis and loop analysis.",0,1,0,0
1828,"Based on the AsyncTask-related information obtained during static analysis, we check the misuse according to predefined detection rules.",0,1,0,0
1829,The proposed approach is implemented into a tool called AsyncChecker.,0,0,1,0
1830,"We evaluate AsyncChecker on a self-designed benchmark suite called AsyncBench and 1,759 real-world apps.",0,1,0,0
1831,"AsyncChecker finds 17,946 misused AsyncTask instances in 1,417 real-world apps (80.6%).",0,0,0,1
1832,"The precision, recall and F-measure of AsyncChecker on real-world applications are 97.2%, 89.8% and 0.93, respectively.",0,0,0,1
1833,"Compared with existing tools, AsyncChecker can detect more asynchronous problems.",0,0,0,1
1834,We report the misuse problems to developers via GitHub.,0,0,0,1
1835,Several developers have confirmed and fixed the problems found by AsyncChecker.,0,0,0,1
1836,The result implies that our approach is effective and developers do take the misuse of AsyncTask as a serious problem.,0,0,0,1
1837,"
Higher-order mutation has the potential for improving major drawbacks of traditional first-order mutation, such as by simulating more realistic faults or improving test-optimization techniques.",1,0,0,0
1838,"Despite interest in studying promising higher-order mutants, such mutants are difficult to find due to the exponential search space of mutation combinations.",1,0,0,0
1839,"State-of-the-art approaches rely on genetic search, which is often incomplete and expensive due to its stochastic nature.",1,0,0,0
1840,"First, we propose a novel way of finding a complete set of higher-order mutants by using variational execution, a technique that can, in many cases, explore large search spaces completely and often efficiently.",0,0,1,0
1841,"Second, we use the identified complete set of higher-order mutants to study their characteristics.",0,0,1,0
1842,"Finally, we use the identified characteristics to design and evaluate a new search strategy, independent of variational execution, that is highly effective at finding higher-order mutants even in large codebases.",0,0,0,1
1843,"
Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks).",1,0,0,0
1844,"However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality.",1,0,0,0
1845,This paper sets out to answer this question.,0,0,1,0
1846,"Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.",0,0,1,0
1847,"
Credit scoring systems are critical FinTech applications that concern the analysis of the creditworthiness of a person or organization.",1,0,0,0
1848,"While decisions were previously based on human expertise, they are now increasingly relying on data analysis and machine learning.",1,0,0,0
1849,"In this paper, we assess the ability of state-of-the-art adversarial machine learning to craft attacks on a real-world credit scoring system.",0,0,1,0
1850,"Interestingly, we find that, while these techniques can generate large numbers of adversarial data, these are practically useless as they all violate domain-specific constraints.",0,0,0,1
1851,"In other words, the generated examples are all false positives as they cannot occur in practice.",0,0,0,1
1852,"To circumvent this limitation, we propose CoEvA2, a search-based method that generates valid adversarial examples (satisfying the domain constraints).",0,0,1,0
1853,"CoEvA2 utilizes multi-objective search in order to simultaneously handle constraints, perform the attack and maximize the overdraft amount requested.",0,0,1,0
1854,We evaluate CoEvA2 on a major bank's real-world system by checking its ability to craft valid attacks.,0,1,0,0
1855,"CoEvA2 generates thousands of valid adversarial examples, revealing a high risk for the banking system.",0,0,0,1
1856,"Fortunately, by improving the system through adversarial training (based on the produced examples), we increase its robustness and make our attack fail.",0,0,0,1
1857,"
Machine learning models are increasingly being used in important decision-making software such as approving bank loans, recommending criminal sentencing, hiring employees, and so on.",1,0,0,0
1858,"It is important to ensure the fairness of these models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making.",1,0,0,0
1859,Algorithms have been developed to measure unfairness and mitigate them to a certain extent.,1,0,0,0
1860,"In this paper, we have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models.",0,1,1,0
1861,"We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness.",0,1,0,0
1862,"Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance.",0,1,0,0
1863,We have found that some model optimization techniques result in inducing unfairness in the models.,0,0,0,1
1864,"On the other hand, although there are some fairness control mechanisms in machine learning libraries, they are not documented.",0,0,0,1
1865,The mitigation algorithm also exhibit common patterns such as mitigation in the post-processing is often costly (in terms of performance) and mitigation in the pre-processing stage is preferred in most cases.,0,0,0,1
1866,We have also presented different trade-off choices of fairness mitigation decisions.,0,0,0,1
1867,Our study suggests future research directions to reduce the gap between theoretical fairness aware algorithms and the software engineering methods to leverage them in practice.,0,0,0,1
1868,"
Database Management Systems (DBMS) are used ubiquitously.",1,0,0,0
1869,"To efficiently access data, they apply sophisticated optimizations.",1,0,0,0
1870,"Incorrect optimizations can result in logic bugs, which cause a query to compute an incorrect result set.",1,0,0,0
1871,"We propose Non-Optimizing Reference Engine Construction (NoREC), a fully-automatic approach to detect optimization bugs in DBMS.",0,0,1,0
1872,"Conceptually, this approach aims to evaluate a query by an optimizing and a non-optimizing version of a DBMS, to then detect differences in their returned result set, which would indicate a bug in the DBMS.",0,0,1,0
1873,"Obtaining a non-optimizing version of a DBMS is challenging, because DBMS typically provide limited control over optimizations.",1,0,0,0
1874,"Our core insight is that a given, potentially randomly-generated optimized query can be rewritten to one that the DBMS cannot optimize.",0,0,1,0
1875,"We evaluated NoREC in an extensive testing campaign on four widely-used DBMS, namely PostgreSQL, MariaDB, SQLite, and CockroachDB.",0,1,0,0
1876,"We found 159 previously unknown bugs in the latest versions of these systems, 141 of which have been fixed by the developers.",0,0,0,1
1877,"Of these, 51 were optimization bugs, while the remaining were error and crash bugs.",0,0,0,1
1878,"Our results suggest that NoREC is effective, general and requires little implementation effort, which makes the technique widely applicable in practice.",0,0,0,1
1879,"
Performance issues compromise the response time and resource consumption of a software system.",1,0,0,0
1880,"Modern software systems use issue tracking systems to manage all kinds of issue reports, including performance issues.",1,0,0,0
1881,The problem is that performance issues are often not explicitly tagged.,1,0,0,0
1882,"The tagging mechanism, if exists, is completely voluntary, depending on the project’s convention and on submitters’ discipline.",1,0,0,0
1883,"For example, the performance tag rate in Apache’s Jira system is below 1%.",1,0,0,0
1884,This paper contributes a hybrid classification approach that combines linguistic patterns and machine/deep learning techniques to automatically detect performance issue reports.,0,0,1,0
1885,We manually analyzed 980 real-life performance issue reports and derived 80 project-agnostic linguistic patterns that recur in the reports.,0,1,0,0
1886,Our approach uses these linguistic patterns to construct the sentence-level and issue-level learning features for training effective machine/deep learning classifiers.,0,1,0,0
1887,"We test our approach on two separate datasets, each consisting of 980 unclassified issue reports, and compare the results with 31 baseline methods.",0,1,0,0
1888,Our approach can reach up to 83% precision and up to 59% recall.,0,0,0,1
1889,"The only comparable baseline method is BERT, which is still 25% lower in the F1-score.",0,0,0,1
1890,"
Atoms of confusion are small patterns of code that have been empirically validated to be difficult to hand-evaluate by programmers.",1,0,0,0
1891,"Previous research focused on defining and quantifying this phenomenon, but not on explaining or critiquing it.",1,0,0,0
1892,"In this work, we address core omissions to the body of work on atoms of confusion, focusing on the ‘how’ and ‘why’ of programmer misunderstanding.",0,0,1,0
1893,"We performed a think-aloud study in which we observed programmers, both professionals and students, as they hand-evaluated confusing code.",0,1,0,0
1894,"We performed a qualitative analysis of the data and found several surprising results, which explain previous results, outline avenues of further research, and suggest improvements of the research methodology.",0,1,0,0
1895,"A notable observation is that correct hand-evaluations do not imply understanding, and incorrect evaluations not misunderstanding.",0,0,0,1
1896,We believe this and other observations may be used to improve future studies and models of program comprehension.,0,0,0,1
1897,We argue that thinking of confusion as an atomic construct may pose challenges to formulating new candidates for atoms of confusion.,0,0,0,1
1898,"Ultimately, we question whether hand-evaluation correctness is, itself, a sufficient instrument to study program comprehension.",0,0,0,1
1899,"
We consider a usage model for automated machine learning (AutoML) in which users can influence the generated pipeline by providing a weak pipeline specification: an unordered set of API components from which the AutoML system draws the components it places into the generated pipeline.",1,0,0,0
1900,"Such specifications allow users to express preferences over the components that appear in the pipeline, for example a desire for interpretable components to appear in the pipeline.",1,0,0,0
1901,"We present AMS, an approach to automatically strengthen weak specifications to include unspecified complementary and functionally related API components, populate the space of hyperparameters and their values, and pair this configuration with a search procedure to produce a strong pipeline specification: a full description of the search space for candidate pipelines.",0,0,1,0
1902,"ams uses normalized pointwise mutual information on a code corpus to identify complementary components, BM25 as a lexical similarity score over the target API's documentation to identify functionally related components, and frequency distributions in the code corpus to extract key hyperparameters and values.",0,1,0,0
1903,"We show that strengthened specifications can produce pipelines that outperform the pipelines generated from the initial weak specification and an expert-annotated variant, while producing pipelines that still reflect the user preferences captured in the original weak specification.",0,0,0,1
1904,"
We present counterintuitive results for the scalability of fuzzing.",0,0,1,0
1905,"Given the same non-deterministic fuzzer, finding the same bugs linearly faster requires linearly more machines.",1,0,0,0
1906,"For instance, with twice the machines, we can find all known bugs in half the time.",1,0,0,0
1907,"Yet, finding linearly more bugs in the same time requires exponentially more machines.",1,0,0,0
1908,"For instance, for every new bug we want to find in 24 hours, we might need twice more machines.",1,0,0,0
1909,Similarly for coverage.,1,0,0,0
1910,"With exponentially more machines, we can cover the same code exponentially faster, but uncovered code only linearly faster.",1,0,0,0
1911,"In other words, re-discovering the same vulnerabilities is cheap but finding new vulnerabilities is expensive.",0,0,0,1
1912,This holds even under the simplifying assumption of no parallelization overhead.,0,0,0,1
1913,"We derive these observations from over four CPU years worth of fuzzing campaigns involving almost three hundred open source programs, two state-of-the-art greybox fuzzers, four measures of code coverage, and two measures of vulnerability discovery.",0,1,0,0
1914,We provide a probabilistic analysis and conduct simulation experiments to explain this phenomenon.,0,1,0,0
1915,"
Energy efficiency is an increasingly important quality attribute for software, particularly for mobile apps.",1,0,0,0
1916,"Just like any other software attribute, energy behavior of mobile apps should be properly tested prior to their release.",1,0,0,0
1917,"However, mobile apps are riddled with energy defects, as currently there is a lack of proper energy testing tools.",1,0,0,0
1918,"Indeed, energy testing is a fledgling area of research and recent advances have mainly focused on test input generation.",1,0,0,0
1919,"This paper presents ACETON, the first approach aimed at solving the oracle problem for testing the energy behavior of mobile apps.",0,0,1,0
1920,"ACETON employs Deep Learning to automatically construct an oracle that not only determines whether a test execution reveals an energy defect, but also the type of energy defect.",0,0,1,0
1921,"By carefully selecting features that can be monitored on any app and mobile device, we are assured the oracle constructed using ACETON is highly reusable.",0,1,0,1
1922,"Our experiments show that the oracle produced by ACETON is both highly accurate, achieving an overall precision and recall of 99%, and efficient, detecting the existence of energy defects in only 37 milliseconds on average.",0,0,0,1
1923,"
Energy accounting is a fundamental problem in energy management, defined as attributing global energy consumption to individual components of interest.",1,0,0,0
1924,"In this paper, we take on this problem at the application level, where the components for accounting are application logical units, such as methods, classes, and packages.",0,0,1,0
1925,"Given a Java application, our novel runtime system Chappie produces an energy footprint, i.e., the relative energy consumption of all programming abstraction units within the application.",0,1,0,0
1926,"First, relative to targeted energy profiling where the profiler determines the energy consumption of a pre-defined application logical unit, e.g., a specific method, Chappie is total: the energy footprint encompasses all methods within an application.",0,0,0,1
1927,"Second, Chappie is concurrency-aware: energy attribution is fully aware of the multi-threaded behavior of Java applications, including JVM bookkeeping threads.",0,0,0,1
1928,"Third, Chappie is an embodiment of a novel philosophy for application-level energy accounting and profiling, which states that the accounting run should preserve the temporal phased power behavior of the application, and the spatial power distribution among the underlying hardware system.",0,0,0,1
1929,We term this important property as calmness.,0,0,0,1
1930,"Against state-of-the-art DaCapo benchmarks, we show that the energy footprint generated by Chappie is precise while incurring negligible overhead.",0,0,0,1
1931,"In addition, all results are produced with a high degree of calmness.",0,0,0,1
1932,"
Android deep link is a URL that takes users to a specific page of a mobile app, enabling seamless user experience from a webpage to an app.",1,0,0,0
1933,"Android app link, a new type of deep link introduced in Android 6.0, is claimed to offer more benefits, such as supporting instant apps and providing more secure verification to protect against hijacking attacks that previous deep links can not.",1,0,0,0
1934,"However, we find that the app link is not as secure as claimed, because the verification process can be bypassed by exploiting instant apps.",0,0,1,0
1935,"In this paper, we explore the weakness of the existing app link mechanism and propose three feasible hijacking attacks.",0,0,1,0
1936,"Our findings show that even popular apps are subject to these attacks, such as Twitter, Whatsapp, Facebook Message.",0,0,0,1
1937,Our observation is confirmed by Google.,0,0,0,1
1938,"To measure the severity of these vulnerabilities, we develop an automatic tool to detect vulnerable apps, and perform a large-scale empirical study on 400,000 Android apps.",0,1,0,0
1939,Experiment results suggest that app link hijacking vulnerabilities are prevalent in the ecosystem.,0,0,0,1
1940,"Specifically, 27.1% apps are vulnerable to link hijacking with smart text selection (STS); 30.0% apps are vulnerable to link hijacking without STS, and all instant apps are vulnerable to instant app attack.",0,0,0,1
1941,We provide an in-depth understanding of the mechanisms behind these types of attacks.,0,0,0,1
1942,"Furthermore, we propose the corresponding detection and defense methods that can successfully prevent the proposed hijackings for all the evaluated apps, thus raising the bar against the attacks on Android app links.",0,0,0,1
1943,Our insights and findings demonstrate the urgency to identify and prevent app link hijacking attacks.,0,0,0,1
1944,"
Program slicing has been widely applied in a variety of software engineering tasks.",1,0,0,0
1945,"However, existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables, rather than neural networks that are composed of neurons and synapses.",1,0,0,0
1946,"In this paper, we introduce NNSlicer, the first approach for slicing deep neural networks based on data-flow analysis.",0,0,1,0
1947,Our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset.,0,0,1,0
1948,"Then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons, and calculate the slice as the neurons and the synapses with larger contributions.",0,0,1,0
1949,"We demonstrate the usefulness and effectiveness of NNSlicer with three applications, including adversarial input detection, model pruning, and selective model protection.",0,1,0,0
1950,"In all applications, NNSlicer significantly outperforms other baselines that do not rely on data flow analysis.",0,0,0,1
1951,"
Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications.",1,0,0,0
1952,"These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras.",1,0,0,0
1953,A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data.,1,0,0,0
1954,"To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted.",1,0,0,0
1955,Existing studies focus on the development of DL software and extensively analyze faults in DL programs.,1,0,0,0
1956,"However, the deployment of DL software has not been comprehensively studied.",1,0,0,0
1957,"To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software.",0,0,1,0
1958,"We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers.",0,1,0,0
1959,"We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.",0,0,0,1
1960,"
Using online Q&A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice.",1,0,0,0
1961,Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO.,1,0,0,0
1962,In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code.,0,0,1,0
1963,MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match.,0,0,1,0
1964,"To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations.",0,0,1,0
1965,"The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences.",0,0,1,0
1966,"We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques.",0,1,0,0
1967,"We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances.",0,1,0,0
1968,In some cases the post is judged to be even better than the one manually found by the participant.,0,0,0,1
1969,"
Assertion oracles are executable boolean expressions placed inside the program that should pass (return true) for all correct executions and fail (return false) for all incorrect executions.",1,0,0,0
1970,"Because designing perfect assertion oracles is difficult, assertions often fail to distinguish between correct and incorrect executions.",1,0,0,0
1971,"In other words, they are prone to false positives and false negatives.",1,0,0,0
1972,"In this paper, we propose GAssert (Genetic ASSERTion improvement), the first technique to automatically improve assertion oracles.",0,0,1,0
1973,"Given an assertion oracle and evidence of false positives and false negatives, GAssert implements a novel co-evolutionary algorithm that explores the space of possible assertions to identify one with fewer false positives and false negatives.",0,0,1,0
1974,Our empirical evaluation on 34 Java methods from 7 different Java code bases shows that GAssert effectively improves assertion oracles.,0,1,0,0
1975,"GAssert outperforms two baselines (random and invariant-based oracle improvement), and is comparable with and in some cases even outperformed human-improved assertions.",0,0,0,1
1976,"
Formal methods use SMT solvers extensively for deciding formula satisfiability, for instance, in software verification, systematic test generation, and program synthesis.",1,0,0,0
1977,"However, due to their complex implementations, solvers may contain critical bugs that lead to unsound results.",1,0,0,0
1978,"Given the wide applicability of solvers in software reliability, relying on such unsound results may have detrimental consequences.",1,0,0,0
1979,"In this paper, we present STORM, a novel blackbox mutational fuzzing technique for detecting critical bugs in SMT solvers.",0,0,1,0
1980,We run our fuzzer on seven mature solvers and find 29 previously unknown critical bugs.,0,1,0,0
1981,STORM is already being used in testing new features of popular solvers before deployment.,0,0,0,1
1982,"
Build systems are essential for modern software maintenance and development, while build failures occur frequently across software systems, inducing non-negligible costs in development activities.",1,0,0,0
1983,"Build failure resolution is a challenging problem and multiple studies have demonstrated that developers spend non-trivial time in resolving encountered build failures; to relieve manual efforts, automated resolution techniques are emerging recently, which are promising but still limitedly effective.",1,0,0,0
1984,Understanding how build failures are resolved in practice can provide guidelines for both developers and researchers on build issue resolution.,1,0,0,0
1985,"Therefore, this work presents a comprehensive study of fix patterns in practical build failures.",0,0,1,0
1986,"Specifically, we study 1,080 build issues of three popular build systems Maven, Ant, and Gradle from Stack Overflow, construct a fine-granularity taxonomy of 50 categories regarding to the failure symptoms, and summarize the fix patterns for different failure types.",0,1,0,0
1987,"Our key findings reveal that build issues stretch over a wide spectrum of symptoms; 67.96% of the build issues are fixed by modifying the build script code related to plugins and dependencies; and there are 20 symptom categories, more than half of whose build issues can be fixed by specific patterns.",0,0,0,1
1988,"Furthermore, we also address the challenges in applying non-intuitive or simplistic fix patterns for developers.",0,0,0,1
1989,"
In this paper, we take the fundamental perspective of fuzzing as a learning process.",1,0,0,0
1990,"Suppose before fuzzing, we know nothing about the behaviors of a program P: What does it do?",1,0,0,0
1991,"Executing the first test input, we learn how P behaves for this input.",1,0,0,0
1992,"Executing the next input, we either observe the same or discover a new behavior.",1,0,0,0
1993,"As such, each execution reveals ”some amount” of information about P’s behaviors.",1,0,0,0
1994,A classic measure of information is Shannon’s entropy.,1,0,0,0
1995,Measuring entropy allows us to quantify how much is learned from each generated test input about the behaviors of the program.,1,0,0,0
1996,"Within a probabilistic model of fuzzing, we show how entropy also measures fuzzer efficiency.",1,0,0,0
1997,"Specifically, it measures the general rate at which the fuzzer discovers new behaviors.",1,0,0,0
1998,"Intuitively, efficient fuzzers maximize information.",1,0,0,0
1999,"From this information theoretic perspective, we develop Entropic, an entropy-based power schedule for greybox fuzzing which assigns more energy to seeds that maximize information.",0,0,1,0
2000,We implemented Entropic into the popular greybox fuzzer LibFuzzer.,0,1,0,0
2001,Our experiments with more than 250 open-source programs (60 million LoC) demonstrate a substantially improved efficiency and confirm our hypothesis that an efficient fuzzer maximizes information.,0,0,0,1
2002,Entropic has been independently evaluated and invited for integration into main-line LibFuzzer.,0,0,0,1
2003,"Entropic now runs on more than 25,000 machines fuzzing hundreds of security-critical software systems simultaneously and continuously.",0,0,0,1
2004,"
When designing a software system, architects make a series of design decisions that directly impact the system's quality.",1,0,0,0
2005,"The number of available design alternatives grows rapidly with system size, creating an enormous space of intertwined design concerns that renders manual exploration impractical.",1,0,0,0
2006,"We present eQual, a model-driven technique for simulation-based assessment of architectural designs.",0,0,1,0
2007,"While it is not possible to guarantee optimal decisions so early in the design process, eQual improves decision quality.",0,0,1,0
2008,eQual is effective in practice because it (1) limits the amount of information the architects have to provide and (2) adapts optimization algorithms to effectively explore massive spaces of design alternatives.,0,0,1,0
2009,We empirically demonstrate that eQual yields designs whose quality is comparable to a set of systems' known optimal designs.,0,1,0,0
2010,"A user study shows that, compared to the state-of-the-art, engineers using eQual produce statistically significantly higher-quality designs with a large effect size, are statistically significantly more confident in their designs, and find eQual easier to use.",0,0,0,1
2011,"
Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice.",1,0,0,0
2012,"However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains.",1,0,0,0
2013,"Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems.",1,0,0,0
2014,"In this work, we propose a novel approach, LEMON, to testing DL libraries.",0,0,1,0
2015,"In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries.",0,0,1,0
2016,"We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet.",0,1,0,0
2017,"The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers.",0,0,0,1
2018,"Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.",0,0,0,1
